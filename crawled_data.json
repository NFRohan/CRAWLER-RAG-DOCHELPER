{
  "results": [
    {
      "url": "https://docs.langchain.com/oss/python/concepts/context.md",
      "title": null,
      "raw_content": "# Context overview \\*\\*Context engineering\\*\\* is the practice of building dynamic systems that provide the right information and tools, in the right format, so that an AI application can accomplish a task. Context can be characterized along two key dimensions: 1. By \\*\\*mutability\\*\\*: \\* \\*\\*Static context\\*\\*: Immutable data that doesn't change during execution (e.g., user metadata, database connections, tools) \\* \\*\\*Dynamic context\\*\\*: Mutable data that evolves as the application runs (e.g., conversation history, intermediate results, tool call observations) 2. By \\*\\*lifetime\\*\\*: \\* \\*\\*Runtime context\\*\\*: Data scoped to a single run or invocation \\* \\*\\*Cross-conversation context\\*\\*: Data that persists across multiple conversations or sessions  Runtime context refers to local context: data and dependencies your code needs to run. It does \\*\\*not\\*\\* refer to: \\* The LLM context, which is the data passed into the LLM's prompt. \\* The \"context window\", which is the maximum number of tokens that can be passed to the LLM. Runtime context is a form of dependency injection and can be used to optimize the LLM context. It lets to provide dependencies (like database connections, user IDs, or API clients) to your tools and nodes at runtime rather than hardcoding them. For example, you can use user metadata in the runtime context to fetch user preferences and feed them into the context window.  LangGraph provides three ways to manage context, which combines the mutability and lifetime dimensions: | Context type | Description | Mutability | Lifetime | Access method | | ------------------------------------------------------------------------------------------- | ------------------------------------------------------ | ---------- | ------------------ | --------------------------------------- | | [\\*\\*Static runtime context\\*\\*](#static-runtime-context) | User metadata, tools, db connections passed at startup | Static | Single run | `context` argument to `invoke`/`stream` | | [\\*\\*Dynamic runtime context (state)\\*\\*](#dynamic-runtime-context-state) | Mutable data that evolves during a single run | Dynamic | Single run | LangGraph state object | | [\\*\\*Dynamic cross-conversation context (store)\\*\\*](#dynamic-cross-conversation-context-store) | Persistent data shared across conversations | Dynamic | Cross-conversation | LangGraph store | ## Static runtime context \\*\\*Static runtime context\\*\\* represents immutable data like user metadata, tools, and database connections that are passed to an application at the start of a run via the `context` argument to `invoke`/`stream`. This data does not change during execution. ```python theme={null} @dataclass class ContextSchema: user\\_name: str graph.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]}, context={\"user\\_name\": \"John Smith\"} # [!code highlight] ) ```   ```python theme={null} from dataclasses import dataclass from langchain.agents import create\\_agent from langchain.agents.middleware import dynamic\\_prompt, ModelRequest @dataclass class ContextSchema: user\\_name: str @dynamic\\_prompt # [!code highlight] def personalized\\_prompt(request: ModelRequest) -> str: # [!code highlight] user\\_name = request.runtime.context.user\\_name return f\"You are a helpful assistant. Address the user as {user\\_name}.\" agent = create\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=[get\\_weather], middleware=[personalized\\_prompt], context\\_schema=ContextSchema ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]}, context=ContextSchema(user\\_name=\"John Smith\") # [!code highlight] ) ``` See [Agents](/oss/python/langchain/agents) for details.   ```python theme={null} from langgraph.runtime import Runtime def node(state: State, runtime: Runtime[ContextSchema]): # [!code highlight] user\\_name = runtime.context.user\\_name ... ``` \\* See [the Graph API](/oss/python/langgraph/graph-api#add-runtime-configuration) for details.   ```python theme={null} from langchain.tools import tool, ToolRuntime @tool def get\\_user\\_email(runtime: ToolRuntime[ContextSchema]) -> str: \"\"\"Retrieve user information based on user ID.\"\"\" # simulate fetching user info from a database email = get\\_user\\_email\\_from\\_db(runtime.context.user\\_name) # [!code highlight] return email ``` See the [tool calling guide](/oss/python/langchain/tools#configuration) for details.    The `Runtime` object can be used to access static context and other utilities like the active store and stream writer. See the [`Runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.Runtime) documentation for details.   ## Dynamic runtime context \\*\\*Dynamic runtime context\\*\\* represents mutable data that can evolve during a single run and is managed through the LangGraph state object. This includes conversation history, intermediate results, and values derived from tools or LLM outputs. In LangGraph, the state object acts as [short-term memory](/oss/python/concepts/memory) during a run.   Example shows how to incorporate state into an agent \\*\\*prompt\\*\\*. State can also be accessed by the agent's \\*\\*tools\\*\\*, which can read or update the state as needed. See [tool calling guide](/oss/python/langchain/tools#short-term-memory) for details. ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import dynamic\\_prompt, ModelRequest from langchain.agents import AgentState class CustomState(AgentState): # [!code highlight] user\\_name: str @dynamic\\_prompt # [!code highlight] def personalized\\_prompt(request: ModelRequest) -> str: # [!code highlight] user\\_name = request.state.get(\"user\\_name\", \"User\") return f\"You are a helpful assistant. User's name is {user\\_name}\" agent = create\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=[...], state\\_schema=CustomState, # [!code highlight] middleware=[personalized\\_prompt], # [!code highlight] ) agent.invoke({ \"messages\": \"hi!\", \"user\\_name\": \"John Smith\" }) ```   ```python theme={null} from typing\\_extensions import TypedDict from langchain.messages import AnyMessage from langgraph.graph import StateGraph class CustomState(TypedDict): # [!code highlight] messages: list[AnyMessage] extra\\_field: int def node(state: CustomState): # [!code highlight] messages = state[\"messages\"] ... return { # [!code highlight] \"extra\\_field\": state[\"extra\\_field\"] + 1 # [!code highlight] } builder = StateGraph(State) builder.add\\_node(node) builder.set\\_entry\\_point(\"node\") graph = builder.compile() ```    \\*\\*Turning on memory\\*\\* Please see the [memory guide](/oss/python/langgraph/add-memory) for more details on how to enable memory. This is a powerful feature that allows you to persist the agent's state across multiple invocations. Otherwise, the state is scoped only to a single run.   ## Dynamic cross-conversation context \\*\\*Dynamic cross-conversation context\\*\\* represents persistent, mutable data that spans across multiple conversations or sessions and is managed through the LangGraph store. This includes user profiles, preferences, and historical interactions. The LangGraph store acts as [long-term memory](/oss/python/concepts/memory#long-term-memory) across multiple runs. This can be used to read or update persistent facts (e.g., user profiles, preferences, prior interactions). ## See also \\* [Memory conceptual overview](/oss/python/concepts/memory) \\* [Short-term memory in LangChain](/oss/python/langchain/short-term-memory) \\* [Long-term memory in LangChain](/oss/python/langchain/long-term-memory) \\* [Memory in LangGraph](/oss/python/langgraph/add-memory) \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/concepts/context.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/concepts/memory.md",
      "title": null,
      "raw_content": "# Memory overview [Memory](/oss/python/langgraph/add-memory) is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction. This conceptual guide covers two types of memory, based on their recall scope: \\* [Short-term memory](#short-term-memory), or [thread](/oss/python/langgraph/persistence#threads)-scoped memory, tracks the ongoing conversation by maintaining message history within a session. LangGraph manages short-term memory as a part of your agent's [state](/oss/python/langgraph/graph-api#state). State is persisted to a database using a [checkpointer](/oss/python/langgraph/persistence#checkpoints) so the thread can be resumed at any time. Short-term memory updates when the graph is invoked or a step is completed, and the State is read at the start of each step. \\* [Long-term memory](#long-term-memory) stores user-specific or application-level data across sessions and is shared \\*across\\* conversational threads. It can be recalled \\*at any time\\* and \\*in any thread\\*. Memories are scoped to any custom namespace, not just within a single thread ID. LangGraph provides [stores](/oss/python/langgraph/persistence#memory-store) ([reference doc](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore)) to let you save and recall long-term memories. ## Short-term memory [Short-term memory](/oss/python/langgraph/add-memory#add-short-term-memory) lets your application remember previous interactions within a single [thread](/oss/python/langgraph/persistence#threads) or conversation. A [thread](/oss/python/langgraph/persistence#threads) organizes multiple interactions in a session, similar to the way email groups messages in a single conversation. LangGraph manages short-term memory as part of the agent's state, persisted via thread-scoped checkpoints. This state can normally include the conversation history along with other stateful data, such as uploaded files, retrieved documents, or generated artifacts. By storing these in the graph's state, the bot can access the full context for a given conversation while maintaining separation between different threads. ### Manage short-term memory Conversation history is the most common form of short-term memory, and long conversations pose a challenge to today's LLMs. A full history may not fit inside an LLM's context window, resulting in an irrecoverable error. Even if your LLM supports the full context length, most LLMs still perform poorly over long contexts. They get \"distracted\" by stale or off-topic content, all while suffering from slower response times and higher costs. Chat models accept context using messages, which include developer provided instructions (a system message) and user inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited and token-rich message lists can be costly, many applications can benefit from using techniques to manually remove or forget stale information. For more information on common techniques for managing messages, see the [Add and manage memory](/oss/python/langgraph/add-memory#manage-short-term-memory) guide. ## Long-term memory [Long-term memory](/oss/python/langgraph/add-memory#add-long-term-memory) in LangGraph allows systems to retain information across different conversations or sessions. Unlike short-term memory, which is \\*\\*thread-scoped\\*\\*, long-term memory is saved within custom \"namespaces.\" Long-term memory is a complex challenge without a one-size-fits-all solution. However, the following questions provide a framework to help you navigate the different techniques: \\* What is the type of memory? Humans use memories to remember facts ([semantic memory](#semantic-memory)), experiences ([episodic memory](#episodic-memory)), and rules ([procedural memory](#procedural-memory)). AI agents can use memory in the same ways. For example, AI agents can use memory to remember specific facts about a user to accomplish a task. \\* [When do you want to update memories?](#writing-memories) Memory can be updated as part of an agent's application logic (e.g., \"on the hot path\"). In this case, the agent typically decides to remember facts before responding to a user. Alternatively, memory can be updated as a background task (logic that runs in the background / asynchronously and generates memories). We explain the tradeoffs between these approaches in the [section below](#writing-memories). Different applications require various types of memory. Although the analogy isn't perfect, examining [human memory types](https://www.psychologytoday.com/us/basics/memory/types-of-memory?ref=blog.langchain.dev) can be insightful. Some research (e.g., the [CoALA paper](https://arxiv.org/pdf/2309.02427)) have even mapped these human memory types to those used in AI agents. | Memory Type | What is Stored | Human Example | Agent Example | | -------------------------------- | -------------- | -------------------------- | ------------------- | | [Semantic](#semantic-memory) | Facts | Things I learned in school | Facts about a user | | [Episodic](#episodic-memory) | Experiences | Things I did | Past agent actions | | [Procedural](#procedural-memory) | Instructions | Instincts or motor skills | Agent system prompt | ### Semantic memory [Semantic memory](https://en.wikipedia.org/wiki/Semantic\\_memory), both in humans and AI agents, involves the retention of specific facts and concepts. In humans, it can include information learned in school and the understanding of concepts and their relationships. For AI agents, semantic memory is often used to personalize applications by remembering facts or concepts from past interactions.  Semantic memory is different from \"semantic search,\" which is a technique for finding similar content using \"meaning\" (usually as embeddings). Semantic memory is a term from psychology, referring to storing facts and knowledge, while semantic search is a method for retrieving information based on meaning rather than exact matches.  Semantic memories can be managed in different ways: #### Profile Memories can be a single, continuously updated \"profile\" of well-scoped and specific information about a user, organization, or other entity (including the agent itself). A profile is generally just a JSON document with various key-value pairs you've selected to represent your domain. When remembering a profile, you will want to make sure that you are \\*\\*updating\\*\\* the profile each time. As a result, you will want to pass in the previous profile and [ask the model to generate a new profile](https://github.com/langchain-ai/memory-template) (or some [JSON patch](https://github.com/hinthornw/trustcall) to apply to the old profile). This can be become error-prone as the profile gets larger, and may benefit from splitting a profile into multiple documents or \\*\\*strict\\*\\* decoding when generating documents to ensure the memory schemas remains valid. #### Collection Alternatively, memories can be a collection of documents that are continuously updated and extended over time. Each individual memory can be more narrowly scoped and easier to generate, which means that you're less likely to \\*\\*lose\\*\\* information over time. It's easier for an LLM to generate \\*new\\* objects for new information than reconcile new information with an existing profile. As a result, a document collection tends to lead to [higher recall downstream](https://en.wikipedia.org/wiki/Precision\\_and\\_recall). However, this shifts some complexity memory updating. The model must now \\*delete\\* or \\*update\\* existing items in the list, which can be tricky. In addition, some models may default to over-inserting and others may default to over-updating. See the [Trustcall](https://github.com/hinthornw/trustcall) package for one way to manage this and consider evaluation (e.g., with a tool like [LangSmith](https://docs.smith.langchain.com/tutorials/Developers/evaluation)) to help you tune the behavior. Working with document collections also shifts complexity to memory \\*\\*search\\*\\* over the list. The `Store` currently supports both [semantic search](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.SearchOp.query) and [filtering by content](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.SearchOp.filter). Finally, using a collection of memories can make it challenging to provide comprehensive context to the model. While individual memories may follow a specific schema, this structure might not capture the full context or relationships between memories. As a result, when using these memories to generate responses, the model may lack important contextual information that would be more readily available in a unified profile approach. Regardless of memory management approach, the central point is that the agent will use the semantic memories to [ground its responses](/oss/python/langchain/retrieval), which often leads to more personalized and relevant interactions. ### Episodic memory [Episodic memory](https://en.wikipedia.org/wiki/Episodic\\_memory), in both humans and AI agents, involves recalling past events or actions. The [CoALA paper](https://arxiv.org/pdf/2309.02427) frames this well: facts can be written to semantic memory, whereas \\*experiences\\* can be written to episodic memory. For AI agents, episodic memory is often used to help an agent remember how to accomplish a task. In practice, episodic memories are often implemented through few-shot example prompting, where agents learn from past sequences to perform tasks correctly. Sometimes it's easier to \"show\" than \"tell\" and LLMs learn well from examples. Few-shot learning lets you [\"program\"](https://x.com/karpathy/status/1627366413840322562) your LLM by updating the prompt with input-output examples to illustrate the intended behavior. While various best-practices can be used to generate few-shot examples, often the challenge lies in selecting the most relevant examples based on user input. Note that the memory [store](/oss/python/langgraph/persistence#memory-store) is just one way to store data as few-shot examples. If you want to have more developer involvement, or tie few-shots more closely to your evaluation harness, you can also use a [LangSmith Dataset](/langsmith/index-datasets-for-dynamic-few-shot-example-selection) to store your data. Then dynamic few-shot example selectors can be used out-of-the box to achieve this same goal. LangSmith will index the dataset for you and enable retrieval of few shot examples that are most relevant to the user input based upon keyword similarity ([using a BM25-like algorithm](/langsmith/index-datasets-for-dynamic-few-shot-example-selection) for keyword based similarity). See this how-to [video](https://www.youtube.com/watch?v=37VaU7e7t5o) for example usage of dynamic few-shot example selection in LangSmith. Also, see this [blog post](https://blog.langchain.dev/few-shot-prompting-to-improve-tool-calling-performance/) showcasing few-shot prompting to improve tool calling performance and this [blog post](https://blog.langchain.dev/aligning-llm-as-a-judge-with-human-preferences/) using few-shot example to align an LLMs to human preferences. ### Procedural memory [Procedural memory](https://en.wikipedia.org/wiki/Procedural\\_memory), in both humans and AI agents, involves remembering the rules used to perform tasks. In humans, procedural memory is like the internalized knowledge of how to perform tasks, such as riding a bike via basic motor skills and balance. Episodic memory, on the other hand, involves recalling specific experiences, such as the first time you successfully rode a bike without training wheels or a memorable bike ride through a scenic route. For AI agents, procedural memory is a combination of model weights, agent code, and agent's prompt that collectively determine the agent's functionality. In practice, it is fairly uncommon for agents to modify their model weights or rewrite their code. However, it is more common for agents to modify their own prompts. One effective approach to refining an agent's instructions is through [\"Reflection\"](https://blog.langchain.dev/reflection-agents/) or meta-prompting. This involves prompting the agent with its current instructions (e.g., the system prompt) along with recent conversations or explicit user feedback. The agent then refines its own instructions based on this input. This method is particularly useful for tasks where instructions are challenging to specify upfront, as it allows the agent to learn and adapt from its interactions. For example, we built a [Tweet generator](https://www.youtube.com/watch?v=Vn8A3BxfplE) using external feedback and prompt re-writing to produce high-quality paper summaries for Twitter. In this case, the specific summarization prompt was difficult to specify \\*a priori\\*, but it was fairly easy for a user to critique the generated Tweets and provide feedback on how to improve the summarization process. The below pseudo-code shows how you might implement this with the LangGraph memory [store](/oss/python/langgraph/persistence#memory-store), using the store to save a prompt, the `update\\_instructions` node to get the current prompt (as well as feedback from the conversation with the user captured in `state[\"messages\"]`), update the prompt, and save the new prompt back to the store. Then, the `call\\_model` get the updated prompt from the store and uses it to generate a response. ```python theme={null} # Node that \\*uses\\* the instructions def call\\_model(state: State, store: BaseStore): namespace = (\"agent\\_instructions\", ) instructions = store.get(namespace, key=\"agent\\_a\")[0] # Application logic prompt = prompt\\_template.format(instructions=instructions.value[\"instructions\"]) ... # Node that updates instructions def update\\_instructions(state: State, store: BaseStore): namespace = (\"instructions\",) instructions = store.search(namespace)[0] # Memory logic prompt = prompt\\_template.format(instructions=instructions.value[\"instructions\"], conversation=state[\"messages\"]) output = llm.invoke(prompt) new\\_instructions = output['new\\_instructions'] store.put((\"agent\\_instructions\",), \"agent\\_a\", {\"instructions\": new\\_instructions}) ... ``` ### Writing memories There are two primary methods for agents to write memories: [\"in the hot path\"](#in-the-hot-path) and [\"in the background\"](#in-the-background). #### In the hot path Creating memories during runtime offers both advantages and challenges. On the positive side, this approach allows for real-time updates, making new memories immediately available for use in subsequent interactions. It also enables transparency, as users can be notified when memories are created and stored. However, this method also presents challenges. It may increase complexity if the agent requires a new tool to decide what to commit to memory. In addition, the process of reasoning about what to save to memory can impact agent latency. Finally, the agent must multitask between memory creation and its other responsibilities, potentially affecting the quantity and quality of memories created. As an example, ChatGPT uses a [save\\\\_memories](https://openai.com/index/memory-and-new-controls-for-chatgpt/) tool to upsert memories as content strings, deciding whether and how to use this tool with each user message. See our [memory-agent](https://github.com/langchain-ai/memory-agent) template as an reference implementation. #### In the background Creating memories as a separate background task offers several advantages. It eliminates latency in the primary application, separates application logic from memory management, and allows for more focused task completion by the agent. This approach also provides flexibility in timing memory creation to avoid redundant work. However, this method has its own challenges. Determining the frequency of memory writing becomes crucial, as infrequent updates may leave other threads without new context. Deciding when to trigger memory formation is also important. Common strategies include scheduling after a set time period (with rescheduling if new events occur), using a cron schedule, or allowing manual triggers by users or the application logic. See our [memory-service](https://github.com/langchain-ai/memory-template) template as an reference implementation. ### Memory storage LangGraph stores long-term memories as JSON documents in a [store](/oss/python/langgraph/persistence#memory-store). Each memory is organized under a custom `namespace` (similar to a folder) and a distinct `key` (like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information. This structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters. ```python theme={null} from langgraph.store.memory import InMemoryStore def embed(texts: list[str]) -> list[list[float]]: # Replace with an actual embedding function or LangChain embeddings object return [[1.0, 2.0] \\* len(texts)] # InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use. store = InMemoryStore(index={\"embed\": embed, \"dims\": 2}) user\\_id = \"my-user\" application\\_context = \"chitchat\" namespace = (user\\_id, application\\_context) store.put( namespace, \"a-memory\", { \"rules\": [ \"User likes short, direct language\", \"User only speaks English & python\", ], \"my-key\": \"my-value\", }, ) # get the \"memory\" by ID item = store.get(namespace, \"a-memory\") # search for \"memories\" within this namespace, filtering on content equivalence, sorted by vector similarity items = store.search( namespace, filter={\"my-key\": \"my-value\"}, query=\"language preferences\" ) ``` For more information about the memory store, see the [Persistence](/oss/python/langgraph/persistence#memory-store) guide. \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/concepts/memory.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/contributing/code.md",
      "title": null,
      "raw_content": "# Contributing to code Code contributions are always welcome! Whether you're fixing bugs, adding features, or improving performance, your contributions help deliver a better developer experience for thousands of developers. ## Getting started  Before submitting large \\*\\*new features or refactors\\*\\*, please first discuss your ideas in [the forum](https://forum.langchain.com/). This ensures alignment with project goals and prevents duplicate work. This does not apply to bugfixes or small improvements, which you can contribute directly via pull requests. See the quickstart guide below.  ### Quick fix: submit a bugfix For simple bugfixes, you can get started immediately:   Create a minimal test case that demonstrates the bug. Maintainers and other contributors should be able to run this test and see the failure without additional setup or modification   Fork the [LangChain](https://github.com/langchain-ai/langchain) or [LangGraph](https://github.com/langchain-ai/langgraph) repo to your personal GitHub account   ```bash theme={null} git clone https://github.com/your-username/name-of-forked-repo.git # For instance, for LangChain: git clone https://github.com/parrot123/langchain.git # For LangGraph: git clone https://github.com/parrot123/langgraph.git ``` ```bash theme={null} # Inside your repo, install dependencies uv sync --all-groups ``` You will need to install [`uv`](https://docs.astral.sh/uv/) if you haven't previously   Create a new branch for your fix. This helps keep your changes organized and makes it easier to submit a pull request later. ```bash theme={null} git checkout -b your-username/short-bugfix-name ```   Add [unit tests](#test-writing-guidelines) that will fail without your fix. This allows us to verify the bug is resolved and prevents regressions   Fix the bug while following our [code quality standards](#code-quality-standards). Make the \\*\\*minimal change necessary\\*\\* to resolve the issue   Ensure that tests pass and no regressions are introduced. Ensure all tests pass locally before submitting your PR ```bash theme={null} make lint make test # For bugfixes involving integrations, also run: make integration\\_tests # (You may need to set up API testing credentials) ```   Update docstrings if behavior changes, add comments for complex logic   Follow the PR template provided. If applicable, reference the issue you're fixing using a [closing keyword](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword) (e.g. `Fixes #ISSUE\\_NUMBER`) so that the issue is automatically closed when your PR is merged.   ### Full development setup For ongoing development or larger contributions: 1. Review our [contribution guidelines](#contribution-guidelines) for features, bugfixes, and integrations 2. Set up your environment following our [setup guide](#development-environment) below 3. Understand the [repository structure](#repository-structure) and package organization 4. Learn our [development workflow](#development-workflow) including testing and linting \\*\\*\\* ## Contribution guidelines Before you start contributing to LangChain, take a moment to think about why you want to. If your only goal is to add a \"first contribution\" to your resume (or if you're just looking for a quick win) you might be better off doing a boot-camp or an online tutorial. Contributing to open source projects takes time and effort, but it can also help you become a better developer and learn new skills. However, it's important to know that it might be harder and slower than following a training course. That said, contributing to open source is worth it if you're willing to take the time to do things well. ### Backwards compatibility  Breaking changes to public APIs are not allowed except for critical security fixes. See our [versioning policy](/oss/python/versioning) for details on major version releases.  Maintain compatibility via:   \\*\\*Always preserve\\*\\*: \\* Function signatures and parameter names \\* Class interfaces and method names \\* Return value structure and types \\* Import paths for public APIs   \\*\\*Acceptable modifications\\*\\*: \\* Adding new optional parameters \\* Adding new methods to classes \\* Improving performance without changing behavior \\* Adding new modules or functions   \\* \\*\\*Would this break existing user code?\\*\\* \\* Check if your target is public \\* If needed, is it exported in `\\_\\_init\\_\\_.py`? \\* Are there existing usage patterns in tests?   ### New features We aim to keep the bar high for new features. We generally don't accept new core abstractions from outside contributors without an existing issue that demonstrates an acute need for them. This also applies to changes to infra and dependencies. In general, feature contribution requirements include:   Open an issue describing: \\* The problem you're solving \\* Proposed API design \\* Expected usage patterns   \\* Follow existing code patterns \\* Include comprehensive tests and documentation \\* Consider security implications   \\* How does this interact with existing features? \\* Are there performance implications? \\* Does this introduce new dependencies? We will reject features that are likely to lead to security vulnerabilities or reports.   ### Security guidelines  Security is paramount. Never introduce vulnerabilities or unsafe patterns.  Security checklist:   \\* Validate and sanitize all user inputs \\* Properly escape data in templates and queries \\* Never use `eval()`, `exec()`, or `pickle` on user data, as this can lead to arbitrary code execution vulnerabilities   \\* Use specific exception types \\* Don't expose sensitive information in error messages \\* Implement proper resource cleanup   \\* Avoid adding hard dependencies \\* Keep optional dependencies minimal \\* Review third-party packages for security issues   \\*\\*\\* ## Development environment  Our Python projects use [`uv`](https://docs.astral.sh/uv/getting-started/installation/) for dependency management. Make sure you have the latest version installed.  Once you've reviewed the [contribution guidelines](#contribution-guidelines), set up a development environment for the package(s) you're working on.     For changes to `langchain-core`: ```bash theme={null} cd libs/core uv sync --all-groups make test # Ensure tests pass before starting development ```   For changes to `langchain`: ```bash theme={null} cd libs/langchain uv sync --all-groups make test # Ensure tests pass before starting development ```   For changes to [partner integrations](/oss/python/integrations/providers/overview): ```bash theme={null} cd libs/partners/langchain-{partner} uv sync --all-groups make test # Ensure tests pass before starting development ```   For changes to community integrations (located in a [separate repo](https://github.com/langchain-ai/langchain-community)): ```bash theme={null} cd libs/community/langchain\\_community/path/to/integration uv sync --all-groups make test # Ensure tests pass before starting development ```     WIP - coming soon! In the meantime, follow instructions for LangChain.   \\*\\*\\* ## Repository structure   LangChain is organized as a monorepo with multiple packages:   \\* \\*\\*[`langchain`](https://github.com/langchain-ai/langchain/tree/master/libs/langchain#readme)\\*\\* (located in `libs/langchain/`): Main package with chains, agents, and retrieval logic \\* \\*\\*[`langchain-core`](https://github.com/langchain-ai/langchain/tree/master/libs/core#readme)\\*\\* (located in `libs/core/`): Base interfaces and core abstractions   Located in `libs/partners/`, these are independently versioned packages for specific integrations. For example: \\* \\*\\*[`langchain-openai`](https://github.com/langchain-ai/langchain/tree/master/libs/partners/openai#readme)\\*\\*: [OpenAI](/oss/python/integrations/providers/openai) integrations \\* \\*\\*[`langchain-anthropic`](https://github.com/langchain-ai/langchain/tree/master/libs/partners/anthropic#readme)\\*\\*: [Anthropic](/oss/python/integrations/providers/anthropic) integrations \\* \\*\\*[`langchain-google-genai`](https://github.com/langchain-ai/langchain-google/)\\*\\*: [Google Generative AI](/oss/python/integrations/chat/google\\_generative\\_ai) integrations Many partner packages are in external repositories. Please check the [list of integrations](/oss/python/integrations/providers/overview) for details.   \\* \\*\\*[`langchain-text-splitters`](https://github.com/langchain-ai/langchain/tree/master/libs/text-splitters#readme)\\*\\*: Text splitting utilities \\* \\*\\*[`langchain-standard-tests`](https://github.com/langchain-ai/langchain/tree/master/libs/standard-tests#readme)\\*\\*: Standard test suites for integrations \\* \\*\\*[`langchain-cli`](https://github.com/langchain-ai/langchain/tree/master/libs/cli#readme)\\*\\*: Command line interface \\* \\*\\*[`langchain-community`](https://github.com/langchain-ai/langchain-community)\\*\\*: Community maintained integrations (located in a separate repo)     WIP - coming soon! In the meantime, follow instructions for LangChain.   \\*\\*\\* ## Development workflow ### Testing requirements  Directories are relative to the package you're working in.  Every code change must include comprehensive tests. #### Unit tests \\*\\*Location\\*\\*: `tests/unit\\_tests/` \\*\\*Requirements\\*\\*: \\* No network calls allowed \\* Test all code paths including edge cases \\* Use mocks for external dependencies ```bash theme={null} make test # Or directly: uv run --group test pytest tests/unit\\_tests ``` #### Integration tests \\*\\*Location\\*\\*: `tests/integration\\_tests/` Integration tests require access to external services/ provider APIs (which can cost money) and therefore are not run by default. Not every code change will require an integration test, but keep in mind that we'll require/ run integration tests separately as apart of our review process. \\*\\*Requirements\\*\\*: \\* Test real integrations with external services \\* Use environment variables for API keys \\* Skip gracefully if credentials unavailable ```bash theme={null} make integration\\_tests ``` ### Code quality standards Contributions must adhere to the following quality requirements:   \\*\\*Required\\*\\*: Complete type annotations for all functions ```python theme={null} def process\\_documents( docs: list[Document], processor: DocumentProcessor, \\*, batch\\_size: int = 100 ) -> ProcessingResult: \"\"\"Process documents in batches. Args: docs: List of documents to process. processor: Document processing instance. batch\\_size: Number of documents per batch. Returns: Processing results with success/failure counts. \"\"\" ```   \\*\\*Required\\*\\*: [Google-style docstrings](https://google.github.io/styleguide/pyguide.html) for all public functions \\* Document all parameters and return values \\* Include usage examples for complex functions \\* Document raised exceptions \\* Focus on \"why\" rather than \"what\"   \\*\\*Automated\\*\\*: Formatting and linting via [`ruff`](https://docs.astral.sh/ruff/) ```bash theme={null} make format # Apply formatting make lint # Check style and types ``` \\*\\*Standards\\*\\*: \\* Descriptive variable names \\* Break up complex functions (aim for fewer than 20 lines) \\* Follow existing patterns in the codebase   \\*\\*\\* ## Testing and validation ### Running tests locally Before submitting your PR, ensure you have completed the following steps. Note that the requirements differ slightly between LangChain and LangGraph.     ```bash theme={null} make test ``` All unit tests must pass   ```bash theme={null} make integration\\_tests ``` (Run if your changes affect integrations)   ```bash theme={null} make format make lint ``` Code must pass all style checks   ```bash theme={null} make type\\_check ``` All type hints must be valid   Push your branch and open a pull request. Follow the provided form template. Note related issues using a [closing keyword](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword). After submitting, wait, and check to ensure the CI checks pass. If any checks fail, address the issues promptly - maintainers may close PRs that do not pass CI within a reasonable timeframe.     WIP - coming soon! In the meantime, follow instructions for LangChain.   ### Test writing guidelines In order to write effective tests, there's a few good practices to follow: \\* Use natural language to describe the test in docstrings \\* Use descriptive variable names \\* Be exhaustive with assertions   ```python theme={null} def test\\_document\\_processor\\_handles\\_empty\\_input(): \"\"\"Test processor gracefully handles empty document list.\"\"\" processor = DocumentProcessor() result = processor.process([]) assert result.success assert result.processed\\_count == 0 assert len(result.errors) == 0 ```   ```python theme={null} @pytest.mark.requires(\"openai\") def test\\_openai\\_chat\\_integration(): \"\"\"Test OpenAI chat integration with real API.\"\"\" chat = ChatOpenAI() response = chat.invoke(\"Hello\") assert isinstance(response.content, str) assert len(response.content) > 0 ```   ```python theme={null} def test\\_retry\\_mechanism(mocker): \"\"\"Test retry mechanism handles transient failures.\"\"\" mock\\_client = mocker.Mock() mock\\_client.call.side\\_effect = [ ConnectionError(\"Temporary failure\"), {\"result\": \"success\"} ] service = APIService(client=mock\\_client) result = service.call\\_with\\_retry() assert result[\"result\"] == \"success\" assert mock\\_client.call.call\\_count == 2 ```   ## Getting help Our goal is to have the most accessible developer setup possible. Should you experience any difficulty getting setup, please ask in the [community slack](https://www.langchain.com/join-community) or open a [forum post](https://forum.langchain.com/).  You're now ready to contribute high-quality code to LangChain!  \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/code.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/contributing/comarketing.md",
      "title": null,
      "raw_content": "# Co-marketing With over 60 million monthly downloads, LangChain has a large audience of developers building LLM applications. Beyond just listing integrations, we aim to highlight high-quality, educational examples that inspire developers and advance the ecosystem.  While we occasionally share integrations, we prioritize content that provides meaningful insights and best practices. Our main social channels are [Twitter](https://x.com/LangChainAI) and [LinkedIn](https://www.linkedin.com/company/langchain/), where we highlight the best examples.  ### Content we're excited to promote   Blogs, YouTube videos and other media showcasing educational content. Note that we prefer content that is NOT framed as \"here's how to use integration XYZ\", but rather \"here's how to do ABC\", as we find that is more educational and helpful for developers.   End-to-end applications are great resources for developers looking to build. We prefer to highlight applications that are more complex/agentic in nature, and that use [LangGraph](https://github.com/langchain-ai/langgraph) as the orchestration framework. We get particularly excited about anything involving: \\* Long-term memory systems \\* Human-in-the-loop interaction patterns \\* Multi-agent architectures   We love highlighting novel research! Whether it is research built on top of LangChain or that integrates with it.   \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/comarketing.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/contributing/documentation.md",
      "title": null,
      "raw_content": "# Contributing to documentation Accessible documentation is a vital part of LangChain. We welcome both documentation for new features and [integrations](/oss/python/contributing/publish-langchain#adding-documentation), as well as community improvements to existing docs.  These are contribution guidelines for our open source projects, but they also apply to the [LangSmith documentation](/langsmith/home).  ## Contribute ### Quick edits For quick changes like fixing typos or changing a link, you can edit directly on GitHub without setting up a local development environment:  \\*\\*Prerequisites:\\*\\* \\* A [GitHub](https://github.com/) account \\* Basic familiarity of the [fork-and-pull workflow](https://graphite.dev/guides/understanding-git-fork-pull-request-workflow) for contributing  1. At the bottom of the page you want to edit, click the link \\*\\*Edit the source of this page on GitHub\\*\\*. 2. GitHub will prompt you to fork the repository to your account. Make sure to fork into your personal account. 3. Make the changes directly in GitHub's web editor. 4. Click \\*\\*Commit changes...\\*\\* and give your commit a descriptive title like `fix(docs): summary of change`. If applicable, add an [extended description](https://www.gitkraken.com/learn/git/best-practices/git-commit-message#git-commit-message-structure). 5. GitHub will redirect you to create a pull request. Give it a title (often the same as the commit) and follow the PR template checklist, if present.  Docs PRs are typically reviewed within a few days. Keep an eye on your PR to address any feedback from maintainers. Do not bump the PR unless you have new information to provide - maintainers will address it as their availability permits.  ### Larger edits and additions For larger changes, additions, or ongoing contributions, it's important to set up a local development environment on your machine. Our documentation build pipeline offers local preview and live reload as you edit, important for ensuring your changes appear as intended before submitting. #### Set up local environment Before you can work on this project, ensure you have the following installed: \\*\\*Required:\\*\\* \\* \\*\\*Python\\*\\* >= 3.13.0, \\< 4.0.0 \\* \\*\\*[`uv`](https://docs.astral.sh/uv/)\\*\\* - Python package manager (used for dependency management) \\* \\*\\*Node.js\\*\\* and \\*\\*npm\\*\\* - For Mintlify CLI and reference documentation builds \\* \\*\\*Make\\*\\* - For running build commands (usually pre-installed on macOS/Linux) \\* \\*\\*Git\\*\\* - For version control \\*\\*Optional but recommended:\\*\\* \\* \\*\\*[`markdownlint-cli`](https://github.com/igorshubovych/markdownlint-cli)\\*\\* - For linting markdown files ```bash theme={null} npm install -g markdownlint-cli ``` \\* \\*\\*[`pnpm`](https://pnpm.io/)\\*\\* - Required only if you're working on reference documentation ```bash theme={null} npm install -g pnpm@10.14.0 ``` \\*\\*Setup steps:\\*\\* 1. Clone the [`langchain-ai/docs`](https://github.com/langchain-ai/docs) repo. Follow the steps outlined in [`IDE\\_SETUP.md`](https://github.com/langchain-ai/docs/blob/main/IDE\\_SETUP.md). 2. Install dependencies: ```bash theme={null} make install ``` This command will: \\* Install Python dependencies using `uv sync --all-groups` \\* Install Mintlify CLI globally via npm 3. Verify your setup: ```bash theme={null} make build ``` This should build the documentation without errors. After install, you'll have access to the `docs` command: ```bash theme={null} docs --help ``` Common commands: \\* `docs dev` - Start development mode with file watching and hot reload \\* `docs build` - Build documentation See [Available commands](#available-commands) for more details. #### Edit documentation  \\*\\*Only edit files in `src/`\\*\\* \u2013 The `build/` directory is automatically generated.  1. Ensure your [dev environment is set up](#set-up-local-environment) and that you have followed the steps in [IDE\\\\_SETUP.md](https://github.com/langchain-ai/docs/blob/main/IDE\\_SETUP.md) to configure your IDE/editor to automatically apply the correct settings. 2. Edit files in `src/`: \\* Make changes to markdown files and the build system will automatically detect changes and rebuild affected files. \\* If OSS content varies between Python and JavaScript/TypeScript, add content for [both in the same file](#co-locate-python-and-javascripttypescript-oss-content). Otherwise, content will be identical for both languages. \\* Use [Mintlify syntax](https://mintlify.com/docs) for formatting. 3. Start development mode to preview changes locally: ```bash theme={null} docs dev ``` This starts a development server with hot reload at `http://localhost:3000`. 4. Iterate: \\* Continue editing and see changes reflected immediately. \\* The development server rebuilds only changed files for faster feedback. 5. Run the [quality checks](#run-quality-checks) to ensure your changes are valid. 6. Get approval from the relevant reviewers. To generate a preview build, [create a sharable preview build](#create-a-sharable-preview-build) (internal team members only). 7. [Publish to production](#publish-to-prod) (internal team members only). #### Create a sharable preview build  Only internal team members can create sharable preview builds.  When you create or update a PR, a [preview branch/ID](https://github.com/langchain-ai/docs/actions/workflows/create-preview-branch.yml) is automatically generated for you. A comment will be left on the PR with the ID, which you can then use to generate a preview. You can also run this workflow manually if needed. Previews are useful for sharing work-in-progress changes with others. 1. Copy the preview branch's ID from the comment. 2. In the [Mintlify dashboard](https://dashboard.mintlify.com/langchain-5e9cc07a/langchain-5e9cc07a?section=previews), click \\*\\*Create preview deployment\\*\\*. 3. Enter the preview branch's ID. 4. Click \\*\\*Create deployment\\*\\*. A \\*\\*Manual update\\*\\* will display in the \\*\\*Previews\\*\\* table. 5. Select the preview and click \\*\\*Visit\\*\\* to view the preview build. To redeploy the preview build, click \\*\\*Redeploy\\*\\* on the Mintlify dashboard. #### Run quality checks Before submitting changes, ensure your code passes formatting and linting checks: ```bash theme={null} # Check broken links make mint-broken-links # Format code automatically make format # Check for linting issues make lint # Fix markdown issues make lint\\_md\\_fix # Run tests to ensure your changes don't break existing functionality make test ``` For more details, see the [available commands](https://github.com/langchain-ai/docs?tab=readme-ov-file#available-commands) section in the README.  All pull requests are automatically checked by CI/CD. The same linting and formatting standards will be enforced, and PRs cannot be merged if these checks fail.  #### Publish to prod  Only internal team members can publish to production.  Once your branch has been merged into `main`, you need to push the changes to `prod` for them to render on the live docs site. Use the [Publish documentation GH action](https://github.com/langchain-ai/docs/actions/workflows/publish.yml): 1. Go to [Publish documentation](https://github.com/langchain-ai/docs/actions/workflows/publish.yml). 2. Click the \\*\\*Run workflow\\*\\* button. 3. Select the \\*\\*main\\*\\* branch to deploy. 4. Click \\*\\*Run workflow\\*\\*. ## Documentation types All documentation falls under one of four categories:   Task-oriented instructions for users who know what they want to accomplish.   Explanations that provide deeper understanding and insights.   Technical descriptions of APIs and implementation details.   Lessons that guide users through practical activities to build understanding.    Where applicable, all documentation must have both Python and JavaScript/TypeScript content. For more details, see the [co-locate Python and JavaScript/TypeScript content](#co-locate-python-and-javascripttypescript-content) section.  ### How-to guides How-to guides are task-oriented instructions for users who know what they want to accomplish. Examples of how-to guides are on the [LangChain](/oss/python/langchain/overview) and [LangGraph](/oss/python/langgraph/overview) tabs.   \\* \\*\\*Task-focused\\*\\*: Focus on a specific task or problem \\* \\*\\*Step-by-step\\*\\*: Break down the task into smaller steps \\* \\*\\*Hands-on\\*\\*: Provide concrete examples and code snippets   \\* Focus on the \\*\\*how\\*\\* rather than the \\*\\*why\\*\\* \\* Use concrete examples and code snippets \\* Break down the task into smaller steps \\* Link to related conceptual guides and references   \\* [Messages](/oss/python/langchain/messages) \\* [Tools](/oss/python/langchain/tools) \\* [Streaming](/oss/python/langgraph/streaming)   ### Conceptual guides Conceptual guide cover core concepts abstractly, providing deep understanding.   \\* \\*\\*Understanding-focused\\*\\*: Explain why things work as they do \\* \\*\\*Broad perspective\\*\\*: Higher and wider view than other types \\* \\*\\*Design-oriented\\*\\*: Explain decisions and trade-offs \\* \\*\\*Context-rich\\*\\*: Use analogies and comparisons   \\* Focus on the \\*\\*\"why\"\\*\\* rather than the \"how\" \\* Provides supplementary information not necessarily required for feature usage \\* Can use analogies and reference alternatives \\* Avoid blending in too much reference content \\* Link to related tutorials and how-to guides   \\* [Memory](/oss/python/concepts/memory) \\* [Context](/oss/python/concepts/context) \\* [Graph API](/oss/python/langgraph/graph-api) \\* [Functional API](/oss/python/langgraph/functional-api)   ### Reference Reference documentation contains detailed, low-level information describing exactly what functionality exists and how to use it.   A good reference should: \\* Describe what exists (all parameters, options, return values) \\* Be comprehensive and structured for easy lookup \\* Serve as the authoritative source for technical details   See the contributing guide for [Python reference docs](https://github.com/langchain-ai/docs/blob/main/reference/python/README.md).   \\* \\*\\*Be consistent\\*\\*; follow existing patterns for provider-specific documentation \\* Include both basic usage (code snippets) and common edge cases/failure modes \\* Note when features require specific versions   \\* New integrations or providers need dedicated reference pages \\* Complex configuration options require detailed explanation \\* API changes introduce new parameters or behavior \\* Community frequently asks questions about specific functionality   ### Tutorials Tutorials are longer form step-by-step guides that builds upon itself and takes users through a specific practical activity to build understanding. Tutorials are typically found on the [Learn](/oss/python/learn) tab.  We generally do not merge new tutorials from outside contributors without an acute need. If you feel that a certain topic is missing from docs or is not sufficiently covered, please [open a new issue](https://github.com/langchain-ai/docs/issues).    \\* \\*\\*Practical\\*\\*: Focus on practical activities to build understanding. \\* \\*\\*Step-by-step\\*\\*: Break down the activity into smaller steps. \\* \\*\\*Hands-on\\*\\*: Provide sequential, working code snippets. \\* \\*\\*Supplementary\\*\\*: Provide additional context and information not necessarily required for feature usage.   \\* Code snippets should be sequential and working if the user follows the steps in order. \\* Provide some context for the activity, but link to related conceptual guides and references for more detailed information.   \\* [Semantic search](/oss/python/langchain/knowledge-base) \\* [RAG agent](/oss/python/langchain/rag)   ## Writing standards  Reference documentation has different standards - see the [reference docs contributing guide](https://github.com/langchain-ai/docs/blob/main/reference/python/README.md) for details.  ### Mintlify components Use [Mintlify components](https://mintlify.com/docs/text) to enhance readability:   \\* `` for helpful supplementary information \\* `` for important cautions and breaking changes \\* `` for best practices and advice \\* `` for neutral contextual information \\* `` for success confirmations   \\* `` for an overview of sequential procedures. \\*\\*Not\\*\\* for long lists of steps or tutorials. \\* `` for platform-specific content. \\* `` and `` for nice-to-have information that can be collapsed by default (e.g., full code examples). \\* `` and `` for highlighting content.   \\* `` for multiple language examples. \\* Always specify language tags on code blocks (e.g., ` ```python`, ` ```javascript`). \\* Titles for code blocks (e.g. `Success`, `Error Response`)   ### Page structure Every documentation page must begin with YAML frontmatter: ```yaml theme={null} --- title: \"Clear, specific title\" sidebarTitle: \"Short title for the sidebar (optional)\" --- ``` ### Co-locate Python and JavaScript/TypeScript content All documentation must be written in both Python and JavaScript/TypeScript when possible. To do so, we use a custom in-line syntax to differentiate between sections that should appear in one or both languages: ```mdx theme={null} :::python Python-specific content. In real docs, the preceding backslash (before `python`) is omitted. ::: :::js JavaScript/TypeScript-specific content. In real docs, the preceding backslash (before `js`) is omitted. ::: Content for both languages (not wrapped) ``` This will generate two outputs (one for each language) at `/oss/python/concepts/foo.mdx` and `/oss/javascript/concepts/foo.mdx`. Each outputted page will need to be added to the `/src/docs.json` file to be included in the navigation.  We don't want a lack of parity to block contributions. If a feature is only available in one language, it's okay to have documentation only in that language until the other language catches up. In such cases, please include a note indicating that the feature is not yet available in the other language. If you need help translating content between Python and JavaScript/TypeScript, please ask in the [community slack](https://www.langchain.com/join-community) or tag a maintainer in your PR.  ## Quality standards ### General guidelines   Multiple pages covering the same material are difficult to maintain and cause confusion. There should be only one canonical page for each concept or feature. Link to other guides instead of re-explaining.   Documentation sections don't exist in a vacuum. Link to other sections frequently to allow users to learn about unfamiliar topics. This includes linking to API references and conceptual sections.   Take a less-is-more approach. If another section with a good explanation exists, link to it rather than re-explain, unless your content presents a new angle.   ### Accessibility requirements Ensure documentation is accessible to all users: \\* Structure content for easy scanning with headers and lists \\* Use specific, actionable link text instead of \"click here\" \\* Include descriptive alt text for all images and diagrams ## Get help Our goal is to have the simplest developer setup possible. Should you experience any difficulty getting setup, please ask in the [community slack](https://www.langchain.com/join-community) or open a [forum post](https://forum.langchain.com/). Internal team members can reach out in the [#documentation](https://langchain.slack.com/archives/C04GWPE38LV) Slack channel. \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/documentation.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/contributing/implement-langchain.md",
      "title": null,
      "raw_content": "# Implement a LangChain integration Integration packages are Python packages that users can install for use in their projects. They implement one or more components that adhere to the LangChain interface standards. LangChain components are subclasses of base classes in [`langchain-core`](https://github.com/langchain-ai/langchain/tree/master/libs/core). Examples include [chat models](/oss/python/integrations/chat), [tools](/oss/python/integrations/tools), [retrievers](/oss/python/integrations/retrievers), and more. Your integration package will typically implement a subclass of at least one of these components. Expand the tabs below to see details on each.   Chat models are subclasses of the [`BaseChatModel`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.language\\_models.chat\\_models.BaseChatModel) class. They implement methods for generating chat completions, handling message formatting, and managing model parameters.  The chat model integration guide is currently WIP. In the meantime, read the [chat model conceptual guide](/oss/python/langchain/models) for details on how LangChain chat models function.    Tools are used in 2 main ways: 1. To define an \"input schema\" or \"args schema\" to pass to a chat model's tool calling feature along with a text request, such that the chat model can generate a \"tool call\", or parameters to call the tool with. 2. To take a \"tool call\" as generated above, and take some action and return a response that can be passed back to the chat model as a ToolMessage. The Tools class must inherit from the [`BaseTool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.BaseTool) base class. This interface has 3 properties and 2 methods that should be implemented in a subclass.  The tools integration guide is currently WIP. In the meantime, read the [tools conceptual guide](/oss/python/langchain/tools) for details on how LangChain tools function.    Retrievers are used to retrieve documents from APIs, databases, or other sources based on a query. The Retriever class must inherit from the BaseRetriever base class.  The retriever integration guide is currently WIP. In the meantime, read the [retriever conceptual guide](/oss/python/integrations/retrievers) for details on how LangChain retrievers function.    All vector stores must inherit from the [`VectorStore`](https://reference.langchain.com/python/langchain\\_core/vectorstores/?h=#langchain\\_core.vectorstores.base.VectorStore) base class. This interface consists of methods for writing, deleting and searching for documents in the vector store. See the [vector store integration guide](/oss/python/integrations/vectorstores) for details on implementing a vector store integration.  The vector store integration guide is currently WIP. In the meantime, read the [vector store conceptual guide](/oss/python/integrations/vectorstores) for details on how LangChain vector stores function.    Embedding models are subclasses of the [`Embeddings`](https://reference.langchain.com/python/langchain\\_core/embeddings/#langchain\\_core.embeddings.embeddings.Embeddings) class.  The embedding model integration guide is currently WIP. In the meantime, read the [embedding model conceptual guide](/oss/python/integrations/text\\_embedding) for details on how LangChain embedding models function.    \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/implement-langchain.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/contributing/integrations-langchain.md",
      "title": null,
      "raw_content": "# Contributing integrations \\*\\*Integrations are a core component of LangChain.\\*\\* LangChain provides standard interfaces for several different components (language models, vector stores, etc) that are crucial when building LLM applications. Contributing an integration helps expand LangChain's ecosystem and makes your service discoverable to millions of developers. ## Why contribute an integration to LangChain?  LangChain is the most used framework for building LLM applications, with over 20 million monthly downloads.   LangChain components expose a standard interface, allowing developers to easily swap them for each other. If you implement a LangChain integration, any developer using a different component will easily be able to swap yours in.   Through their standard interface, LangChain components encourage and facilitate best practices (streaming, async, etc.) that improve developer experience and application performance.  ## Components to integrate While any component can be integrated into LangChain, there are specific types of integrations we encourage more: \\*\\*Integrate these \u2705\\*\\*: \\* [\\*\\*Chat Models\\*\\*](/oss/python/integrations/chat): Most actively used component type \\* [\\*\\*Tools/Toolkits\\*\\*](/oss/python/integrations/tools): Enable agent capabilities \\* [\\*\\*Retrievers\\*\\*](/oss/python/integrations/retrievers): Core to RAG applications \\* [\\*\\*Embedding Models\\*\\*](/oss/python/integrations/text\\_embedding): Foundation for vector operations \\* [\\*\\*Vector Stores\\*\\*](/oss/python/integrations/vectorstores): Essential for semantic search \\*\\*Not these \u274c\\*\\*: \\* \\*\\*LLMs (Text-Completion Models)\\*\\*: Deprecated in favor of [Chat Models](/oss/python/integrations/chat) \\* [\\*\\*Document Loaders\\*\\*](/oss/python/integrations/document\\_loaders): High maintenance burden \\* [\\*\\*Key-Value Stores\\*\\*](/oss/python/integrations/stores): Limited usage \\* \\*\\*Document Transformers\\*\\*: Niche use cases \\* \\*\\*Model Caches\\*\\*: Infrastructure concerns \\* \\*\\*Graphs\\*\\*: Complex abstractions \\* \\*\\*Message Histories\\*\\*: Storage abstractions \\* \\*\\*Callbacks\\*\\*: System-level components \\* \\*\\*Chat Loaders\\*\\*: Limited demand \\* \\*\\*Adapters\\*\\*: Edge case utilities ## How to contribute an integration   Verify that your integration is in the list of [encouraged components](#components-to-integrate) we are currently accepting.     If applicable, implement support for LangChain's [standard test](/oss/python/contributing/standard-tests-langchain) suite for your integration and successfully run them.     Open a PR to add documentation for your integration to the official LangChain docs.  An integration is only as useful as its documentation. To ensure a consistent experience for users, docs are required for all new integrations. We have a standard starting-point template for each type of integration for you to copy and modify. In a new PR to the LangChain [docs repo](https://github.com/langchain-ai/docs), create a new file in the relevant directory under `src/oss/python/integrations//integration\\_name.mdx` using the appropriate template file: \\* [Chat models](https://github.com/langchain-ai/docs/blob/main/src/oss/python/integrations/chat/TEMPLATE.mdx) \\* [Tools and toolkits](https://github.com/langchain-ai/docs/blob/main/src/oss/python/integrations/tools/TEMPLATE.mdx) \\* [Retrievers](https://github.com/langchain-ai/docs/blob/main/src/oss/python/integrations/retrievers/TEMPLATE.mdx) \\* Text splitters - Coming soon \\* Embedding models - Coming soon \\* [Vector stores](https://github.com/langchain-ai/docs/blob/main/src/oss/python/integrations/vectorstores/TEMPLATE.mdx) \\* Document loaders - Coming soon \\* Key-value stores - Coming soon For reference docs, please open an issue on the repo so that a maintainer can add them.    (Optional) Engage with the LangChain team for joint [co-marketing](/oss/python/contributing/comarketing).   \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/integrations-langchain.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/contributing/overview.md",
      "title": null,
      "raw_content": "# Contributing \\*\\*Welcome! Thank you for your interest in contributing.\\*\\* LangChain has helped form the largest developer community in generative AI, and we're always open to new contributors. Whether you're fixing bugs, adding features, improving documentation, or sharing feedback, your involvement helps make LangChain and LangGraph better for everyone \ud83e\udd9c\u2764\ufe0f ## Ways to Contribute   Found a bug? Please help us fix it by following these steps:   Check if the issue already exists in our GitHub Issues for the respective repo:  Issues Issues    If no issue exists, create a new one. When writing, be sure to follow the template provided and to include a [minimal, reproducible, example](https://stackoverflow.com/help/minimal-reproducible-example). Attach any relevant labels to the final issue once created. If a project maintainer is unable to reproduce the issue, it is unlikely to be addressed in a timely manner.   A project maintainer will triage the issue and may ask for additional information. Please be patient as we manage a high volume of issues. Do not bump the issue unless you have new information to provide.   If you are adding an issue, please try to keep it focused on a single topic. If two issues are related, or blocking, please [link them](https://docs.github.com/en/issues/tracking-your-work-with-issues/using-issues/linking-a-pull-request-to-an-issue#linking-a-pull-request-to-an-issue-using-a-keyword) rather than combining them. For example, ```text theme={null} This issue is blocked by #123 and related to #456. ```   Have an idea for a new feature or enhancement?   Search the issues for the respective repository for existing feature requests:  Issues Issues    If no requests exist, start a new discussion under the [relevant category](https://forum.langchain.com/c/help/langchain/14) so that project maintainers and the community can provide feedback.   Be sure to describe the use case and why it would be valuable to others. If possible, provide examples or mockups where applicable. Outline test cases that should pass.     Documentation improvements are always welcome! We strive to keep our docs clear and comprehensive, and your perspective can make a big difference. Guide   With a large userbase, it can be hard for our small team to keep up with all the feature requests and bug fixes. If you have the skills and time, we would love your help! Guide If you start working on an issue, please assign it to yourself or ask a maintainer to do so. This helps avoid duplicate work. If you are looking for something to work on, check out the issues labeled \"good first issue\" or \"help wanted\" in our repos:  Labels Labels    Guide to adding a new LangChain integration   ## Acceptable uses of LLMs Generative AI can be a useful tool for contributors, but like any tool should be used with critical thinking and good judgement. We struggle when contributors' entire work (code changes, documentation update, pull request descriptions) are LLM-generated. These drive-by contributions often mean well but often miss the mark in terms of contextual relevance, accuracy, and quality. We will close those pull requests and issues that are unproductive, so we can focus our maintainer capacity elsewhere. \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/overview.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/contributing/publish-langchain.md",
      "title": null,
      "raw_content": "# Publish an integration \\*\\*Make your integration available to the community.\\*\\*  \\*\\*Important: New integrations should be standalone packages, not PRs to the LangChain monorepo.\\*\\* While LangChain maintains a small subset of first-party and high-usage integrations (like OpenAI, Anthropic, and Ollama) in the main repository, \\*\\*new integrations should be published as separate PyPI packages and repositories\\*\\* (e.g., `langchain-yourservice`) that users install alongside the core LangChain packages. You \\*\\*should not\\*\\* submit a PR to add your integration directly to the main LangChain repository.  Now that your package is implemented and tested, you can publish it and add documentation to make it discoverable by the community. ## Publishing your package  This guide assumes you have already implemented your package and written tests for it. If you haven't, please refer to the [implementation guide](/oss/python/contributing/implement-langchain) and [testing guide](/oss/python/contributing/standard-tests-langchain).  For the purposes of this guide, we'll be using PyPI as the package registry. You may choose to publish to other registries if you prefer; instructions will vary. ### Setup credentials First, make sure you have a PyPI account:     Go to the [PyPI website](https://pypi.org/) and create an account   Verify your email address by clicking the link that PyPI emails to you   Go to your account settings and click \"Generate Recovery Codes\" to enable 2FA. To generate an API token, you \\*\\*must\\*\\* have 2FA enabled   Go to your account settings and [generate a new API token](https://pypi.org/manage/account/token/)     ### Build and publish  Helpful guide from `uv` on how to build and publish a package to PyPI.  ## Adding documentation To add documentation for your package to this site under the [integrations tab](/oss/python/integrations/providers/overview), you will need to create the relevant documentation pages and open a PR in the [LangChain docs repository](https://github.com/langchain-ai/docs). ### Writing docs Depending on the type of integration you have built, you will need to create different types of documentation pages. LangChain provides templates for different types of integrations to help you get started.    To reference existing documentation, you can look at the [list of integrations](/oss/python/integrations/providers/overview) and find similar ones to yours. To view a given documentation page in raw markdown, use the dropdown button next to \"Copy page\" on the top right of the page and select \"View as Markdown\".  ### Submitting a PR Make a fork of the [LangChain docs repository](https://github.com/langchain-ai/docs) under a personal GitHub account, and clone it locally. Create a new branch for your integration. Copy the template and modify them using your favorite markdown text editor. Make sure to refer to and follow the [documentation guide](/oss/python/contributing/documentation) when writing your documentation.  We may reject PRs or ask for modification if: \\* CI checks fail \\* Severe grammatical errors or typos are present \\* [Mintlify components](/oss/python/contributing/documentation#mintlify-components) are used incorrectly \\* Pages are missing a [frontmatter](/oss/python/contributing/documentation#page-structure) \\* [Localization](/oss/python/contributing/documentation#localization) is missing (where applicable) \\* [Code examples](/oss/python/contributing/documentation#in-code-documentation) do not run or have errors \\* [Quality standards](/oss/python/contributing/documentation#quality-standards) are not met  Please be patient as we handle a large volume of PRs. We will review your PR as soon as possible and provide feedback or merge it. \\*\\*Do not repeatedly tag maintainers about your PR.\\*\\* \\*\\*\\* ## Next steps \\*\\*Congratulations!\\*\\* Your integration is now published and documented, making it available to the entire LangChain community.  Get in touch with the LangChain marketing team to explore co-marketing opportunities.  \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/contributing/publish-langchain.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/deepagents/backends.md",
      "title": null,
      "raw_content": "# Backends > Choose and configure filesystem backends for deep agents. You can specify routes to different backends, implement virtual filesystems, and enforce policies. Deep agents expose a filesystem surface to the agent via tools like `ls`, `read\\_file`, `write\\_file`, `edit\\_file`, `glob`, and `grep`. These tools operate through a pluggable backend. ```mermaid theme={null} graph TB Tools[Filesystem Tools] --> Backend[Backend] Backend --> State[State] Backend --> Disk[Filesystem] Backend --> Store[Store] Backend --> Composite[Composite] Backend --> Custom[Custom] Composite --> Router{Routes} Router --> State Router --> Disk Router --> Store ``` This page explains how to [choose a backend](#specify-a-backend), [route different paths to different backends](#route-to-different-backends), [implement your own virtual filesystem](#use-a-virtual-filesystem) (e.g., S3 or Postgres), [add policy hooks](#add-policy-hooks), and [comply with the backend protocol](#protocol-reference). ## Quickstart Here are a few pre-built filesystem backends that you can quickly use with your deep agent: | Built-in backend | Description | | ---------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | | [Default](#statebackend-ephemeral) | `agent = create\\_deep\\_agent()`   \n Ephemeral in state. The default filesystem backend for an agent is stored in `langgraph` state. Note that this filesystem only persists \\*for a single thread\\*. | | [Local filesystem persistence](#filesystembackend-local-disk) | `agent = create\\_deep\\_agent(backend=FilesystemBackend(root\\_dir=\"/Users/nh/Desktop/\"))`   \nThis gives the deep agent access to your local machine's filesystem. You can specify the root directory that the agent has access to. Note that any provided `root\\_dir` must be an absolute path. | | [Durable store (LangGraph store)](#storebackend-langgraph-store) | `agent = create\\_deep\\_agent(backend=lambda rt: StoreBackend(rt))`   \nThis gives the agent access to long-term storage that is \\*persisted across threads\\*. This is great for storing longer term memories or instructions that are applicable to the agent over multiple executions. | | [Composite](#compositebackend-router) | Ephemeral by default, `/memories/` persisted. The Composite backend is maximally flexible. You can specify different routes in the filesystem to point towards different backends. See Composite routing below for a ready-to-paste example. | ## Built-in backends ### StateBackend (ephemeral) ```python theme={null} # By default we provide a StateBackend agent = create\\_deep\\_agent() # Under the hood, it looks like from deepagents.backends import StateBackend agent = create\\_deep\\_agent( backend=(lambda rt: StateBackend(rt)) # Note that the tools access State through the runtime.state ) ``` \\*\\*How it works:\\*\\* \\* Stores files in LangGraph agent state for the current thread. \\* Persists across multiple agent turns on the same thread via checkpoints. \\*\\*Best for:\\*\\* \\* A scratch pad for the agent to write intermediate results. \\* Automatic eviction of large tool outputs which the agent can then read back in piece by piece. ### FilesystemBackend (local disk) ```python theme={null} from deepagents.backends import FilesystemBackend agent = create\\_deep\\_agent( backend=FilesystemBackend(root\\_dir=\".\", virtual\\_mode=True) ) ``` \\*\\*How it works:\\*\\* \\* Reads/writes real files under a configurable `root\\_dir`. \\* You can optionally set `virtual\\_mode=True` to sandbox and normalize paths under `root\\_dir`. \\* Uses secure path resolution, prevents unsafe symlink traversal when possible, can use ripgrep for fast `grep`. \\*\\*Best for:\\*\\* \\* Local projects on your machine \\* CI sandboxes \\* Mounted persistent volumes ### StoreBackend (LangGraph Store) ```python theme={null} from langgraph.store.memory import InMemoryStore from deepagents.backends import StoreBackend agent = create\\_deep\\_agent( backend=(lambda rt: StoreBackend(rt)), # Note that the tools access Store through the runtime.store store=InMemoryStore() ) ``` \\*\\*How it works:\\*\\* \\* Stores files in a LangGraph `BaseStore` provided by the runtime, enabling cross\u2011thread durable storage. \\*\\*Best for:\\*\\* \\* When you already run with a configured LangGraph store (for example, Redis, Postgres, or cloud implementations behind `BaseStore`). \\* When you're deploying your agent through LangSmith Deployments (a store is automatically provisioned for your agent). ### CompositeBackend (router) ```python theme={null} from deepagents import create\\_deep\\_agent from deepagents.backends import CompositeBackend, StateBackend, StoreBackend from langgraph.store.memory import InMemoryStore composite\\_backend = lambda rt: CompositeBackend( default=StateBackend(rt), routes={ \"/memories/\": StoreBackend(rt), } ) agent = create\\_deep\\_agent( backend=composite\\_backend, store=InMemoryStore() # Store passed to create\\_deep\\_agent, not backend ) ``` \\*\\*How it works:\\*\\* \\* Routes file operations to different backends based on path prefix. \\* Preserves the original path prefixes in listings and search results. \\*\\*Best for:\\*\\* \\* When you want to give your agent both ephemeral and cross-thread storage, a CompositeBackend allows you provide both a StateBackend and StoreBackend \\* When you have multiple sources of information that you want to provide to your agent as part of a single filesystem. \\* e.g. You have long-term memories stored under /memories/ in one Store and you also have a custom backend that has documentation accessible at /docs/. ## Specify a backend \\* Pass a backend to `create\\_deep\\_agent(backend=...)`. The filesystem middleware uses it for all tooling. \\* You can pass either: \\* An instance implementing `BackendProtocol` (for example, `FilesystemBackend(root\\_dir=\".\")`), or \\* A factory `BackendFactory = Callable[[ToolRuntime], BackendProtocol]` (for backends that need runtime like `StateBackend` or `StoreBackend`). \\* If omitted, the default is `lambda rt: StateBackend(rt)`. ## Route to different backends Route parts of the namespace to different backends. Commonly used to persist `/memories/\\*` and keep everything else ephemeral. ```python theme={null} from deepagents import create\\_deep\\_agent from deepagents.backends import CompositeBackend, StateBackend, FilesystemBackend composite\\_backend = lambda rt: CompositeBackend( default=StateBackend(rt), routes={ \"/memories/\": FilesystemBackend(root\\_dir=\"/deepagents/myagent\", virtual\\_mode=True), }, ) agent = create\\_deep\\_agent(backend=composite\\_backend) ``` Behavior: \\* `/workspace/plan.md` \u2192 StateBackend (ephemeral) \\* `/memories/agent.md` \u2192 FilesystemBackend under `/deepagents/myagent` \\* `ls`, `glob`, `grep` aggregate results and show original path prefixes. Notes: \\* Longer prefixes win (for example, route `\"/memories/projects/\"` can override `\"/memories/\"`). \\* For StoreBackend routing, ensure the agent runtime provides a store (`runtime.store`). ## Use a virtual filesystem Build a custom backend to project a remote or database filesystem (e.g., S3 or Postgres) into the tools namespace. Design guidelines: \\* Paths are absolute (`/x/y.txt`). Decide how to map them to your storage keys/rows. \\* Implement `ls\\_info` and `glob\\_info` efficiently (server-side listing where available, otherwise local filter). \\* Return user-readable error strings for missing files or invalid regex patterns. \\* For external persistence, set `files\\_update=None` in results; only in-state backends should return a `files\\_update` dict. S3-style outline: ```python theme={null} from deepagents.backends.protocol import BackendProtocol, WriteResult, EditResult from deepagents.backends.utils import FileInfo, GrepMatch class S3Backend(BackendProtocol): def \\_\\_init\\_\\_(self, bucket: str, prefix: str = \"\"): self.bucket = bucket self.prefix = prefix.rstrip(\"/\") def \\_key(self, path: str) -> str: return f\"{self.prefix}{path}\" def ls\\_info(self, path: str) -> list[FileInfo]: # List objects under \\_key(path); build FileInfo entries (path, size, modified\\_at) ... def read(self, file\\_path: str, offset: int = 0, limit: int = 2000) -> str: # Fetch object; return numbered content or an error string ... def grep\\_raw(self, pattern: str, path: str | None = None, glob: str | None = None) -> list[GrepMatch] | str: # Optionally filter server\u2011side; else list and scan content ... def glob\\_info(self, pattern: str, path: str = \"/\") -> list[FileInfo]: # Apply glob relative to path across keys ... def write(self, file\\_path: str, content: str) -> WriteResult: # Enforce create\u2011only semantics; return WriteResult(path=file\\_path, files\\_update=None) ... def edit(self, file\\_path: str, old\\_string: str, new\\_string: str, replace\\_all: bool = False) -> EditResult: # Read \u2192 replace (respect uniqueness vs replace\\_all) \u2192 write \u2192 return occurrences ... ``` Postgres-style outline: \\* Table `files(path text primary key, content text, created\\_at timestamptz, modified\\_at timestamptz)` \\* Map tool operations onto SQL: \\* `ls\\_info` uses `WHERE path LIKE $1 || '%'` \\* `glob\\_info` filter in SQL or fetch then apply glob in Python \\* `grep\\_raw` can fetch candidate rows by extension or last modified time, then scan lines ## Add policy hooks Enforce enterprise rules by subclassing or wrapping a backend. Block writes/edits under selected prefixes (subclass): ```python theme={null} from deepagents.backends.filesystem import FilesystemBackend from deepagents.backends.protocol import WriteResult, EditResult class GuardedBackend(FilesystemBackend): def \\_\\_init\\_\\_(self, \\*, deny\\_prefixes: list[str], \\*\\*kwargs): super().\\_\\_init\\_\\_(\\*\\*kwargs) self.deny\\_prefixes = [p if p.endswith(\"/\") else p + \"/\" for p in deny\\_prefixes] def write(self, file\\_path: str, content: str) -> WriteResult: if any(file\\_path.startswith(p) for p in self.deny\\_prefixes): return WriteResult(error=f\"Writes are not allowed under {file\\_path}\") return super().write(file\\_path, content) def edit(self, file\\_path: str, old\\_string: str, new\\_string: str, replace\\_all: bool = False) -> EditResult: if any(file\\_path.startswith(p) for p in self.deny\\_prefixes): return EditResult(error=f\"Edits are not allowed under {file\\_path}\") return super().edit(file\\_path, old\\_string, new\\_string, replace\\_all) ``` Generic wrapper (works with any backend): ```python theme={null} from deepagents.backends.protocol import BackendProtocol, WriteResult, EditResult from deepagents.backends.utils import FileInfo, GrepMatch class PolicyWrapper(BackendProtocol): def \\_\\_init\\_\\_(self, inner: BackendProtocol, deny\\_prefixes: list[str] | None = None): self.inner = inner self.deny\\_prefixes = [p if p.endswith(\"/\") else p + \"/\" for p in (deny\\_prefixes or [])] def \\_deny(self, path: str) -> bool: return any(path.startswith(p) for p in self.deny\\_prefixes) def ls\\_info(self, path: str) -> list[FileInfo]: return self.inner.ls\\_info(path) def read(self, file\\_path: str, offset: int = 0, limit: int = 2000) -> str: return self.inner.read(file\\_path, offset=offset, limit=limit) def grep\\_raw(self, pattern: str, path: str | None = None, glob: str | None = None) -> list[GrepMatch] | str: return self.inner.grep\\_raw(pattern, path, glob) def glob\\_info(self, pattern: str, path: str = \"/\") -> list[FileInfo]: return self.inner.glob\\_info(pattern, path) def write(self, file\\_path: str, content: str) -> WriteResult: if self.\\_deny(file\\_path): return WriteResult(error=f\"Writes are not allowed under {file\\_path}\") return self.inner.write(file\\_path, content) def edit(self, file\\_path: str, old\\_string: str, new\\_string: str, replace\\_all: bool = False) -> EditResult: if self.\\_deny(file\\_path): return EditResult(error=f\"Edits are not allowed under {file\\_path}\") return self.inner.edit(file\\_path, old\\_string, new\\_string, replace\\_all) ``` ## Protocol reference Backends must implement the `BackendProtocol`. Required endpoints: \\* `ls\\_info(path: str) -> list[FileInfo]` \\* Return entries with at least `path`. Include `is\\_dir`, `size`, `modified\\_at` when available. Sort by `path` for deterministic output. \\* `read(file\\_path: str, offset: int = 0, limit: int = 2000) -> str` \\* Return numbered content. On missing file, return `\"Error: File '/x' not found\"`. \\* `grep\\_raw(pattern: str, path: Optional[str] = None, glob: Optional[str] = None) -> list[GrepMatch] | str` \\* Return structured matches. For an invalid regex, return a string like `\"Invalid regex pattern: ...\"` (do not raise). \\* `glob\\_info(pattern: str, path: str = \"/\") -> list[FileInfo]` \\* Return matched files as `FileInfo` entries (empty list if none). \\* `write(file\\_path: str, content: str) -> WriteResult` \\* Create-only. On conflict, return `WriteResult(error=...)`. On success, set `path` and for state backends set `files\\_update={...}`; external backends should use `files\\_update=None`. \\* `edit(file\\_path: str, old\\_string: str, new\\_string: str, replace\\_all: bool = False) -> EditResult` \\* Enforce uniqueness of `old\\_string` unless `replace\\_all=True`. If not found, return error. Include `occurrences` on success. Supporting types: \\* `WriteResult(error, path, files\\_update)` \\* `EditResult(error, path, files\\_update, occurrences)` \\* `FileInfo` with fields: `path` (required), optionally `is\\_dir`, `size`, `modified\\_at`. \\* `GrepMatch` with fields: `path`, `line`, `text`. \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/backends.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/deepagents/cli.md",
      "title": null,
      "raw_content": "# Deep Agents CLI > Interactive command-line interface for building with Deep Agents A terminal interface for building agents with persistent memory. Agents maintain context across sessions, learn project conventions, and execute code with approval controls. The Deep Agents CLI has the following built-in capabilities: \\* \\*\\*File operations\\*\\* - read, write, and edit files in your project with tools that enable agents to manage and modify code and documentation. \\* \\*\\*Shell command execution\\*\\* - execute shell commands to run tests, build projects, manage dependencies, and interact with version control systems. \\* \\*\\*Web search\\*\\* - search the web for up-to-date information and documentation (requires Tavily API key). \\* \\*\\*HTTP requests\\*\\* - make HTTP requests to APIs and external services for data fetching and integration tasks. \\* \\*\\*Task planning and tracking\\*\\* - break down complex tasks into discrete steps and track progress through the built-in todo system. \\* \\*\\*Memory storage and retrieval\\*\\* - store and retrieve information across sessions, enabling agents to remember project conventions and learned patterns. \\* \\*\\*Human-in-the-loop\\*\\* - require human approval for sensitive tool operations.  [Watch the demo video](https://youtu.be/IrnacLa9PJc?si=3yUnPbxnm2yaqVQb) to see how the Deep Agents CLI works.  ## Quick start   Export as an environment variable: ```bash theme={null} export ANTHROPIC\\_API\\_KEY=\"your-api-key\" ``` Or create a `.env` file in your project root: ```bash theme={null} ANTHROPIC\\_API\\_KEY=your-api-key ```   ```bash theme={null} uvx deepagents-cli ```   ```bash theme={null} > Create a Python script that prints \"Hello, World!\" ``` The agent proposes changes with diffs for your approval before modifying files.    Install locally if needed:  ```bash pip theme={null} pip install deepagents-cli ``` ```bash uv theme={null} uv add deepagents-cli ```  The CLI uses Anthropic Claude Sonnet 4 by default. To use OpenAI: ```bash theme={null} export OPENAI\\_API\\_KEY=\"your-key\" ``` Enable web search (optional): ```bash theme={null} export TAVILY\\_API\\_KEY=\"your-key\" ``` API keys can be set as environment variables or in a `.env` file.  ## Configuration   | Option | Description | | ---------------------- | ----------------------------------------------------------- | | `--agent NAME` | Use named agent with separate memory | | `--auto-approve` | Skip tool confirmation prompts (toggle with `Ctrl+T`) | | `--sandbox TYPE` | Execute in remote sandbox: `modal`, `daytona`, or `runloop` | | `--sandbox-id ID` | Reuse existing sandbox | | `--sandbox-setup PATH` | Run setup script in sandbox |   | Command | Description | | ----------------------------------------------- | --------------------------------------- | | `deepagents list` | List all agents | | `deepagents help` | Show help | | `deepagents reset --agent NAME` | Clear agent memory and reset to default | | `deepagents reset --agent NAME --target SOURCE` | Copy memory from another agent |   ## Interactive mode   Use these commands within the CLI session: \\* `/tokens` - Display token usage \\* `/clear` - Clear conversation history \\* `/exit` - Exit the CLI   Execute shell commands directly by prefixing with `!`: ```bash theme={null} !git status !npm test !ls -la ```   | Shortcut | Action | | ----------- | ------------------- | | `Enter` | Submit | | `Alt+Enter` | Newline | | `Ctrl+E` | External editor | | `Ctrl+T` | Toggle auto-approve | | `Ctrl+C` | Interrupt | | `Ctrl+D` | Exit |   ## Set project conventions with memories Agents store information in `~/.deepagents/AGENT\\_NAME/memories/` as markdown files using a memory-first protocol: 1. \\*\\*Research\\*\\*: Searches memory for relevant context before starting tasks 2. \\*\\*Response\\*\\*: Checks memory when uncertain during execution 3. \\*\\*Learning\\*\\*: Automatically saves new information for future sessions Organize memories by topic with descriptive filenames: ``` ~/.deepagents/backend-dev/memories/ \u251c\u2500\u2500 api-conventions.md \u251c\u2500\u2500 database-schema.md \u2514\u2500\u2500 deployment-process.md ``` Teach the agent conventions once: ```bash theme={null} uvx deepagents-cli --agent backend-dev > Our API uses snake\\_case and includes created\\_at/updated\\_at timestamps ``` It remembers for future sessions: ```bash theme={null} > Create a /users endpoint # Applies conventions without prompting ``` ## Use remote sandboxes Execute code in isolated remote environments for safety and flexibility. Remote sandboxes provide the following benefits: \\* \\*\\*Safety\\*\\*: Protect your local machine from potentially harmful code execution \\* \\*\\*Clean environments\\*\\*: Use specific dependencies or OS configurations without local setup \\* \\*\\*Parallel execution\\*\\*: Run multiple agents simultaneously in isolated environments \\* \\*\\*Long-running tasks\\*\\*: Execute time-intensive operations without blocking your machine \\* \\*\\*Reproducibility\\*\\*: Ensure consistent execution environments across teams To use a remote sandbox, follow these steps: 1. Configure your sandbox provider ([Runloop](https://www.runloop.ai/), [Daytona](https://www.daytona.io/), or [Modal](https://modal.com/)): ```bash theme={null} # Runloop export RUNLOOP\\_API\\_KEY=\"your-key\" # Daytona export DAYTONA\\_API\\_KEY=\"your-key\" # Modal modal setup ``` 2. Run the CLI with a sandbox: ```bash theme={null} uvx deepagents-cli --sandbox runloop --sandbox-setup ./setup.sh ``` The agent runs locally but executes all code operations in the remote sandbox. Optional setup scripts can configure environment variables, clone repositories, and prepare dependencies. 3. (Optional) Create a `setup.sh` file to configure your sandbox environment: ```bash theme={null} #!/bin/bash set -e # Clone repository using GitHub token git clone https://x-access-token:${GITHUB\\_TOKEN}@github.com/username/repo.git $HOME/workspace cd $HOME/workspace # Make environment variables persistent cat >> ~/.bashrc <<'EOF' export GITHUB\\_TOKEN=\"${GITHUB\\_TOKEN}\" export OPENAI\\_API\\_KEY=\"${OPENAI\\_API\\_KEY}\" cd $HOME/workspace EOF source ~/.bashrc ``` Store secrets in a local `.env` file for the setup script to access.  Sandboxes isolate code execution, but agents remain vulnerable to prompt injection with untrusted inputs. Use human-in-the-loop approval, short-lived secrets, and trusted setup scripts only. Note that sandbox APIs are evolving rapidly, and we expect more providers to support proxies that help mitigate prompt injection and secrets management concerns.  \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/cli.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/deepagents/customization.md",
      "title": null,
      "raw_content": "# Customize Deep Agents > Learn how to customize deep agents with system prompts, tools, subagents, and more ```mermaid theme={null} graph LR Create[create\\_deep\\_agent] --> Core[Core Config] Create --> Features[Features] Core --> Model[Model] Core --> Prompt[System Prompt] Core --> Tools[Tools] Features --> Backend[Backend] Features --> Sub[Subagents] Features --> Interrupt[Interrupts] Model --> Agent[Customized Agent] Prompt --> Agent Tools --> Agent Backend --> Agent Sub --> Agent Interrupt --> Agent ``` ## Model By default, `deepagents` uses [`claude-sonnet-4-5-20250929`](https://platform.claude.com/docs/en/about-claude/models/overview). You can customize the model used by passing any supported model identifier string or [LangChain model object](/oss/python/integrations/chat).  ```python Model string theme={null} from langchain.chat\\_models import init\\_chat\\_model from deepagents import create\\_deep\\_agent model = init\\_chat\\_model(model=\"gpt-5\") agent = create\\_deep\\_agent(model=model) ``` ```python LangChain model object theme={null} from langchain\\_ollama import ChatOllama from langchain.chat\\_models import init\\_chat\\_model from deepagents import create\\_deep\\_agent model = init\\_chat\\_model( model=ChatOllama( model=\"llama3.1\", temperature=0, # other params... ) ) agent = create\\_deep\\_agent(model=model) ```  ## System prompt Deep agents come with a built-in system prompt inspired by Claude Code's system prompt. The default system prompt contains detailed instructions for using the built-in planning tool, file system tools, and subagents. Each deep agent tailored to a use case should include a custom system prompt specific to that use case. ```python theme={null} from deepagents import create\\_deep\\_agent research\\_instructions = \"\"\"\\ You are an expert researcher. Your job is to conduct \\ thorough research, and then write a polished report. \\ \"\"\" agent = create\\_deep\\_agent( system\\_prompt=research\\_instructions, ) ``` ## Tools Just like tool-calling agents, a deep agent gets a set of top level tools that it has access to. ```python theme={null} import os from typing import Literal from tavily import TavilyClient from deepagents import create\\_deep\\_agent tavily\\_client = TavilyClient(api\\_key=os.environ[\"TAVILY\\_API\\_KEY\"]) def internet\\_search( query: str, max\\_results: int = 5, topic: Literal[\"general\", \"news\", \"finance\"] = \"general\", include\\_raw\\_content: bool = False, ): \"\"\"Run a web search\"\"\" return tavily\\_client.search( query, max\\_results=max\\_results, include\\_raw\\_content=include\\_raw\\_content, topic=topic, ) agent = create\\_deep\\_agent( tools=[internet\\_search] ) ``` In addition to any tools that you provide, deep agents also get access to a number of default tools: \\* `write\\_todos` \u2013 Update the agent's to-do list \\* `ls` \u2013 List all files in the agent's filesystem \\* `read\\_file` \u2013 Read a file from the agent's filesystem \\* `write\\_file` \u2013 Write a new file in the agent's filesystem \\* `edit\\_file` \u2013 Edit an existing file in the agent's filesystem \\* `task` \u2013 Spawn a subagent to handle a specific task \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/customization.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/deepagents/harness.md",
      "title": null,
      "raw_content": "# Agent harness capabilities We think of `deepagents` as an [\"agent harness\"](https://blog.langchain.com/agent-frameworks-runtimes-and-harnesses-oh-my/). It is the same core tool calling loop as other agent frameworks, but with built-in tools and capabilities. ```mermaid theme={null} graph TB Agent[Deep Agent] --> Tools[File System Tools] Agent --> Todo[To-Do List] Agent --> Sub[Subagents] Tools --> Backend[Storage Backend] Backend --> State[State] Backend --> Disk[Filesystem] Backend --> Store[Store] Sub --> |isolated work| Result[Final Result] Result --> Agent ``` This page lists out the components that make up the agent harness. ## File system access The harness provides six tools for file system operations, making files first-class citizens in the agent's environment: | Tool | Description | | ------------ | --------------------------------------------------------------------------------------------- | | `ls` | List files in a directory with metadata (size, modified time) | | `read\\_file` | Read file contents with line numbers, supports offset/limit for large files | | `write\\_file` | Create new files | | `edit\\_file` | Perform exact string replacements in files (with global replace mode) | | `glob` | Find files matching patterns (e.g., `\\*\\*/\\*.py`) | | `grep` | Search file contents with multiple output modes (files only, content with context, or counts) | ## Large tool result eviction The harness automatically dumps large tool results to the file system when they exceed a token threshold, preventing context window saturation. \\*\\*How it works:\\*\\* \\* Monitors tool call results for size (default threshold: 20,000 tokens) \\* When exceeded, writes the result to a file instead \\* Replaces the tool result with a concise reference to the file \\* Agent can later read the file if needed ## Pluggable storage backends The harness abstracts file system operations behind a protocol, allowing different storage strategies for different use cases. \\*\\*Available backends:\\*\\* 1. \\*\\*StateBackend\\*\\* - Ephemeral in-memory storage \\* Files live in the agent's state (checkpointed with conversation) \\* Persists within a thread but not across threads \\* Useful for temporary working files 2. \\*\\*FilesystemBackend\\*\\* - Real filesystem access \\* Read/write from actual disk \\* Supports virtual mode (sandboxed to a root directory) \\* Integrates with system tools (ripgrep for grep) \\* Security features: path validation, size limits, symlink prevention 3. \\*\\*StoreBackend\\*\\* - Persistent cross-conversation storage \\* Uses LangGraph's BaseStore for durability \\* Namespaced per assistant\\\\_id \\* Files persist across conversations \\* Useful for long-term memory or knowledge bases 4. \\*\\*CompositeBackend\\*\\* - Route different paths to different backends \\* Example: `/` \u2192 StateBackend, `/memories/` \u2192 StoreBackend \\* Longest-prefix matching for routing \\* Enables hybrid storage strategies ## Task delegation (subagents) The harness allows the main agent to create ephemeral \"subagents\" for isolated multi-step tasks. \\*\\*Why it's useful:\\*\\* \\* \\*\\*Context isolation\\*\\* - Subagent's work doesn't clutter main agent's context \\* \\*\\*Parallel execution\\*\\* - Multiple subagents can run concurrently \\* \\*\\*Specialization\\*\\* - Subagents can have different tools/configurations \\* \\*\\*Token efficiency\\*\\* - Large subtask context is compressed into a single result \\*\\*How it works:\\*\\* \\* Main agent has a `task` tool \\* When invoked, creates a fresh agent instance with its own context \\* Subagent executes autonomously until completion \\* Returns a single final report to the main agent \\* Subagents are stateless (can't send multiple messages back) \\*\\*Default subagent:\\*\\* \\* \"general-purpose\" subagent automatically available \\* Has filesystem tools by default \\* Can be customized with additional tools/middleware \\*\\*Custom subagents:\\*\\* \\* Define specialized subagents with specific tools \\* Example: code-reviewer, web-researcher, test-runner \\* Configure via `subagents` parameter ## Conversation history summarization The harness automatically compresses old conversation history when token usage becomes excessive. \\*\\*Configuration:\\*\\* \\* Triggers at 170,000 tokens \\* Keeps the most recent 6 messages intact \\* Older messages are summarized by the model \\*\\*Why it's useful:\\*\\* \\* Enables very long conversations without hitting context limits \\* Preserves recent context while compressing ancient history \\* Transparent to the agent (appears as a special system message) ## Dangling tool call repair The harness fixes message history when tool calls are interrupted or cancelled before receiving results. \\*\\*The problem:\\*\\* \\* Agent requests tool call: \"Please run X\" \\* Tool call is interrupted (user cancels, error, etc.) \\* Agent sees tool\\\\_call in AIMessage but no corresponding ToolMessage \\* This creates an invalid message sequence \\*\\*The solution:\\*\\* \\* Detects AIMessages with tool\\\\_calls that have no results \\* Creates synthetic ToolMessage responses indicating the call was cancelled \\* Repairs the message history before agent execution \\*\\*Why it's useful:\\*\\* \\* Prevents agent confusion from incomplete message chains \\* Gracefully handles interruptions and errors \\* Maintains conversation coherence ## To-do list tracking The harness provides a `write\\_todos` tool that agents can use to maintain a structured task list. \\*\\*Features:\\*\\* \\* Track multiple tasks with statuses (pending, in\\\\_progress, completed) \\* Persisted in agent state \\* Helps agent organize complex multi-step work \\* Useful for long-running tasks and planning ## Human-in-the-Loop The harness pauses agent execution at specified tool calls to allow human approval/modification. \\*\\*Configuration:\\*\\* \\* Map tool names to interrupt configurations \\* Example: `{\"edit\\_file\": True}` - pause before every edit \\* Can provide approval messages or modify tool inputs \\*\\*Why it's useful:\\*\\* \\* Safety gates for destructive operations \\* User verification before expensive API calls \\* Interactive debugging and guidance ## Prompt caching (Anthropic) The harness enables Anthropic's prompt caching feature to reduce redundant token processing. \\*\\*How it works:\\*\\* \\* Caches portions of the prompt that repeat across turns \\* Significantly reduces latency and cost for long system prompts \\* Automatically skips for non-Anthropic models \\*\\*Why it's useful:\\*\\* \\* System prompts (especially with filesystem docs) can be 5k+ tokens \\* These repeat every turn without caching \\* Caching provides \\~10x speedup and cost reduction \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/harness.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/deepagents/human-in-the-loop.md",
      "title": null,
      "raw_content": "# Human-in-the-loop > Learn how to configure human approval for sensitive tool operations Some tool operations may be sensitive and require human approval before execution. Deep agents support human-in-the-loop workflows through LangGraph's interrupt capabilities. You can configure which tools require approval using the `interrupt\\_on` parameter. ```mermaid theme={null} graph LR Agent[Agent] --> Check{Interrupt?} Check --> |no| Execute[Execute] Check --> |yes| Human{Human} Human --> |approve| Execute Human --> |edit| Execute Human --> |reject| Cancel[Cancel] Execute --> Agent Cancel --> Agent ``` ## Basic configuration The `interrupt\\_on` parameter accepts a dictionary mapping tool names to interrupt configurations. Each tool can be configured with: \\* \\*\\*`True`\\*\\*: Enable interrupts with default behavior (approve, edit, reject allowed) \\* \\*\\*`False`\\*\\*: Disable interrupts for this tool \\* \\*\\*`{\"allowed\\_decisions\": [...]}`\\*\\*: Custom configuration with specific allowed decisions ```python theme={null} from langchain.tools import tool from deepagents import create\\_deep\\_agent from langgraph.checkpoint.memory import MemorySaver @tool def delete\\_file(path: str) -> str: \"\"\"Delete a file from the filesystem.\"\"\" return f\"Deleted {path}\" @tool def read\\_file(path: str) -> str: \"\"\"Read a file from the filesystem.\"\"\" return f\"Contents of {path}\" @tool def send\\_email(to: str, subject: str, body: str) -> str: \"\"\"Send an email.\"\"\" return f\"Sent email to {to}\" # Checkpointer is REQUIRED for human-in-the-loop checkpointer = MemorySaver() agent = create\\_deep\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=[delete\\_file, read\\_file, send\\_email], interrupt\\_on={ \"delete\\_file\": True, # Default: approve, edit, reject \"read\\_file\": False, # No interrupts needed \"send\\_email\": {\"allowed\\_decisions\": [\"approve\", \"reject\"]}, # No editing }, checkpointer=checkpointer # Required! ) ``` ## Decision types The `allowed\\_decisions` list controls what actions a human can take when reviewing a tool call: \\* \\*\\*`\"approve\"`\\*\\*: Execute the tool with the original arguments as proposed by the agent \\* \\*\\*`\"edit\"`\\*\\*: Modify the tool arguments before execution \\* \\*\\*`\"reject\"`\\*\\*: Skip executing this tool call entirely You can customize which decisions are available for each tool: ```python theme={null} interrupt\\_on = { # Sensitive operations: allow all options \"delete\\_file\": {\"allowed\\_decisions\": [\"approve\", \"edit\", \"reject\"]}, # Moderate risk: approval or rejection only \"write\\_file\": {\"allowed\\_decisions\": [\"approve\", \"reject\"]}, # Must approve (no rejection allowed) \"critical\\_operation\": {\"allowed\\_decisions\": [\"approve\"]}, } ``` ## Handle interrupts When an interrupt is triggered, the agent pauses execution and returns control. Check for interrupts in the result and handle them accordingly. ```python theme={null} import uuid from langgraph.types import Command # Create config with thread\\_id for state persistence config = {\"configurable\": {\"thread\\_id\": str(uuid.uuid4())}} # Invoke the agent result = agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"Delete the file temp.txt\"}] }, config=config) # Check if execution was interrupted if result.get(\"\\_\\_interrupt\\_\\_\"): # Extract interrupt information interrupts = result[\"\\_\\_interrupt\\_\\_\"][0].value action\\_requests = interrupts[\"action\\_requests\"] review\\_configs = interrupts[\"review\\_configs\"] # Create a lookup map from tool name to review config config\\_map = {cfg[\"action\\_name\"]: cfg for cfg in review\\_configs} # Display the pending actions to the user for action in action\\_requests: review\\_config = config\\_map[action[\"name\"]] print(f\"Tool: {action['name']}\") print(f\"Arguments: {action['args']}\") print(f\"Allowed decisions: {review\\_config['allowed\\_decisions']}\") # Get user decisions (one per action\\_request, in order) decisions = [ {\"type\": \"approve\"} # User approved the deletion ] # Resume execution with decisions result = agent.invoke( Command(resume={\"decisions\": decisions}), config=config # Must use the same config! ) # Process final result print(result[\"messages\"][-1].content) ``` ## Multiple tool calls When the agent calls multiple tools that require approval, all interrupts are batched together in a single interrupt. You must provide decisions for each one in order. ```python theme={null} config = {\"configurable\": {\"thread\\_id\": str(uuid.uuid4())}} result = agent.invoke({ \"messages\": [{ \"role\": \"user\", \"content\": \"Delete temp.txt and send an email to admin@example.com\" }] }, config=config) if result.get(\"\\_\\_interrupt\\_\\_\"): interrupts = result[\"\\_\\_interrupt\\_\\_\"][0].value action\\_requests = interrupts[\"action\\_requests\"] # Two tools need approval assert len(action\\_requests) == 2 # Provide decisions in the same order as action\\_requests decisions = [ {\"type\": \"approve\"}, # First tool: delete\\_file {\"type\": \"reject\"} # Second tool: send\\_email ] result = agent.invoke( Command(resume={\"decisions\": decisions}), config=config ) ``` ## Edit tool arguments When `\"edit\"` is in the allowed decisions, you can modify the tool arguments before execution: ```python theme={null} if result.get(\"\\_\\_interrupt\\_\\_\"): interrupts = result[\"\\_\\_interrupt\\_\\_\"][0].value action\\_request = interrupts[\"action\\_requests\"][0] # Original args from the agent print(action\\_request[\"args\"]) # {\"to\": \"everyone@company.com\", ...} # User decides to edit the recipient decisions = [{ \"type\": \"edit\", \"edited\\_action\": { \"name\": action\\_request[\"name\"], # Must include the tool name \"args\": {\"to\": \"team@company.com\", \"subject\": \"...\", \"body\": \"...\"} } }] result = agent.invoke( Command(resume={\"decisions\": decisions}), config=config ) ``` ## Subagent interrupts Each subagent can have its own `interrupt\\_on` configuration that overrides the main agent's settings: ```python theme={null} agent = create\\_deep\\_agent( tools=[delete\\_file, read\\_file], interrupt\\_on={ \"delete\\_file\": True, \"read\\_file\": False, }, subagents=[{ \"name\": \"file-manager\", \"description\": \"Manages file operations\", \"system\\_prompt\": \"You are a file management assistant.\", \"tools\": [delete\\_file, read\\_file], \"interrupt\\_on\": { # Override: require approval for reads in this subagent \"delete\\_file\": True, \"read\\_file\": True, # Different from main agent! } }], checkpointer=checkpointer ) ``` When a subagent triggers an interrupt, the handling is the same \u2013 check for `\\_\\_interrupt\\_\\_` and resume with `Command`. ## Best practices ### Always use a checkpointer Human-in-the-loop requires a checkpointer to persist agent state between the interrupt and resume: ```python theme={null} from langgraph.checkpoint.memory import MemorySaver checkpointer = MemorySaver() agent = create\\_deep\\_agent( tools=[...], interrupt\\_on={...}, checkpointer=checkpointer # Required for HITL ) ``` ### Use the same thread ID When resuming, you must use the same config with the same `thread\\_id`: ```python theme={null} # First call config = {\"configurable\": {\"thread\\_id\": \"my-thread\"}} result = agent.invoke(input, config=config) # Resume (use same config) result = agent.invoke(Command(resume={...}), config=config) ``` ### Match decision order to actions The decisions list must match the order of `action\\_requests`: ```python theme={null} if result.get(\"\\_\\_interrupt\\_\\_\"): interrupts = result[\"\\_\\_interrupt\\_\\_\"][0].value action\\_requests = interrupts[\"action\\_requests\"] # Create one decision per action, in order decisions = [] for action in action\\_requests: decision = get\\_user\\_decision(action) # Your logic decisions.append(decision) result = agent.invoke( Command(resume={\"decisions\": decisions}), config=config ) ``` ### Tailor configurations by risk Configure different tools based on their risk level: ```python theme={null} interrupt\\_on = { # High risk: full control (approve, edit, reject) \"delete\\_file\": {\"allowed\\_decisions\": [\"approve\", \"edit\", \"reject\"]}, \"send\\_email\": {\"allowed\\_decisions\": [\"approve\", \"edit\", \"reject\"]}, # Medium risk: no editing allowed \"write\\_file\": {\"allowed\\_decisions\": [\"approve\", \"reject\"]}, # Low risk: no interrupts \"read\\_file\": False, \"list\\_files\": False, } ``` \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/human-in-the-loop.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/deepagents/long-term-memory.md",
      "title": null,
      "raw_content": "# Long-term memory > Learn how to extend deep agents with persistent memory across threads Deep agents come with a local filesystem to offload memory. By default, this filesystem is stored in agent state and is \\*\\*transient to a single thread\\*\\*\u2014files are lost when the conversation ends. You can extend deep agents with \\*\\*long-term memory\\*\\* by using a \\*\\*CompositeBackend\\*\\* that routes specific paths to persistent storage. This enables hybrid storage where some files persist across threads while others remain ephemeral. ```mermaid theme={null} graph LR Agent[Deep Agent] --> Router{Path Router} Router --> |/memories/\\*| Store[Store Backend] Router --> |other| State[State Backend] Store --> Persist[(Persistent  \nacross threads)] State --> Ephemeral[(Ephemeral  \nsingle thread)] ``` ## Setup Configure long-term memory by using a `CompositeBackend` that routes the `/memories/` path to a `StoreBackend`: ```python theme={null} from deepagents import create\\_deep\\_agent from deepagents.backends import CompositeBackend, StateBackend, StoreBackend from langgraph.store.memory import InMemoryStore def make\\_backend(runtime): return CompositeBackend( default=StateBackend(runtime), # Ephemeral storage routes={ \"/memories/\": StoreBackend(runtime) # Persistent storage } ) agent = create\\_deep\\_agent( store=InMemoryStore(), # Required for StoreBackend backend=make\\_backend ) ``` ## How it works When using `CompositeBackend`, deep agents maintain \\*\\*two separate filesystems\\*\\*: ### 1. Short-term (transient) filesystem \\* Stored in the agent's state (via `StateBackend`) \\* Persists only within a single thread \\* Files are lost when the thread ends \\* Accessed through standard paths: `/notes.txt`, `/workspace/draft.md` ### 2. Long-term (persistent) filesystem \\* Stored in a LangGraph Store (via `StoreBackend`) \\* Persists across all threads and conversations \\* Survives agent restarts \\* Accessed through paths prefixed with `/memories/`: `/memories/preferences.txt` ### Path routing The `CompositeBackend` routes file operations based on path prefixes: \\* Files with paths starting with `/memories/` are stored in the Store (persistent) \\* Files without this prefix remain in transient state \\* All filesystem tools (`ls`, `read\\_file`, `write\\_file`, `edit\\_file`) work with both ```python theme={null} # Transient file (lost after thread ends) agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"Write draft to /draft.txt\"}] }) # Persistent file (survives across threads) agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"Save final report to /memories/report.txt\"}] }) ``` ## Cross-thread persistence Files in `/memories/` can be accessed from any thread: ```python theme={null} import uuid # Thread 1: Write to long-term memory config1 = {\"configurable\": {\"thread\\_id\": str(uuid.uuid4())}} agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"Save my preferences to /memories/preferences.txt\"}] }, config=config1) # Thread 2: Read from long-term memory (different conversation!) config2 = {\"configurable\": {\"thread\\_id\": str(uuid.uuid4())}} agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"What are my preferences?\"}] }, config=config2) # Agent can read /memories/preferences.txt from the first thread ``` ## Use cases ### User preferences Store user preferences that persist across sessions: ```python theme={null} agent = create\\_deep\\_agent( store=InMemoryStore(), backend=lambda rt: CompositeBackend( default=StateBackend(rt), routes={\"/memories/\": StoreBackend(rt)} ), system\\_prompt=\"\"\"When users tell you their preferences, save them to /memories/user\\_preferences.txt so you remember them in future conversations.\"\"\" ) ``` ### Self-improving instructions An agent can update its own instructions based on feedback: ```python theme={null} agent = create\\_deep\\_agent( store=InMemoryStore(), backend=lambda rt: CompositeBackend( default=StateBackend(rt), routes={\"/memories/\": StoreBackend(rt)} ), system\\_prompt=\"\"\"You have a file at /memories/instructions.txt with additional instructions and preferences. Read this file at the start of conversations to understand user preferences. When users provide feedback like \"please always do X\" or \"I prefer Y\", update /memories/instructions.txt using the edit\\_file tool.\"\"\" ) ``` Over time, the instructions file accumulates user preferences, helping the agent improve. ### Knowledge base Build up knowledge over multiple conversations: ```python theme={null} # Conversation 1: Learn about a project agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"We're building a web app with React. Save project notes.\"}] }) # Conversation 2: Use that knowledge agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"What framework are we using?\"}] }) # Agent reads /memories/project\\_notes.txt from previous conversation ``` ### Research projects Maintain research state across sessions: ```python theme={null} research\\_agent = create\\_deep\\_agent( store=InMemoryStore(), backend=lambda rt: CompositeBackend( default=StateBackend(rt), routes={\"/memories/\": StoreBackend(rt)} ), system\\_prompt=\"\"\"You are a research assistant. Save your research progress to /memories/research/: - /memories/research/sources.txt - List of sources found - /memories/research/notes.txt - Key findings and notes - /memories/research/report.md - Final report draft This allows research to continue across multiple sessions.\"\"\" ) ``` ## Store implementations Any LangGraph `BaseStore` implementation works: ### InMemoryStore (development) Good for testing and development, but data is lost on restart: ```python theme={null} from langgraph.store.memory import InMemoryStore store = InMemoryStore() agent = create\\_deep\\_agent( store=store, backend=lambda rt: CompositeBackend( default=StateBackend(rt), routes={\"/memories/\": StoreBackend(rt)} ) ) ``` ### PostgresStore (production) For production, use a persistent store: ```python theme={null} from langgraph.store.postgres import PostgresStore import os store = PostgresStore(connection\\_string=os.environ[\"DATABASE\\_URL\"]) agent = create\\_deep\\_agent( store=store, backend=lambda rt: CompositeBackend( default=StateBackend(rt), routes={\"/memories/\": StoreBackend(rt)} ) ) ``` ## Best practices ### Use descriptive paths Organize persistent files with clear paths: ``` /memories/user\\_preferences.txt /memories/research/topic\\_a/sources.txt /memories/research/topic\\_a/notes.txt /memories/project/requirements.md ``` ### Document the memory structure Tell the agent what's stored where in your system prompt: ``` Your persistent memory structure: - /memories/preferences.txt: User preferences and settings - /memories/context/: Long-term context about the user - /memories/knowledge/: Facts and information learned over time ``` ### Prune old data Implement periodic cleanup of outdated persistent files to keep storage manageable. ### Choose the right storage \\* \\*\\*Development\\*\\*: Use `InMemoryStore` for quick iteration \\* \\*\\*Production\\*\\*: Use `PostgresStore` or other persistent stores \\* \\*\\*Multi-tenant\\*\\*: Consider using assistant\\\\_id-based namespacing in your store \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/long-term-memory.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/deepagents/middleware.md",
      "title": null,
      "raw_content": "# Deep Agents Middleware > Understand the middleware that powers deep agents Deep agents are built with a modular middleware architecture. Deep agents have access to: 1. A planning tool 2. A filesystem for storing context and long-term memories 3. The ability to spawn subagents Each feature is implemented as separate middleware. When you create a deep agent with `create\\_deep\\_agent`, we automatically attach `TodoListMiddleware`, `FilesystemMiddleware`, and `SubAgentMiddleware` to your agent. ```mermaid theme={null} graph LR Agent[create\\_deep\\_agent] --> Todo[TodoList] Agent --> FS[Filesystem] Agent --> Sub[SubAgent] Todo --> Tools[Agent Tools] FS --> Tools Sub --> Tools ``` Middleware is composable\u2014you can add as many or as few middleware to an agent as needed. You can use any middleware independently. The following sections explain what each middleware provides. ## To-do list middleware Planning is integral to solving complex problems. If you've used Claude Code recently, you'll notice how it writes out a to-do list before tackling complex, multi-part tasks. You'll also notice how it can adapt and update this to-do list on the fly as more information comes in. `TodoListMiddleware` provides your agent with a tool specifically for updating this to-do list. Before and while it executes a multi-part task, the agent is prompted to use the `write\\_todos` tool to keep track of what it's doing and what still needs to be done. ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import TodoListMiddleware # TodoListMiddleware is included by default in create\\_deep\\_agent # You can customize it if building a custom agent agent = create\\_agent( model=\"claude-sonnet-4-5-20250929\", # Custom planning instructions can be added via middleware middleware=[ TodoListMiddleware( system\\_prompt=\"Use the write\\_todos tool to...\" # Optional: Custom addition to the system prompt ), ], ) ``` ## Filesystem middleware Context engineering is a main challenge in building effective agents. This is particularly difficult when using tools that return variable-length results (for example, web\\\\_search and rag), as long tool results can quickly fill your context window. `FilesystemMiddleware` provides four tools for interacting with both short-term and long-term memory: \\* \\*\\*ls\\*\\*: List the files in the filesystem \\* \\*\\*read\\\\_file\\*\\*: Read an entire file or a certain number of lines from a file \\* \\*\\*write\\\\_file\\*\\*: Write a new file to the filesystem \\* \\*\\*edit\\\\_file\\*\\*: Edit an existing file in the filesystem ```python theme={null} from langchain.agents import create\\_agent from deepagents.middleware.filesystem import FilesystemMiddleware # FilesystemMiddleware is included by default in create\\_deep\\_agent # You can customize it if building a custom agent agent = create\\_agent( model=\"claude-sonnet-4-5-20250929\", middleware=[ FilesystemMiddleware( backend=None, # Optional: custom backend (defaults to StateBackend) system\\_prompt=\"Write to the filesystem when...\", # Optional custom addition to the system prompt custom\\_tool\\_descriptions={ \"ls\": \"Use the ls tool when...\", \"read\\_file\": \"Use the read\\_file tool to...\" } # Optional: Custom descriptions for filesystem tools ), ], ) ``` ### Short-term vs. long-term filesystem By default, these tools write to a local \"filesystem\" in your graph state. To enable persistent storage across threads, configure a `CompositeBackend` that routes specific paths (like `/memories/`) to a `StoreBackend`. ```python theme={null} from langchain.agents import create\\_agent from deepagents.middleware import FilesystemMiddleware from deepagents.backends import CompositeBackend, StateBackend, StoreBackend from langgraph.store.memory import InMemoryStore store = InMemoryStore() agent = create\\_agent( model=\"claude-sonnet-4-5-20250929\", store=store, middleware=[ FilesystemMiddleware( backend=lambda rt: CompositeBackend( default=StateBackend(rt), routes={\"/memories/\": StoreBackend(rt)} ), custom\\_tool\\_descriptions={ \"ls\": \"Use the ls tool when...\", \"read\\_file\": \"Use the read\\_file tool to...\" } # Optional: Custom descriptions for filesystem tools ), ], ) ``` When you configure a `CompositeBackend` with a `StoreBackend` for `/memories/`, any files prefixed with \\*\\*/memories/\\*\\* are saved to persistent storage and survive across different threads. Files without this prefix remain in ephemeral state storage. ## Subagent middleware Handing off tasks to subagents isolates context, keeping the main (supervisor) agent's context window clean while still going deep on a task. The subagents middleware allows you to supply subagents through a `task` tool. ```python theme={null} from langchain.tools import tool from langchain.agents import create\\_agent from deepagents.middleware.subagents import SubAgentMiddleware @tool def get\\_weather(city: str) -> str: \"\"\"Get the weather in a city.\"\"\" return f\"The weather in {city} is sunny.\" agent = create\\_agent( model=\"claude-sonnet-4-5-20250929\", middleware=[ SubAgentMiddleware( default\\_model=\"claude-sonnet-4-5-20250929\", default\\_tools=[], subagents=[ { \"name\": \"weather\", \"description\": \"This subagent can get weather in cities.\", \"system\\_prompt\": \"Use the get\\_weather tool to get the weather in a city.\", \"tools\": [get\\_weather], \"model\": \"gpt-4o\", \"middleware\": [], } ], ) ], ) ``` A subagent is defined with a \\*\\*name\\*\\*, \\*\\*description\\*\\*, \\*\\*system prompt\\*\\*, and \\*\\*tools\\*\\*. You can also provide a subagent with a custom \\*\\*model\\*\\*, or with additional \\*\\*middleware\\*\\*. This can be particularly useful when you want to give the subagent an additional state key to share with the main agent. For more complex use cases, you can also provide your own pre-built LangGraph graph as a subagent. ```python theme={null} from langchain.agents import create\\_agent from deepagents.middleware.subagents import SubAgentMiddleware from deepagents import CompiledSubAgent from langgraph.graph import StateGraph # Create a custom LangGraph graph def create\\_weather\\_graph(): workflow = StateGraph(...) # Build your custom graph return workflow.compile() weather\\_graph = create\\_weather\\_graph() # Wrap it in a CompiledSubAgent weather\\_subagent = CompiledSubAgent( name=\"weather\", description=\"This subagent can get weather in cities.\", runnable=weather\\_graph ) agent = create\\_agent( model=\"claude-sonnet-4-5-20250929\", middleware=[ SubAgentMiddleware( default\\_model=\"claude-sonnet-4-5-20250929\", default\\_tools=[], subagents=[weather\\_subagent], ) ], ) ``` In addition to any user-defined subagents, the main agent has access to a `general-purpose` subagent at all times. This subagent has the same instructions as the main agent and all the tools it has access to. The primary purpose of the `general-purpose` subagent is context isolation\u2014the main agent can delegate a complex task to this subagent and get a concise answer back without bloat from intermediate tool calls. \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/middleware.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/deepagents/overview.md",
      "title": null,
      "raw_content": "# Deep Agents overview > Build agents that can plan, use subagents, and leverage file systems for complex tasks [`deepagents`](https://pypi.org/project/deepagents/) is a standalone library for building agents that can tackle complex, multi-step tasks. Built on LangGraph and inspired by applications like Claude Code, Deep Research, and Manus, deep agents come with planning capabilities, file systems for context management, and the ability to spawn subagents. ## When to use deep agents Use deep agents when you need agents that can: \\* \\*\\*Handle complex, multi-step tasks\\*\\* that require planning and decomposition \\* \\*\\*Manage large amounts of context\\*\\* through file system tools \\* \\*\\*Delegate work\\*\\* to specialized subagents for context isolation \\* \\*\\*Persist memory\\*\\* across conversations and threads For simpler use cases, consider using LangChain's [`create\\_agent`](/oss/python/langchain/agents) or building a custom [LangGraph](/oss/python/langgraph/overview) workflow. ## Core capabilities  Deep agents include a built-in `write\\_todos` tool that enables agents to break down complex tasks into discrete steps, track progress, and adapt plans as new information emerges.   File system tools (`ls`, `read\\_file`, `write\\_file`, `edit\\_file`) allow agents to offload large context to memory, preventing context window overflow and enabling work with variable-length tool results.   A built-in `task` tool enables agents to spawn specialized subagents for context isolation. This keeps the main agent's context clean while still going deep on specific subtasks.   Extend agents with persistent memory across threads using LangGraph's Store. Agents can save and retrieve information from previous conversations.  ## Relationship to the LangChain ecosystem Deep agents is built on top of: \\* [LangGraph](/oss/python/langgraph/overview) - Provides the underlying graph execution and state management \\* [LangChain](/oss/python/langchain/overview) - Tools and model integrations work seamlessly with deep agents \\* [LangSmith](/langsmith/home) - Observability, evaluation, and deployment Deep agents applications can be deployed via [LangSmith Deployment](/langsmith/deployments) and monitored with [LangSmith Observability](/langsmith/observability). ## Get started   Build your first deep agent   Learn about customization options   Understand the middleware architecture   See the `deepagents` API reference   \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/overview.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/deepagents/quickstart.md",
      "title": null,
      "raw_content": "# Quickstart > Build your first deep agent in minutes This guide walks you through creating your first deep agent with planning, file system tools, and subagent capabilities. You'll build a research agent that can conduct research and write reports. ## Prerequisites Before you begin, make sure you have an API key from a model provider (e.g., Anthropic, OpenAI). ### Step 1: Install dependencies  ```bash pip theme={null} pip install deepagents tavily-python ``` ```bash uv theme={null} uv add deepagents tavily-python ``` ```bash poetry theme={null} poetry add deepagents tavily-python ```  ### Step 2: Set up your API keys ```bash theme={null} export ANTHROPIC\\_API\\_KEY=\"your-api-key\" export TAVILY\\_API\\_KEY=\"your-tavily-api-key\" ``` ### Step 3: Create a search tool ```python theme={null} import os from typing import Literal from tavily import TavilyClient from deepagents import create\\_deep\\_agent tavily\\_client = TavilyClient(api\\_key=os.environ[\"TAVILY\\_API\\_KEY\"]) def internet\\_search( query: str, max\\_results: int = 5, topic: Literal[\"general\", \"news\", \"finance\"] = \"general\", include\\_raw\\_content: bool = False, ): \"\"\"Run a web search\"\"\" return tavily\\_client.search( query, max\\_results=max\\_results, include\\_raw\\_content=include\\_raw\\_content, topic=topic, ) ``` ### Step 4: Create a deep agent ```python theme={null} # System prompt to steer the agent to be an expert researcher research\\_instructions = \"\"\"You are an expert researcher. Your job is to conduct thorough research and then write a polished report. You have access to an internet search tool as your primary means of gathering information. ## `internet\\_search` Use this to run an internet search for a given query. You can specify the max number of results to return, the topic, and whether raw content should be included. \"\"\" agent = create\\_deep\\_agent( tools=[internet\\_search], system\\_prompt=research\\_instructions ) ``` ### Step 5: Run the agent ```python theme={null} result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"What is langgraph?\"}]}) # Print the agent's response print(result[\"messages\"][-1].content) ``` ## What happened? Your deep agent automatically: 1. \\*\\*Planned its approach\\*\\*: Used the built-in `write\\_todos` tool to break down the research task 2. \\*\\*Conducted research\\*\\*: Called the `internet\\_search` tool to gather information 3. \\*\\*Managed context\\*\\*: Used file system tools (`write\\_file`, `read\\_file`) to offload large search results 4. \\*\\*Spawned subagents\\*\\* (if needed): Delegated complex subtasks to specialized subagents 5. \\*\\*Synthesized a report\\*\\*: Compiled findings into a coherent response ## Next steps Now that you've built your first deep agent: \\* \\*\\*Customize your agent\\*\\*: Learn about [customization options](/oss/python/deepagents/customization), including custom system prompts, tools, and subagents. \\* \\*\\*Understand middleware\\*\\*: Dive into the [middleware architecture](/oss/python/deepagents/middleware) that powers deep agents. \\* \\*\\*Add long-term memory\\*\\*: Enable [persistent memory](/oss/python/deepagents/long-term-memory) across conversations. \\* \\*\\*Deploy to production\\*\\*: Learn about [deployment options](/oss/python/langgraph/deploy) for LangGraph applications. \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/quickstart.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/deepagents/subagents.md",
      "title": null,
      "raw_content": "# Subagents > Learn how to use subagents to delegate work and keep context clean Deep agents can create subagents to delegate work. You can specify custom subagents in the `subagents` parameter. Subagents are useful for [context quarantine](https://www.dbreunig.com/2025/06/26/how-to-fix-your-context.html#context-quarantine) (keeping the main agent's context clean) and for providing specialized instructions. ```mermaid theme={null} graph TB Main[Main Agent] --> |task tool| Sub[Subagent] Sub --> Research[Research] Sub --> Code[Code] Sub --> General[General] Research --> |isolated work| Result[Final Result] Code --> |isolated work| Result General --> |isolated work| Result Result --> Main ``` ## Why use subagents? Subagents solve the \\*\\*context bloat problem\\*\\*. When agents use tools with large outputs (web search, file reads, database queries), the context window fills up quickly with intermediate results. Subagents isolate this detailed work\u2014the main agent receives only the final result, not the dozens of tool calls that produced it. \\*\\*When to use subagents:\\*\\* \\* \u2705 Multi-step tasks that would clutter the main agent's context \\* \u2705 Specialized domains that need custom instructions or tools \\* \u2705 Tasks requiring different model capabilities \\* \u2705 When you want to keep the main agent focused on high-level coordination \\*\\*When NOT to use subagents:\\*\\* \\* \u274c Simple, single-step tasks \\* \u274c When you need to maintain intermediate context \\* \u274c When the overhead outweighs benefits ## Configuration `subagents` should be a list of dictionaries or `CompiledSubAgent` objects. There are two types: ### SubAgent (Dictionary-based) For most use cases, define subagents as dictionaries: \\*\\*Required fields:\\*\\* \\* \\*\\*name\\*\\* (`str`): Unique identifier for the subagent. The main agent uses this name when calling the `task()` tool. \\* \\*\\*description\\*\\* (`str`): What this subagent does. Be specific and action-oriented. The main agent uses this to decide when to delegate. \\* \\*\\*system\\\\_prompt\\*\\* (`str`): Instructions for the subagent. Include tool usage guidance and output format requirements. \\* \\*\\*tools\\*\\* (`List[Callable]`): Tools the subagent can use. Keep this minimal and include only what's needed. \\*\\*Optional fields:\\*\\* \\* \\*\\*model\\*\\* (`str | BaseChatModel`): Override the main agent's model. Use the format `\"provider:model-name\"` (for example, `\"openai:gpt-4o\"`). \\* \\*\\*middleware\\*\\* (`List[Middleware]`): Additional middleware for custom behavior, logging, or rate limiting. \\* \\*\\*interrupt\\\\_on\\*\\* (`Dict[str, bool]`): Configure human-in-the-loop for specific tools. Requires a checkpointer. ### CompiledSubAgent For complex workflows, use a pre-built LangGraph graph: \\*\\*Fields:\\*\\* \\* \\*\\*name\\*\\* (`str`): Unique identifier \\* \\*\\*description\\*\\* (`str`): What this subagent does \\* \\*\\*runnable\\*\\* (`Runnable`): A compiled LangGraph graph (must call `.compile()` first) ## Using SubAgent ```python theme={null} import os from typing import Literal from tavily import TavilyClient from deepagents import create\\_deep\\_agent tavily\\_client = TavilyClient(api\\_key=os.environ[\"TAVILY\\_API\\_KEY\"]) def internet\\_search( query: str, max\\_results: int = 5, topic: Literal[\"general\", \"news\", \"finance\"] = \"general\", include\\_raw\\_content: bool = False, ): \"\"\"Run a web search\"\"\" return tavily\\_client.search( query, max\\_results=max\\_results, include\\_raw\\_content=include\\_raw\\_content, topic=topic, ) research\\_subagent = { \"name\": \"research-agent\", \"description\": \"Used to research more in depth questions\", \"system\\_prompt\": \"You are a great researcher\", \"tools\": [internet\\_search], \"model\": \"openai:gpt-4o\", # Optional override, defaults to main agent model } subagents = [research\\_subagent] agent = create\\_deep\\_agent( model=\"claude-sonnet-4-5-20250929\", subagents=subagents ) ``` ## Using CompiledSubAgent For more complex use cases, you can provide your own pre-built LangGraph graph as a subagent: ```python theme={null} from deepagents import create\\_deep\\_agent, CompiledSubAgent from langchain.agents import create\\_agent # Create a custom agent graph custom\\_graph = create\\_agent( model=your\\_model, tools=specialized\\_tools, prompt=\"You are a specialized agent for data analysis...\" ) # Use it as a custom subagent custom\\_subagent = CompiledSubAgent( name=\"data-analyzer\", description=\"Specialized agent for complex data analysis tasks\", runnable=custom\\_graph ) subagents = [custom\\_subagent] agent = create\\_deep\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=[internet\\_search], system\\_prompt=research\\_instructions, subagents=subagents ) ``` ## The general-purpose subagent In addition to any user-defined subagents, deep agents have access to a `general-purpose` subagent at all times. This subagent: \\* Has the same system prompt as the main agent \\* Has access to all the same tools \\* Uses the same model (unless overridden) ### When to use it The general-purpose subagent is ideal for context isolation without specialized behavior. The main agent can delegate a complex multi-step task to this subagent and get a concise result back without bloat from intermediate tool calls.  Instead of the main agent making 10 web searches and filling its context with results, it delegates to the general-purpose subagent: `task(name=\"general-purpose\", task=\"Research quantum computing trends\")`. The subagent performs all the searches internally and returns only a summary.  ## Best practices ### Write clear descriptions The main agent uses descriptions to decide which subagent to call. Be specific: \u2705 \\*\\*Good:\\*\\* `\"Analyzes financial data and generates investment insights with confidence scores\"` \u274c \\*\\*Bad:\\*\\* `\"Does finance stuff\"` ### Keep system prompts detailed Include specific guidance on how to use tools and format outputs: ```python theme={null} research\\_subagent = { \"name\": \"research-agent\", \"description\": \"Conducts in-depth research using web search and synthesizes findings\", \"system\\_prompt\": \"\"\"You are a thorough researcher. Your job is to: 1. Break down the research question into searchable queries 2. Use internet\\_search to find relevant information 3. Synthesize findings into a comprehensive but concise summary 4. Cite sources when making claims Output format: - Summary (2-3 paragraphs) - Key findings (bullet points) - Sources (with URLs) Keep your response under 500 words to maintain clean context.\"\"\", \"tools\": [internet\\_search], } ``` ### Minimize tool sets Only give subagents the tools they need. This improves focus and security: ```python theme={null} # \u2705 Good: Focused tool set email\\_agent = { \"name\": \"email-sender\", \"tools\": [send\\_email, validate\\_email], # Only email-related } # \u274c Bad: Too many tools email\\_agent = { \"name\": \"email-sender\", \"tools\": [send\\_email, web\\_search, database\\_query, file\\_upload], # Unfocused } ``` ### Choose models by task Different models excel at different tasks: ```python theme={null} subagents = [ { \"name\": \"contract-reviewer\", \"description\": \"Reviews legal documents and contracts\", \"system\\_prompt\": \"You are an expert legal reviewer...\", \"tools\": [read\\_document, analyze\\_contract], \"model\": \"claude-sonnet-4-5-20250929\", # Large context for long documents }, { \"name\": \"financial-analyst\", \"description\": \"Analyzes financial data and market trends\", \"system\\_prompt\": \"You are an expert financial analyst...\", \"tools\": [get\\_stock\\_price, analyze\\_fundamentals], \"model\": \"openai:gpt-5\", # Better for numerical analysis }, ] ``` ### Return concise results Instruct subagents to return summaries, not raw data: ```python theme={null} data\\_analyst = { \"system\\_prompt\": \"\"\"Analyze the data and return: 1. Key insights (3-5 bullet points) 2. Overall confidence score 3. Recommended next actions Do NOT include: - Raw data - Intermediate calculations - Detailed tool outputs Keep response under 300 words.\"\"\" } ``` ## Common patterns ### Multiple specialized subagents Create specialized subagents for different domains: ```python theme={null} from deepagents import create\\_deep\\_agent subagents = [ { \"name\": \"data-collector\", \"description\": \"Gathers raw data from various sources\", \"system\\_prompt\": \"Collect comprehensive data on the topic\", \"tools\": [web\\_search, api\\_call, database\\_query], }, { \"name\": \"data-analyzer\", \"description\": \"Analyzes collected data for insights\", \"system\\_prompt\": \"Analyze data and extract key insights\", \"tools\": [statistical\\_analysis], }, { \"name\": \"report-writer\", \"description\": \"Writes polished reports from analysis\", \"system\\_prompt\": \"Create professional reports from insights\", \"tools\": [format\\_document], }, ] agent = create\\_deep\\_agent( model=\"claude-sonnet-4-5-20250929\", system\\_prompt=\"You coordinate data analysis and reporting. Use subagents for specialized tasks.\", subagents=subagents ) ``` \\*\\*Workflow:\\*\\* 1. Main agent creates high-level plan 2. Delegates data collection to data-collector 3. Passes results to data-analyzer 4. Sends insights to report-writer 5. Compiles final output Each subagent works with clean context focused only on its task. ## Troubleshooting ### Subagent not being called \\*\\*Problem\\*\\*: Main agent tries to do work itself instead of delegating. \\*\\*Solutions\\*\\*: 1. \\*\\*Make descriptions more specific:\\*\\* ```python theme={null} # \u2705 Good {\"name\": \"research-specialist\", \"description\": \"Conducts in-depth research on specific topics using web search. Use when you need detailed information that requires multiple searches.\"} # \u274c Bad {\"name\": \"helper\", \"description\": \"helps with stuff\"} ``` 2. \\*\\*Instruct main agent to delegate:\\*\\* ```python theme={null} agent = create\\_deep\\_agent( system\\_prompt=\"\"\"...your instructions... IMPORTANT: For complex tasks, delegate to your subagents using the task() tool. This keeps your context clean and improves results.\"\"\", subagents=[...] ) ``` ### Context still getting bloated \\*\\*Problem\\*\\*: Context fills up despite using subagents. \\*\\*Solutions\\*\\*: 1. \\*\\*Instruct subagent to return concise results:\\*\\* ```python theme={null} system\\_prompt=\"\"\"... IMPORTANT: Return only the essential summary. Do NOT include raw data, intermediate search results, or detailed tool outputs. Your response should be under 500 words.\"\"\" ``` 2. \\*\\*Use filesystem for large data:\\*\\* ```python theme={null} system\\_prompt=\"\"\"When you gather large amounts of data: 1. Save raw data to /data/raw\\_results.txt 2. Process and analyze the data 3. Return only the analysis summary This keeps context clean.\"\"\" ``` ### Wrong subagent being selected \\*\\*Problem\\*\\*: Main agent calls inappropriate subagent for the task. \\*\\*Solution\\*\\*: Differentiate subagents clearly in descriptions: ```python theme={null} subagents = [ { \"name\": \"quick-researcher\", \"description\": \"For simple, quick research questions that need 1-2 searches. Use when you need basic facts or definitions.\", }, { \"name\": \"deep-researcher\", \"description\": \"For complex, in-depth research requiring multiple searches, synthesis, and analysis. Use for comprehensive reports.\", } ] ``` \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/deepagents/subagents.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/integrations/chat/index.md",
      "title": null,
      "raw_content": "# Chat models [Chat models](/oss/python/langchain/models) are language models that use a sequence of [messages](/oss/python/langchain/messages) as inputs and return messages as outputs (as opposed to traditional, plaintext LLMs). ## Featured providers  \\*\\*While these LangChain classes support the indicated advanced feature\\*\\*, you may need to refer to provider-specific documentation to learn which hosted models or backends support the feature.  | Model | [Tool calling](/oss/python/langchain/tools) | [Structured output](/oss/python/langchain/structured-output/) | JSON mode | Local | [Multimodal](/oss/python/langchain/messages#multimodal) | | ------------------------------------------------------------------------------ | ------------------------------------------- | ------------------------------------------------------------- | --------- | ----- | ------------------------------------------------------- | | [`ChatAnthropic`](/oss/python/integrations/chat/anthropic) | \u2705 | \u2705 | \u274c | \u274c | \u2705 | | [`ChatOpenAI`](/oss/python/integrations/chat/openai) | \u2705 | \u2705 | \u2705 | \u274c | \u2705 | | [`AzureChatOpenAI`](/oss/python/integrations/chat/azure\\_chat\\_openai) | \u2705 | \u2705 | \u2705 | \u274c | \u2705 | | [`ChatVertexAI`](/oss/python/integrations/chat/google\\_vertex\\_ai) | \u2705 | \u2705 | \u274c | \u274c | \u2705 | | [`ChatGoogleGenerativeAI`](/oss/python/integrations/chat/google\\_generative\\_ai) | \u2705 | \u2705 | \u274c | \u274c | \u2705 | | [`ChatGroq`](/oss/python/integrations/chat/groq) | \u2705 | \u2705 | \u2705 | \u274c | \u274c | | [`ChatBedrock`](/oss/python/integrations/chat/bedrock) | \u2705 | \u2705 | \u274c | \u274c | \u274c | | [`ChatHuggingFace`](/oss/python/integrations/chat/huggingface) | \u2705 | \u2705 | \u274c | \u2705 | \u274c | | [`ChatOllama`](/oss/python/integrations/chat/ollama) | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | | [`ChatWatsonx`](/oss/python/integrations/chat/ibm\\_watsonx) | \u2705 | \u2705 | \u2705 | \u274c | \u2705 | | [`ChatXAI`](/oss/python/integrations/chat/xai) | \u2705 | \u2705 | \u274c | \u274c | \u274c | | [`ChatNVIDIA`](/oss/python/integrations/chat/nvidia\\_ai\\_endpoints) | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | | [`ChatCohere`](/oss/python/integrations/chat/cohere) | \u2705 | \u2705 | \u274c | \u274c | \u274c | | [`ChatMistralAI`](/oss/python/integrations/chat/mistralai) | \u2705 | \u2705 | \u274c | \u274c | \u274c | | [`ChatTogether`](/oss/python/integrations/chat/together) | \u2705 | \u2705 | \u2705 | \u274c | \u274c | | [`ChatFireworks`](/oss/python/integrations/chat/fireworks) | \u2705 | \u2705 | \u2705 | \u274c | \u274c | | [`ChatLlamaCpp`](/oss/python/integrations/chat/llamacpp) | \u2705 | \u2705 | \u274c | \u2705 | \u274c | | [`ChatDatabricks`](/oss/python/integrations/chat/databricks) | \u2705 | \u2705 | \u274c | \u274c | \u274c | | [`ChatPerplexity`](/oss/python/integrations/chat/perplexity) | \u274c | \u2705 | \u2705 | \u274c | \u2705 | ## Chat Completions API Certain model providers offer endpoints that are compatible with OpenAI's [Chat Completions API](https://platform.openai.com/docs/api-reference/chat). In such case, you can use [`ChatOpenAI`](/oss/python/integrations/chat/openai) with a custom `base\\_url` to connect to these endpoints.  To use OpenRouter, you will need to sign up for an account and obtain an [API key](https://openrouter.ai/docs/api-reference/authentication). ```python theme={null} from langchain\\_openai import ChatOpenAI model = ChatOpenAI( model=\"...\", # Specify a model available on OpenRouter api\\_key=\"OPENROUTER\\_API\\_KEY\", base\\_url=\"https://openrouter.ai/api/v1\", ) ``` Refer to the [OpenRouter documentation](https://openrouter.ai/docs/quickstart) for more details.  To capture [reasoning tokens](https://openrouter.ai/docs/use-cases/reasoning-tokens), 1. Switch imports from `langchain\\_openai` to `langchain\\_deepseek` 2. Use `ChatDeepSeek` instead of `ChatOpenAI`. You will need to change param `base\\_url` to `api\\_base`. 3. Adjust reasoning parameters as needed under `extra\\_body`, e.g.: ```python theme={null} model = ChatDeepSeek( model=\"...\", api\\_key=\"...\", api\\_base=\"https://openrouter.ai/api/v1\", extra\\_body={\"reasoning\": {\"enabled\": True}}, ) ``` This is a known limitation with `ChatOpenAI` and will be addressed in a future release.   ## All chat models    If you'd like to contribute an integration, see [Contributing integrations](/oss/python/contributing#add-a-new-integration).  \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/chat/index.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/integrations/providers/all_providers.md",
      "title": null,
      "raw_content": "# All integration providers Browse the complete collection of integrations available for Python. LangChain Python offers the most extensive ecosystem with 1000+ integrations across LLMs, chat models, retrievers, vector stores, document loaders, and more. ## Providers   Custom AI integration platform for enterprise workflows.   Knowledge management platform with AI-powered organization.   Vector database for AI applications with deep learning focus.   Advertising platform for GPT applications and AI services.   Web scraping with natural language queries.   AI21 Labs' Jurassic models for text generation.   Experiment tracking and management platform.   Unified API for multiple AI and ML services.   Decentralized AI computing network platform.   Data integration platform for ETL and ELT pipelines.   Cloud-based spreadsheet and database platform.   Blockchain development platform and APIs.   European AI company's multilingual language models.   Alibaba's cloud computing and AI services.   Alibaba Cloud's real-time analytics database.   Browser automation and web scraping tools.   Approximate nearest neighbors search library.   Claude models for advanced reasoning and conversation.   Distributed computing platform for ML workloads.   Real-time analytical database management system.   Apache Software Foundation tools and libraries.   Web scraping and automation platform.   Apple's machine learning and AI frameworks.   Multi-model database with graph capabilities.   Domain-specific language model training platform.   Geographic information system platform.   Data labeling and annotation platform for NLP.   ML observability and performance monitoring.   AI model monitoring and governance platform.   Academic paper repository and search platform.   Data engineering and pipeline automation platform.   Real-time news search and analysis API.   Speech-to-text and audio intelligence API.   DataStax Astra DB vector database platform.   Data visualization and exploration platform.   Vector database for AI and ML applications.   Amazon Web Services cloud platform and AI services.   Song lyrics database and search platform.   Microsoft Azure AI and cognitive services.   Beijing Academy of AI research and models.   Vector database and semantic search platform.   Multi-modal AI database and storage system.   Chinese language model from Baichuan AI.   Baidu's AI services and language models.   Serverless GPU infrastructure for ML models.   ML model deployment and serving platform.   Serverless GPU computing platform.   HTML and XML parsing library for web scraping.   Bibliography management and citation format.   Chinese video sharing platform integration.   Decentralized AI network and incentive protocol.   Educational technology and learning management.   High-performance analytics and data processing.   AI-powered reading and research assistant.   Cloud content management and collaboration.   Privacy-focused search engine API.   AI knowledge management and retrieval platform.   Web data platform and proxy services.   Headless browser automation platform.   Serverless browser automation service.   ByteDance's AI models and services.   Distributed NoSQL database management system.   AI compute platform with specialized processors.   Serverless GPU platform for AI applications.   No-code AI chatbot and automation platform.   Open-source embedding database for AI apps.   Computer vision and AI model platform.   ML experiment tracking and automation.   Fast columnar database for analytics.   Project management and productivity platform.   Web infrastructure and security services.   Naver's AI assistant and NLP platform.   Time series database for IoT and analytics.   Memory layer for AI applications and agents.   AI knowledge management and retrieval system.   Language AI platform for enterprise applications.   College admissions and education platform.   ML experiment tracking and model management.   AI observability and monitoring platform.   Team collaboration and documentation platform.   Plugin system for AI agents and applications.   Context management for AI applications.   Contextual AI and language understanding.   NoSQL cloud database platform.   Conversational AI platform and chatbot builder.   Distributed SQL database for machine data.   Python bindings for transformer models in C/C++.   Fast inference engine for Transformer models.   Semantic layer for building data applications.   Real-time AI data platform and API.   Alibaba Cloud's vector database service.   Unified analytics platform for big data and ML.   Monitoring and analytics platform for applications.   Log management and analysis platform.   SEO and SERP data API platform.   Natural language to SQL query platform.   Secure and elastic infrastructure for running your AI-generated code.   Document analysis and structure detection.   Serverless inference for deep learning models.   Vector database for deep learning applications.   Advanced reasoning and coding AI models.   Inference runtime for sparse neural networks.   Dell Technologies AI and computing solutions.   Web data extraction and knowledge graph.   Distributed vector database system.   Communication platform integration and bots.   Discord analytics and moderation tools.   Data structure for multimodal AI applications.   Document processing and AI integration.   Document transformation and processing.   Document AI and semantic processing.   Documentation website generator and platform.   Decentralized knowledge retrieval network.   Cloud storage and file sharing platform.   In-process SQL OLAP database management system.   Privacy-focused search engine integration.   Cloud development environment platform.   Unified API for multiple AI services.   Distributed search and analytics engine.   AI voice synthesis and speech platform.   Framework for creating RAG applications.   Vector database for AI and ML applications.   Ethereum blockchain explorer and analytics.   Serverless AI inference platform.   Note-taking and organization platform.   AI-powered search engine for developers.   Meta's social platform integration and APIs.   Graph database with ultra-low latency.   Serverless, globally distributed database.   Fast and efficient AI model serving.   AI observability and monitoring platform.   Design collaboration and prototyping platform.   Web scraping and crawling API service.   Fast inference platform for open-source models.   Workflow orchestration for ML and data processing.   Financial market data and analytics API.   Fine-tuning platform for language models.   Optimized serving engine for AI models.   Prompt-driven engineering assistant.   Knowledge extraction and NLP platform.   Geographic data analysis with Python.   Version control system integration.   Documentation platform and knowledge base.   Code hosting and collaboration platform.   DevOps platform and code repository.   Tool use framework for AI agents.   Knowledge graph and data platform.   Google's AI services and cloud platform.   Google Search API service.   Fully managed NLP-as-a-Service platform.   Open-source LLM ecosystem for local deployment.   AI model training and deployment platform.   Single endpoint to multiple LLMs via serverless inference.   Graph-based retrieval augmented generation.   AI observability and monitoring platform.   Sustainable AI computing platform.   Machine learning library for bibliographic data.   Ultra-fast inference with specialized hardware.   Project Gutenberg digital library access.   Tech news and discussion platform.   Machine learning research and tools.   LLM observability and monitoring platform.   Real-time interactive analytics service.   HTML to plain text conversion utility.   Huawei Cloud AI services and models.   Open platform for ML models and datasets.   Web automation and scraping platform.   IBM Watson AI and enterprise solutions.   Enterprise AI and system integration.   Repair guides and technical documentation.   Chinese speech and language AI platform.   Internet Movie Script Database access.   Distributed cache and data grid platform.   High-performance embedding inference server.   Observability and monitoring platform.   Intel's AI optimization tools and libraries.   Legal AI models, apps, and data.   Brazilian payment processing platform.   Vector database and search platform.   AI model gateway and management platform.   Automation server and CI/CD platform.   Neural search framework and cloud platform.   Enterprise NLP and healthcare AI platform.   Open-source note taking and organization.   Time-series vector database platform.   Real-time analytics and database platform.   Browser-based AI writing assistant.   Generative AI platform and model hosting.   Korean natural language processing toolkit.   Embedded graph database management system.   Data labeling and annotation platform.   Git-like version control for data lakes.   Developer-friendly embedded vector database.   Syntactic sugar and utilities for LangChain.   Bias testing framework for language models.   LLM engineering platform and observability.   PostgreSQL vector database extension.   Alibaba Cloud's multi-model database service.   Real-time job market data and search.   Unified interface for 100+ LLM APIs.   Data framework for LLM applications.   Port of Meta's LLaMA model in C/C++.   Edge computing platform for LLaMA models.   Single-file executable for running LLMs.   Observability platform for LLM applications.   Self-hosted OpenAI-compatible API server.   LLM data management and observability.   Open-source relational database management.   Brazilian Portuguese language model.   End-to-end vector search engine.   Wikipedia and MediaWiki data processing.   Lightning-fast search engine platform.   Distributed memory caching system.   Real-time graph database platform.   Managed vector search and retrieval.   Microsoft Azure AI and enterprise services.   Open-source vector database for AI applications.   AI layer for databases and data platforms.   Chinese AI company's language models.   Efficient open-source language models.   ML lifecycle management platform.   Experiment tracking and model registry.   Apple's machine learning framework.   Serverless cloud computing for data science.   Alibaba's open-source model hub.   Payment operations and treasury management.   Serverless cache and vector index.   Document-based NoSQL database platform.   Cloud-hosted MongoDB with vector search.   Serverless analytics with DuckDB in the cloud.   Long-term memory for AI conversations.   SQL-compatible vector database platform.   Naver's AI services and language models.   AI cloud platform and infrastructure.   Native graph database and analytics platform.   Decentralized AI computing network.   Web intelligence and data extraction.   Production-ready NLP API platform.   Open-source embedding models and tools.   All-in-one workspace and collaboration platform.   AI-powered search and understanding platform.   NVIDIA's AI computing platform and models.   Connected note-taking and knowledge management.   Distributed relational database system.   Oracle Cloud Infrastructure AI services.   Efficient AI compute and model serving.   Run Large Language Models (LLMs) locally.   RDF database and semantic graph platform.   GPT models and comprehensive AI platform.   Safe, Open, High-Performance \u2014 PDF for AI   AI model training and fine-tuning platform.   Operating LLMs in production environment.   Distributed search and analytics suite.   Weather data and forecasting API.   Oracle's AI and machine learning services.   Team knowledge base and wiki platform.   Structured generation for language models.   Web scraping and proxy services.   Data analysis and manipulation library.   AI-powered web search and content extraction for LLMs.   Real-time news and media monitoring.   Authorization and access control platform.   AI-powered search and reasoning engine.   Distributed inference for Large Language Models.   PostgreSQL vector embedding extensions.   Vector similarity search for PostgreSQL.   Managed vector database for ML applications.   ML pipeline and model deployment platform.   AI-powered content moderation platform.   Document-loaders for various file formats.   AI gateway and observability platform.   Fine-tuning platform for Large Language Models.   AI model security and compliance platform.   AI platform for model deployment and management.   Wallets and payments for AI agents.   Logic programming language integration.   Prompt engineering and observability platform.   Universal API for SaaS integrations.   Biomedical literature database access.   Markdown content extraction and processing.   Conversational AI model platform.   PDF processing optimized for LLM ingestion.   Vector similarity search engine.   RAG toolkit with ColBERT indexing.   BM25 ranking algorithm implementation.   Scalable model serving framework.   Prompt injection detection and prevention.   Social media platform integration and APIs.   In-memory data structure store and cache.   AI memory and context management.   Cloud platform for running ML models.   Research and note-taking platform.   Python automation and RPA platform.   Real-time analytics database platform.   GPU cloud platform for AI workloads.   CRM platform and business automation.   AI platform with specialized hardware.   Enterprise software and AI solutions.   AI-powered web scraping framework.   Web scraping API and proxy service.   Real-time search engine results API.   Privacy-respecting metasearch engine.   Vector database for semantic search.   Google Search results scraping API.   Decentralized AI inference protocol.   Distributed database with vector capabilities.   Machine learning library for Python.   Business communication and collaboration.   Cloud data platform and analytics.   Industrial-strength NLP library.   Unified analytics engine for big data.   iFlytek's multilingual language model.   Payment orchestration platform.   Embedded relational database engine.   Q\\&A platform network integration.   High-performance analytical database.   GPU cloud platform for ML acceleration.   Web app framework for data science.   Online payment processing platform.   Open-source Firebase alternative.   Multi-model database for modern applications.   Conversation intelligence platform.   Data visualization and business intelligence.   Project management platform for agile teams.   Alibaba Cloud's in-memory database.   AI-optimized search API for applications.   Messaging platform and bot integration.   Tencent Cloud AI services and models.   Collection of ready-to-use datasets.   Data infrastructure for ML applications.   Distributed SQL database platform.   Scalable graph database and analytics.   Globally distributed database platform.   Entity resolution and data matching.   Semantic layer for data integration and querying.   Fast inference for open-source models.   HTML to Markdown conversion utility.   Extended toolkit for LangChain applications.   Big data platform and analytics suite.   Visual project management and collaboration.   LLM evaluation and analytics platform.   ML platform for model deployment.   Evaluation framework for LLM applications.   Social media platform integration.   Fast and typo-tolerant search engine.   Data extraction and processing platform.   Document processing and data extraction.   Document AI and OCR platform.   Serverless data platform for Redis and Kafka.   ML observability and evaluation platform.   Single-file vector search engine.   AI platform for healthcare applications.   AI-powered data analysis platform.   Visual data management system.   Distributed vector search engine.   Neural search platform with built-in understanding.   Vector database and semantic search.   Big data serving engine for vector search.   Simple vector database for embeddings.   Embedding models and semantic search.   ML experiment tracking and collaboration.   Experiment tracking and model management.   LLM tracing and observability.   Weather data and forecasting services.   Open-source vector database with GraphQL.   Messaging platform integration and automation.   AI observability and data monitoring.   Wikipedia content access and search.   Computational knowledge engine.   Enterprise models and tools for building, activating, and supervising AI agents.   xAI's Grok models for conversational AI.   Serverless database with vector search.   Distributed inference framework for LLMs.   Yahoo services and data integration.   Yandex AI services and language models.   Yandex Database distributed storage system.   AI agent framework and development platform.   Data warehouse and analytics platform.   01.AI's bilingual language models.   You.com search engine and AI platform.   Video platform integration and content access.   Long-term memory for AI assistants.   High-performance vector database.   ChatGLM and other Chinese language models.   Managed Milvus vector database service.   Reference management and research tool.   \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/all\\_providers.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/integrations/providers/anthropic.md",
      "title": null,
      "raw_content": "# Anthropic (Claude) This page covers all LangChain integrations with [Anthropic](https://www.anthropic.com/), the makers of Claude. ## Model interfaces   Anthropic chat models.   (Legacy) Anthropic text completion models.   ## Middleware Middleware specifically designed for Anthropic's Claude models. Learn more about [middleware](/oss/python/langchain/middleware/overview). | Middleware | Description | | --------------------------------- | -------------------------------------------------------------- | | [Prompt caching](#prompt-caching) | Reduce costs by caching repetitive prompt prefixes | | [Bash tool](#bash-tool) | Execute Claude's native bash tool with local command execution | | [Text editor](#text-editor) | Provide Claude's text editor tool for file editing | | [Memory](#memory) | Provide Claude's memory tool for persistent agent memory | | [File search](#file-search) | Search tools for state-based file systems | ### Prompt caching Reduce costs and latency by caching static or repetitive prompt content (like system prompts, tool definitions, and conversation history) on Anthropic's servers. This middleware implements a \\*\\*conversational caching strategy\\*\\* that places cache breakpoints after the most recent message, allowing the entire conversation history (including the latest user message) to be cached and reused in subsequent API calls. Prompt caching is useful for the following: \\* Applications with long, static system prompts that don't change between requests \\* Agents with many tool definitions that remain constant across invocations \\* Conversations where early message history is reused across multiple turns \\* High-volume deployments where reducing API costs and latency is critical  Learn more about [Anthropic prompt caching](https://platform.claude.com/docs/en/build-with-claude/prompt-caching#cache-limitations) strategies and limitations.  \\*\\*API reference:\\*\\* [`AnthropicPromptCachingMiddleware`](https://reference.langchain.com/python/integrations/langchain\\_anthropic/middleware/#langchain\\_anthropic.middleware.AnthropicPromptCachingMiddleware) ```python theme={null} from langchain\\_anthropic import ChatAnthropic from langchain\\_anthropic.middleware import AnthropicPromptCachingMiddleware from langchain.agents import create\\_agent agent = create\\_agent( model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"), system\\_prompt=\"\", middleware=[AnthropicPromptCachingMiddleware(ttl=\"5m\")], ) ```   Cache type. Only `'ephemeral'` is currently supported.   Time to live for cached content. Valid values: `'5m'` or `'1h'`   Minimum number of messages before caching starts   Behavior when using non-Anthropic models. Options: `'ignore'`, `'warn'`, or `'raise'`    The middleware caches content up to and including the latest message in each request. On subsequent requests within the TTL window (5 minutes or 1 hour), previously seen content is retrieved from cache rather than reprocessed, significantly reducing costs and latency. \\*\\*How it works:\\*\\* 1. First request: System prompt, tools, and the user message \"Hi, my name is Bob\" are sent to the API and cached 2. Second request: The cached content (system prompt, tools, and first message) is retrieved from cache. Only the new message \"What's my name?\" needs to be processed, plus the model's response from the first request 3. This pattern continues for each turn, with each request reusing the cached conversation history ```python theme={null} from langchain\\_anthropic import ChatAnthropic from langchain\\_anthropic.middleware import AnthropicPromptCachingMiddleware from langchain.agents import create\\_agent from langchain.messages import HumanMessage LONG\\_PROMPT = \"\"\" Please be a helpful assistant.  \"\"\" agent = create\\_agent( model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"), system\\_prompt=LONG\\_PROMPT, middleware=[AnthropicPromptCachingMiddleware(ttl=\"5m\")], ) # First invocation: Creates cache with system prompt, tools, and \"Hi, my name is Bob\" agent.invoke({\"messages\": [HumanMessage(\"Hi, my name is Bob\")]}) # Second invocation: Reuses cached system prompt, tools, and previous messages # Only processes the new message \"What's my name?\" and the previous AI response agent.invoke({\"messages\": [HumanMessage(\"What's my name?\")]}) ```  ### Bash tool Execute Claude's native `bash\\_20250124` tool with local command execution. The bash tool middleware is useful for the following: \\* Using Claude's built-in bash tool with local execution \\* Leveraging Claude's optimized bash tool interface \\* Agents that need persistent shell sessions with Anthropic models  This middleware wraps `ShellToolMiddleware` and exposes it as Claude's native bash tool.  \\*\\*API reference:\\*\\* [`ClaudeBashToolMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ClaudeBashToolMiddleware) ```python theme={null} from langchain\\_anthropic import ChatAnthropic from langchain\\_anthropic.middleware import ClaudeBashToolMiddleware from langchain.agents import create\\_agent agent = create\\_agent( model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"), tools=[], middleware=[ ClaudeBashToolMiddleware( workspace\\_root=\"/workspace\", ), ], ) ```  `ClaudeBashToolMiddleware` accepts all parameters from [`ShellToolMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ShellToolMiddleware), including:  Base directory for the shell session   Commands to run when the session starts   Execution policy (`HostExecutionPolicy`, `DockerExecutionPolicy`, or `CodexSandboxExecutionPolicy`)   Rules for sanitizing command output  See [Shell tool](/oss/python/langchain/middleware/built-in#shell-tool) for full configuration details.   ```python theme={null} from langchain\\_anthropic import ChatAnthropic from langchain\\_anthropic.middleware import ClaudeBashToolMiddleware from langchain.agents import create\\_agent from langchain.agents.middleware import DockerExecutionPolicy agent = create\\_agent( model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"), tools=[], middleware=[ ClaudeBashToolMiddleware( workspace\\_root=\"/workspace\", startup\\_commands=[\"pip install requests\"], execution\\_policy=DockerExecutionPolicy( image=\"python:3.11-slim\", ), ), ], ) # Claude can now use its native bash tool result = agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"List files in the workspace\"}] }) ```  ### Text editor Provide Claude's text editor tool (`text\\_editor\\_20250728`) for file creation and editing. The text editor middleware is useful for the following: \\* File-based agent workflows \\* Code editing and refactoring tasks \\* Multi-file project work \\* Agents that need persistent file storage  Available in two variants: \\*\\*State-based\\*\\* (files in LangGraph state) and \\*\\*Filesystem-based\\*\\* (files on disk).  \\*\\*API reference:\\*\\* [`StateClaudeTextEditorMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.StateClaudeTextEditorMiddleware), [`FilesystemClaudeTextEditorMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.FilesystemClaudeTextEditorMiddleware) ```python theme={null} from langchain\\_anthropic import ChatAnthropic from langchain\\_anthropic.middleware import StateClaudeTextEditorMiddleware from langchain.agents import create\\_agent # State-based (files in LangGraph state) agent = create\\_agent( model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"), tools=[], middleware=[ StateClaudeTextEditorMiddleware(), ], ) ```  \\*\\*[`StateClaudeTextEditorMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.StateClaudeTextEditorMiddleware) (state-based)\\*\\*  Optional list of allowed path prefixes. If specified, only paths starting with these prefixes are allowed.  \\*\\*[`FilesystemClaudeTextEditorMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.FilesystemClaudeTextEditorMiddleware) (filesystem-based)\\*\\*  Root directory for file operations   Optional list of allowed virtual path prefixes (default: `[\"/\"]`)   Maximum file size in MB    Claude's text editor tool supports the following commands: \\* `view` - View file contents or list directory \\* `create` - Create a new file \\* `str\\_replace` - Replace string in file \\* `insert` - Insert text at line number \\* `delete` - Delete a file \\* `rename` - Rename/move a file ```python theme={null} from langchain\\_anthropic import ChatAnthropic from langchain\\_anthropic.middleware import ( StateClaudeTextEditorMiddleware, FilesystemClaudeTextEditorMiddleware, ) from langchain.agents import create\\_agent # State-based: Files persist in LangGraph state agent\\_state = create\\_agent( model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"), tools=[], middleware=[ StateClaudeTextEditorMiddleware( allowed\\_path\\_prefixes=[\"/project\"], ), ], ) # Filesystem-based: Files persist on disk agent\\_fs = create\\_agent( model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"), tools=[], middleware=[ FilesystemClaudeTextEditorMiddleware( root\\_path=\"/workspace\", allowed\\_prefixes=[\"/src\"], max\\_file\\_size\\_mb=10, ), ], ) ```  ### Memory Provide Claude's memory tool (`memory\\_20250818`) for persistent agent memory across conversation turns. The memory middleware is useful for the following: \\* Long-running agent conversations \\* Maintaining context across interruptions \\* Task progress tracking \\* Persistent agent state management  Claude's memory tool uses a `/memories` directory and automatically injects a system prompt encouraging the agent to check and update memory.  \\*\\*API reference:\\*\\* [`StateClaudeMemoryMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.StateClaudeMemoryMiddleware), [`FilesystemClaudeMemoryMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.FilesystemClaudeMemoryMiddleware) ```python theme={null} from langchain\\_anthropic import ChatAnthropic from langchain\\_anthropic.middleware import StateClaudeMemoryMiddleware from langchain.agents import create\\_agent # State-based memory agent = create\\_agent( model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"), tools=[], middleware=[ StateClaudeMemoryMiddleware(), ], ) ```  \\*\\*[`StateClaudeMemoryMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.StateClaudeMemoryMiddleware) (state-based)\\*\\*  Optional list of allowed path prefixes. Defaults to `[\"/memories\"]`.   System prompt to inject. Defaults to Anthropic's recommended memory prompt that encourages the agent to check and update memory.  \\*\\*[`FilesystemClaudeMemoryMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.FilesystemClaudeMemoryMiddleware) (filesystem-based)\\*\\*  Root directory for file operations   Optional list of allowed virtual path prefixes. Defaults to `[\"/memories\"]`.   Maximum file size in MB   System prompt to inject    ```python theme={null} from langchain\\_anthropic import ChatAnthropic from langchain\\_anthropic.middleware import ( StateClaudeMemoryMiddleware, FilesystemClaudeMemoryMiddleware, ) from langchain.agents import create\\_agent # State-based: Memory persists in LangGraph state agent\\_state = create\\_agent( model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"), tools=[], middleware=[ StateClaudeMemoryMiddleware(), ], ) # Filesystem-based: Memory persists on disk agent\\_fs = create\\_agent( model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"), tools=[], middleware=[ FilesystemClaudeMemoryMiddleware( root\\_path=\"/workspace\", ), ], ) # The agent will automatically: # 1. Check /memories directory at start # 2. Record progress and thoughts during execution # 3. Update memory files as work progresses ```  ### File search Provide Glob and Grep search tools for files stored in LangGraph state. File search middleware is useful for the following: \\* Searching through state-based virtual file systems \\* Works with text editor and memory tools \\* Finding files by patterns \\* Content search with regex \\*\\*API reference:\\*\\* [`StateFileSearchMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.StateFileSearchMiddleware) ```python theme={null} from langchain\\_anthropic import ChatAnthropic from langchain\\_anthropic.middleware import ( StateClaudeTextEditorMiddleware, StateFileSearchMiddleware, ) from langchain.agents import create\\_agent agent = create\\_agent( model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"), tools=[], middleware=[ StateClaudeTextEditorMiddleware(), StateFileSearchMiddleware(), # Search text editor files ], ) ```   State key containing files to search. Use `\"text\\_editor\\_files\"` for text editor files or `\"memory\\_files\"` for memory files.    The middleware adds Glob and Grep search tools that work with state-based files. ```python theme={null} from langchain\\_anthropic import ChatAnthropic from langchain\\_anthropic.middleware import ( StateClaudeTextEditorMiddleware, StateClaudeMemoryMiddleware, StateFileSearchMiddleware, ) from langchain.agents import create\\_agent # Search text editor files agent = create\\_agent( model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"), tools=[], middleware=[ StateClaudeTextEditorMiddleware(), StateFileSearchMiddleware(state\\_key=\"text\\_editor\\_files\"), ], ) # Search memory files agent\\_memory = create\\_agent( model=ChatAnthropic(model=\"claude-sonnet-4-5-20250929\"), tools=[], middleware=[ StateClaudeMemoryMiddleware(), StateFileSearchMiddleware(state\\_key=\"memory\\_files\"), ], ) ```  \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/anthropic.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/integrations/providers/aws.md",
      "title": null,
      "raw_content": "# AWS (Amazon) This page covers all LangChain integrations with the [Amazon Web Services (AWS)](https://aws.amazon.com/) platform. ## Chat models ### Bedrock Chat > [Amazon Bedrock](https://aws.amazon.com/bedrock/) is a fully managed service that offers a choice of > high-performing foundation models (FMs) from leading AI companies like `AI21 Labs`, `Anthropic`, `Cohere`, > `Meta`, `Stability AI`, and `Amazon` via a single API, along with a broad set of capabilities you need to > build generative AI applications with security, privacy, and responsible AI. Using `Amazon Bedrock`, > you can easily experiment with and evaluate top FMs for your use case, privately customize them with > your data using techniques such as fine-tuning and `Retrieval Augmented Generation` (`RAG`), and build > agents that execute tasks using your enterprise systems and data sources. Since `Amazon Bedrock` is > serverless, you don't have to manage any infrastructure, and you can securely integrate and deploy > generative AI capabilities into your applications using the AWS services you are already familiar with. See a [usage example](/oss/python/integrations/chat/bedrock). ```python theme={null} from langchain\\_aws import ChatBedrock ``` ### Bedrock Converse AWS Bedrock maintains a [Converse API](https://docs.aws.amazon.com/bedrock/latest/APIReference/API\\_runtime\\_Converse.html) that provides a unified conversational interface for Bedrock models. This API does not yet support custom models. You can see a list of all [models that are supported here](https://docs.aws.amazon.com/bedrock/latest/userguide/conversation-inference.html).  \\*\\*We recommend the Converse API for users who do not need to use custom models. It can be accessed using [ChatBedrockConverse](https://python.langchain.com/api\\_reference/aws/chat\\_models/langchain\\_aws.chat\\_models.bedrock\\_converse.ChatBedrockConverse.html).\\*\\*  See a [usage example](/oss/python/integrations/chat/bedrock). ```python theme={null} from langchain\\_aws import ChatBedrockConverse ``` ## LLMs ### Bedrock See a [usage example](/oss/python/integrations/llms/bedrock). ```python theme={null} from langchain\\_aws import BedrockLLM ``` ### Amazon API Gateway > [Amazon API Gateway](https://aws.amazon.com/api-gateway/) is a fully managed service that makes it easy for > developers to create, publish, maintain, monitor, and secure APIs at any scale. APIs act as the \"front door\" > for applications to access data, business logic, or functionality from your backend services. Using > `API Gateway`, you can create RESTful APIs and WebSocket APIs that enable real-time two-way communication > applications. `API Gateway` supports containerized and serverless workloads, as well as web applications. > > `API Gateway` handles all the tasks involved in accepting and processing up to hundreds of thousands of > concurrent API calls, including traffic management, CORS support, authorization and access control, > throttling, monitoring, and API version management. `API Gateway` has no minimum fees or startup costs. > You pay for the API calls you receive and the amount of data transferred out and, with the `API Gateway` > tiered pricing model, you can reduce your cost as your API usage scales. See a [usage example](/oss/python/integrations/llms/amazon\\_api\\_gateway). ```python theme={null} from langchain\\_community.llms import AmazonAPIGateway ``` ### SageMaker Endpoint > [Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a system that can build, train, and deploy > machine learning (ML) models with fully managed infrastructure, tools, and workflows. We use `SageMaker` to host our model and expose it as the `SageMaker Endpoint`. See a [usage example](/oss/python/integrations/llms/sagemaker). ```python theme={null} from langchain\\_aws import SagemakerEndpoint ``` ## Embedding Models ### Bedrock See a [usage example](/oss/python/integrations/text\\_embedding/bedrock). ```python theme={null} from langchain\\_aws import BedrockEmbeddings ``` ### SageMaker Endpoint See a [usage example](/oss/python/integrations/text\\_embedding/sagemaker-endpoint). ```python theme={null} from langchain\\_community.embeddings import SagemakerEndpointEmbeddings from langchain\\_community.llms.sagemaker\\_endpoint import ContentHandlerBase ``` ## Document loaders ### AWS S3 Directory and File > [Amazon Simple Storage Service (Amazon S3)](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-folders.html) > is an object storage service. > [AWS S3 Directory](https://docs.aws.amazon.com/AmazonS3/latest/userguide/using-folders.html) > [AWS S3 Buckets](https://docs.aws.amazon.com/AmazonS3/latest/userguide/UsingBucket.html) See a [usage example for S3DirectoryLoader](/oss/python/integrations/document\\_loaders/aws\\_s3\\_directory). See a [usage example for S3FileLoader](/oss/python/integrations/document\\_loaders/aws\\_s3\\_file). ```python theme={null} from langchain\\_community.document\\_loaders import S3DirectoryLoader, S3FileLoader ``` ### Amazon Textract > [Amazon Textract](https://docs.aws.amazon.com/managedservices/latest/userguide/textract.html) is a machine > learning (ML) service that automatically extracts text, handwriting, and data from scanned documents. See a [usage example](/oss/python/integrations/document\\_loaders/amazon\\_textract). ```python theme={null} from langchain\\_community.document\\_loaders import AmazonTextractPDFLoader ``` ### Amazon Athena > [Amazon Athena](https://aws.amazon.com/athena/) is a serverless, interactive analytics service built > on open-source frameworks, supporting open-table and file formats. See a [usage example](/oss/python/integrations/document\\_loaders/athena). ```python theme={null} from langchain\\_community.document\\_loaders.athena import AthenaLoader ``` ### AWS Glue > The [AWS Glue Data Catalog](https://docs.aws.amazon.com/en\\_en/glue/latest/dg/catalog-and-crawler.html) is a centralized metadata > repository that allows you to manage, access, and share metadata about > your data stored in AWS. It acts as a metadata store for your data assets, > enabling various AWS services and your applications to query and connect > to the data they need efficiently. See a [usage example](/oss/python/integrations/document\\_loaders/glue\\_catalog). ```python theme={null} from langchain\\_community.document\\_loaders.glue\\_catalog import GlueCatalogLoader ``` ## Vector stores ### Amazon OpenSearch Service > [Amazon OpenSearch Service](https://aws.amazon.com/opensearch-service/) performs > interactive log analytics, real-time application monitoring, website search, and more. `OpenSearch` is > an open source, > distributed search and analytics suite derived from `Elasticsearch`. `Amazon OpenSearch Service` offers the > latest versions of `OpenSearch`, support for many versions of `Elasticsearch`, as well as > visualization capabilities powered by `OpenSearch Dashboards` and `Kibana`. We need to install several python libraries.  ```bash pip theme={null} pip install boto3 requests requests-aws4auth ``` ```bash uv theme={null} uv add boto3 requests requests-aws4auth ```  See a [usage example](/oss/python/integrations/vectorstores/opensearch#using-aos-amazon-opensearch-service). ```python theme={null} from langchain\\_community.vectorstores import OpenSearchVectorSearch ``` ### Amazon DocumentDB Vector Search > [Amazon DocumentDB (with MongoDB Compatibility)](https://docs.aws.amazon.com/documentdb/) makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud. > With Amazon DocumentDB, you can run the same application code and use the same drivers and tools that you use with MongoDB. > Vector search for Amazon DocumentDB combines the flexibility and rich querying capability of a JSON-based document database with the power of vector search. #### Installation and Setup See [detail configuration instructions](/oss/python/integrations/vectorstores/documentdb). We need to install the `pymongo` python package.  ```bash pip theme={null} pip install pymongo ``` ```bash uv theme={null} uv add pymongo ```  #### Deploy DocumentDB on AWS [Amazon DocumentDB (with MongoDB Compatibility)](https://docs.aws.amazon.com/documentdb/) is a fast, reliable, and fully managed database service. Amazon DocumentDB makes it easy to set up, operate, and scale MongoDB-compatible databases in the cloud. AWS offers services for computing, databases, storage, analytics, and other functionality. For an overview of all AWS services, see [Cloud Computing with Amazon Web Services](https://aws.amazon.com/what-is-aws/). See a [usage example](/oss/python/integrations/vectorstores/documentdb). ```python theme={null} from langchain\\_community.vectorstores import DocumentDBVectorSearch ``` ### Amazon MemoryDB [Amazon MemoryDB](https://aws.amazon.com/memorydb/) is a durable, in-memory database service that delivers ultra-fast performance. MemoryDB is compatible with Redis OSS, a popular open source data store, enabling you to quickly build applications using the same flexible and friendly Redis OSS APIs, and commands that they already use today. InMemoryVectorStore class provides a vectorstore to connect with Amazon MemoryDB. ```python theme={null} from langchain\\_aws.vectorstores.inmemorydb import InMemoryVectorStore vds = InMemoryVectorStore.from\\_documents( chunks, embeddings, redis\\_url=\"rediss://cluster\\_endpoint:6379/ssl=True ssl\\_cert\\_reqs=none\", vector\\_schema=vector\\_schema, index\\_name=INDEX\\_NAME, ) ``` See a [usage example](/oss/python/integrations/vectorstores/memorydb). ## Retrievers ### Amazon Kendra > [Amazon Kendra](https://docs.aws.amazon.com/kendra/latest/dg/what-is-kendra.html) is an intelligent search service > provided by `Amazon Web Services` (`AWS`). It utilizes advanced natural language processing (NLP) and machine > learning algorithms to enable powerful search capabilities across various data sources within an organization. > `Kendra` is designed to help users find the information they need quickly and accurately, > improving productivity and decision-making. > With `Kendra`, we can search across a wide range of content types, including documents, FAQs, knowledge bases, > manuals, and websites. It supports multiple languages and can understand complex queries, synonyms, and > contextual meanings to provide highly relevant search results. We need to install the `langchain-aws` library.  ```bash pip theme={null} pip install langchain-aws ``` ```bash uv theme={null} uv add langchain-aws ```  See a [usage example](/oss/python/integrations/retrievers/amazon\\_kendra\\_retriever). ```python theme={null} from langchain\\_aws import AmazonKendraRetriever ``` ### Amazon Bedrock (Knowledge Bases) > [Knowledge bases for Amazon Bedrock](https://aws.amazon.com/bedrock/knowledge-bases/) is an > `Amazon Web Services` (`AWS`) offering which lets you quickly build RAG applications by using your > private data to customize foundation model response. We need to install the `langchain-aws` library.  ```bash pip theme={null} pip install langchain-aws ``` ```bash uv theme={null} uv add langchain-aws ```  See a [usage example](/oss/python/integrations/retrievers/bedrock). ```python theme={null} from langchain\\_aws import AmazonKnowledgeBasesRetriever ``` ## Tools ### AWS Lambda > [`Amazon AWS Lambda`](https://aws.amazon.com/pm/lambda/) is a serverless computing service provided by > `Amazon Web Services` (`AWS`). It helps developers to build and run applications and services without > provisioning or managing servers. This serverless architecture enables you to focus on writing and > deploying code, while AWS automatically takes care of scaling, patching, and managing the > infrastructure required to run your applications. We need to install `boto3` python library.  ```bash pip theme={null} pip install boto3 ``` ```bash uv theme={null} uv add boto3 ```  See a [usage example](/oss/python/integrations/tools/awslambda). ```python theme={null} from langchain\\_community.chat\\_message\\_histories import DynamoDBChatMessageHistory ``` ## Graphs ### Amazon Neptune > [Amazon Neptune](https://aws.amazon.com/neptune/) > is a high-performance graph analytics and serverless database for superior scalability and availability. For the Cypher and SPARQL integrations below, we need to install the `langchain-aws` library.  ```bash pip theme={null} pip install langchain-aws ``` ```bash uv theme={null} uv add langchain-aws ```  ### Amazon Neptune with Cypher See a [usage example](/oss/python/integrations/graphs/amazon\\_neptune\\_open\\_cypher). ```python theme={null} from langchain\\_aws.graphs import NeptuneGraph from langchain\\_aws.graphs import NeptuneAnalyticsGraph from langchain\\_aws.chains import create\\_neptune\\_opencypher\\_qa\\_chain ``` ### Amazon Neptune with SPARQL ```python theme={null} from langchain\\_aws.graphs import NeptuneRdfGraph from langchain\\_aws.chains import create\\_neptune\\_sparql\\_qa\\_chain ``` ## Callbacks ### Bedrock token usage ```python theme={null} from langchain\\_community.callbacks.bedrock\\_anthropic\\_callback import BedrockAnthropicTokenUsageCallbackHandler ``` ### SageMaker Tracking > [Amazon SageMaker](https://aws.amazon.com/sagemaker/) is a fully managed service that is used to quickly > and easily build, train and deploy machine learning (ML) models. > [Amazon SageMaker Experiments](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments.html) is a capability > of `Amazon SageMaker` that lets you organize, track, > compare and evaluate ML experiments and model versions. We need to install several python libraries.  ```bash pip theme={null} pip install google-search-results sagemaker ``` ```bash uv theme={null} uv add google-search-results sagemaker ```  See a [usage example](/oss/python/integrations/callbacks/sagemaker\\_tracking). ```python theme={null} from langchain\\_community.callbacks import SageMakerCallbackHandler ``` ## Chains ### Amazon Comprehend Moderation Chain > [Amazon Comprehend](https://aws.amazon.com/comprehend/) is a natural-language processing (NLP) service that > uses machine learning to uncover valuable insights and connections in text. We need to install the `boto3` and `nltk` libraries.  ```bash pip theme={null} pip install boto3 nltk ``` ```bash uv theme={null} uv add boto3 nltk ```  See a [usage example](https://python.langchain.com/v0.1/docs/guides/productionization/safety/amazon\\_comprehend\\_chain/). ```python theme={null} from langchain\\_experimental.comprehend\\_moderation import AmazonComprehendModerationChain ``` \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/aws.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/integrations/providers/google.md",
      "title": null,
      "raw_content": "# Google This page covers all LangChain integrations with [Google Gemini](https://ai.google.dev/gemini-api/docs), [Google Cloud](https://cloud.google.com/), and other Google products (such as Google Maps, YouTube, and [more](#other-google-products)). Not sure which to use?   Access Google Gemini models directly via the [Gemini Developer API](https://ai.google.dev/). This is often the best starting point for individual developers. [See integrations.](#google-generative-ai)   Access Gemini models, Vertex AI Model Garden and a wide range of cloud services (databases, storage, document AI, etc.) via the [Google Cloud Platform](https://cloud.google.com/). Use the `langchain-google-vertexai` package for Vertex AI models and specific packages (e.g., `langchain-google-community`, `langchain-google-cloud-sql-pg`) for other cloud services. This is ideal for developers already using Google Cloud or needing enterprise features like MLOps, specific model tuning or enterprise support. [See integrations.](#google-cloud)   See Google's guide on [migrating from the Gemini API to Vertex AI](https://ai.google.dev/gemini-api/docs/migrate-to-cloud) for more details on the differences.  Integration packages for Gemini models and the Vertex AI platform are maintained in the [`langchain-google`](https://github.com/langchain-ai/langchain-google) repository. You can find a host of LangChain integrations with other Google APIs and services in the `langchain-google-community` package (listed on this page) and the [`googleapis`](https://github.com/orgs/googleapis/repositories?q=langchain) Github organization.  \\*\\*\\* ## Google Generative AI Access Google Gemini models directly using the [Gemini Developer API](https://ai.google.dev/gemini-api/docs), best suited for rapid development and experimentation. ### Chat models   Google Gemini chat models via the Gemini Developer API.   ### LLMs   Access the same Gemini models using the (legacy) LLM text completion interface.   ### Embedding models   Gemini embedding models.   \\*\\*\\* ## Google Cloud Access Gemini models, Vertex AI Model Garden and other Google Cloud services via [Vertex AI](https://docs.cloud.google.com/vertex-ai/generative-ai/docs) and specific cloud integrations. ### Chat models   Access chat models like Gemini via the Vertex AI platform.     ```python wrap theme={null} from langchain\\_google\\_vertexai.model\\_garden\\_maas.llama import VertexModelGardenLlama ```   ```python wrap theme={null} from langchain\\_google\\_vertexai.model\\_garden\\_maas.mistral import VertexModelGardenMistral ```   Local Gemma model loaded from HuggingFace. ```python wrap theme={null} from langchain\\_google\\_vertexai.gemma import GemmaChatLocalHF ```   Local Gemma model loaded from Kaggle. ```python wrap theme={null} from langchain\\_google\\_vertexai.gemma import GemmaChatLocalKaggle ```   ```python wrap theme={null} from langchain\\_google\\_vertexai.gemma import GemmaChatVertexAIModelGarden ```   Implementation of the Image Captioning model as a chat. ```python wrap theme={null} from langchain\\_google\\_vertexai.vision\\_models import VertexAIImageCaptioningChat ```   Given an image and a prompt, edit the image. Currently only supports mask-free editing. ```python wrap theme={null} from langchain\\_google\\_vertexai.vision\\_models import VertexAIImageEditorChat ```   Generates an image from a prompt. ```python wrap theme={null} from langchain\\_google\\_vertexai.vision\\_models import VertexAIImageGeneratorChat ```   Chat implementation of a visual QnA model. ```python wrap theme={null} from langchain\\_google\\_vertexai.vision\\_models import VertexAIVisualQnAChat ```   ### LLMs (legacy) string-in, string-out LLM interface.   Access Gemini, and hundreds of OSS models via Vertex AI Model Garden service.   Gemma:   Local Gemma model loaded from HuggingFace. ```python wrap theme={null} from langchain\\_google\\_vertexai.gemma import GemmaLocalHF ```   Local Gemma model loaded from Kaggle. ```python wrap theme={null} from langchain\\_google\\_vertexai.gemma import GemmaLocalKaggle ```   ```python wrap theme={null} from langchain\\_google\\_vertexai.gemma import GemmaVertexAIModelGarden ```   Implementation of the Image Captioning model as an LLM. ```python wrap theme={null} from langchain\\_google\\_vertexai.vision\\_models import VertexAIImageCaptioning ```   ### Embedding models   Generate embeddings using models deployed on Vertex AI.   ### Document loaders Load documents from various Google Cloud sources.   Google Cloud AlloyDB is a fully managed PostgreSQL-compatible database service.   Google Cloud BigQuery is a serverless data warehouse.   Google Cloud Bigtable is a fully managed NoSQL Big Data database service.   Google Cloud SQL for MySQL is a fully-managed MySQL database service.   Google Cloud SQL for SQL Server is a fully-managed SQL Server database service.   Google Cloud SQL for PostgreSQL is a fully-managed PostgreSQL database service.   Google Cloud Storage is a managed service for storing unstructured data.   Google Cloud Storage is a managed service for storing unstructured data.   Google El Carro Oracle Operator runs Oracle databases in Kubernetes.   Google Cloud Firestore is a NoSQL document database.   Google Cloud Firestore in Datastore mode   Google Cloud Memorystore for Redis is a fully managed Redis service.   Google Cloud Spanner is a fully managed, globally distributed relational database service.   Google Cloud Speech-to-Text transcribes audio files.    Load data using Google Cloud Vision API. ```python theme={null} from langchain\\_google\\_community.vision import CloudVisionLoader ```  ### Document transformers Transform documents using Google Cloud services.   Transform unstructured data from documents into structured data, making it easier to understand, analyze, and consume.   Translate text and HTML with the Google Cloud Translation API.   ### Vector stores Store and search vectors using Google Cloud databases and Vertex AI Vector Search.   Google Cloud AlloyDB is a fully managed relational database service that offers high performance, seamless integration, and impressive scalability on Google Cloud. AlloyDB is 100% compatible with PostgreSQL.   BigQuery vector search lets you use GoogleSQL to do semantic search, using vector indexes for fast but approximate results, or using brute force for exact results.   Vector store using Memorystore for Redis   Vector store using Cloud Spanner   Vector store using Firestore   Vector store using Cloud SQL for MySQL   Vector store using Cloud SQL for PostgreSQL.   Formerly known as Vertex AI Matching Engine, provides a low latency vector database. These vector databases are commonly referred to as vector similarity-matching or an approximate nearest neighbor (ANN) service.   Vector search using Datastore for document storage.   ### Retrievers Retrieve information using Google Cloud services.   Build generative AI powered search engines using Vertex AI Search   Search, store, and manage documents using Document AI Warehouse.   ```python Other retrievers theme={null} from langchain\\_google\\_community import VertexAIMultiTurnSearchRetriever from langchain\\_google\\_community import VertexAISearchRetriever from langchain\\_google\\_community import VertexAISearchSummaryTool ``` ### Tools Integrate agents with various Google Cloud services.   Google Cloud Text-to-Speech synthesizes natural-sounding speech with 100+ voices in multiple languages.   ### Callbacks Track LLM/Chat model usage.   Callback Handler that tracks `VertexAI` usage info. ```python wrap theme={null} from langchain\\_google\\_vertexai.callbacks import VertexAICallbackHandler ```   ### Evaluators Evaluate model outputs using Vertex AI.   Pair-wise evaluation using Vertex AI models. ```python wrap theme={null} from langchain\\_google\\_vertexai.evaluators.evaluation import VertexPairWiseStringEvaluator ```   Evaluate a single prediction string using Vertex AI models. ```python wrap theme={null} from langchain\\_google\\_vertexai.evaluators.evaluation import VertexStringEvaluator ```   \\*\\*\\* ## Other Google products Integrations with various Google services beyond the core Cloud Platform. ### Document loaders   Google Drive file storage. Currently supports Google Docs.   ### Vector stores   ScaNN is a method for efficient vector similarity search at scale.   ### Retrievers   Retrieve documents from Google Drive.   ### Tools   Perform web searches using Google Custom Search Engine (CSE).   Tools for interacting with Google Drive.   Query financial data.   Query job listings.   Perform visual searches.   Search for places information.   Search academic papers.   Query Google Trends data.   ### MCP   Simple and efficient way to connect to your databases, including those on Google Cloud like Cloud SQL and AlloyDB   ### Toolkits Collections of tools for specific Google services.   Toolkit to create, get, search, and send emails using the Gmail API.   ### Chat loaders   Load chat history from Gmail threads.   \\*\\*\\* ## 3rd party integrations Access Google services via unofficial third-party APIs. ### Search   searchapi.io provides API access to Google search results, YouTube, and more.   SerpApi provides API access to Google search results.   serper.dev provides API access to Google search results.   ### YouTube   Search YouTube videos without the official API.   Download audio from YouTube videos.   Load video transcripts.   \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/google.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/integrations/providers/groq.md",
      "title": null,
      "raw_content": "# Groq  This page makes reference to [Groq](https://console.groq.com/docs/overview), an AI hardware and software company. For information on how to use Grok models (provided by [xAI](https://docs.x.ai/docs/overview)), see the [xAI provider page](/oss/python/integrations/providers/xai).  > [Groq](https://groq.com) developed the world's first Language Processing Unit\u2122, or `LPU`. > The `Groq LPU` has a deterministic, single core streaming architecture that sets the standard > for GenAI inference speed with predictable and repeatable performance for any given workload. > > Beyond the architecture, `Groq` software is designed to empower developers like you with > the tools you need to create innovative, powerful AI applications. > > With Groq as your engine, you can: > > \\* Achieve uncompromised low latency and performance for real-time AI and HPC inferences > \\* Know the exact performance and compute time for any given workload > \\* Take advantage of our cutting-edge technology to stay ahead of the competition ## Installation and Setup Install the integration package:  ```bash pip theme={null} pip install langchain-groq ``` ```bash uv theme={null} uv add langchain-groq ```  Request an [API key](https://console.groq.com/login?utm\\_source=langchain\\&utm\\_content=provider\\_page) and set it as an environment variable: ```bash theme={null} export GROQ\\_API\\_KEY=gsk\\_... ``` ## Chat models See a [usage example](/oss/python/integrations/chat/groq). ```python theme={null} from langchain\\_groq import ChatGroq ``` \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/groq.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/integrations/providers/huggingface.md",
      "title": null,
      "raw_content": "# Hugging Face This page covers all LangChain integrations with [Hugging Face Hub](https://huggingface.co/) and libraries like [transformers](https://huggingface.co/docs/transformers/index), [sentence transformers](https://sbert.net/), and [datasets](https://huggingface.co/docs/datasets/index). ## Chat models ### ChatHuggingFace We can use the `Hugging Face` LLM classes or directly use the `ChatHuggingFace` class. See a [usage example](/oss/python/integrations/chat/huggingface). ```python theme={null} from langchain\\_huggingface import ChatHuggingFace ``` ## LLMs ### HuggingFaceEndpoint We can use the `HuggingFaceEndpoint` class to run open source models via serverless [Inference Providers](https://huggingface.co/docs/inference-providers) or via dedicated [Inference Endpoints](https://huggingface.co/inference-endpoints/dedicated). See a [usage example](/oss/python/integrations/llms/huggingface\\_endpoint). ```python theme={null} from langchain\\_huggingface import HuggingFaceEndpoint ``` ### HuggingFacePipeline We can use the `HuggingFacePipeline` class to run open source models locally. See a [usage example](/oss/python/integrations/llms/huggingface\\_pipelines). ```python theme={null} from langchain\\_huggingface import HuggingFacePipeline ``` ## Embedding Models ### HuggingFaceEmbeddings We can use the `HuggingFaceEmbeddings` class to run open source embedding models locally. See a [usage example](/oss/python/integrations/text\\_embedding/huggingfacehub). ```python theme={null} from langchain\\_huggingface import HuggingFaceEmbeddings ``` ### HuggingFaceEndpointEmbeddings We can use the `HuggingFaceEndpointEmbeddings` class to run open source embedding models via a dedicated [Inference Endpoint](https://huggingface.co/inference-endpoints/dedicated). See a [usage example](/oss/python/integrations/text\\_embedding/huggingfacehub). ```python theme={null} from langchain\\_huggingface import HuggingFaceEndpointEmbeddings ``` ### HuggingFaceInferenceAPIEmbeddings We can use the `HuggingFaceInferenceAPIEmbeddings` class to run open source embedding models via [Inference Providers](https://huggingface.co/docs/inference-providers). See a [usage example](/oss/python/integrations/text\\_embedding/huggingfacehub). ```python theme={null} from langchain\\_community.embeddings import HuggingFaceInferenceAPIEmbeddings ``` ### HuggingFaceInstructEmbeddings We can use the `HuggingFaceInstructEmbeddings` class to run open source embedding models locally. See a [usage example](/oss/python/integrations/text\\_embedding/instruct\\_embeddings). ```python theme={null} from langchain\\_community.embeddings import HuggingFaceInstructEmbeddings ``` ### HuggingFaceBgeEmbeddings > [BGE models on the HuggingFace](https://huggingface.co/BAAI/bge-large-en-v1.5) are one of [the best open-source embedding models](https://huggingface.co/spaces/mteb/leaderboard). > BGE model is created by the [Beijing Academy of Artificial Intelligence (BAAI)](https://en.wikipedia.org/wiki/Beijing\\_Academy\\_of\\_Artificial\\_Intelligence). `BAAI` is a private non-profit organization engaged in AI research and development. See a [usage example](/oss/python/integrations/text\\_embedding/bge\\_huggingface). ```python theme={null} from langchain\\_community.embeddings import HuggingFaceBgeEmbeddings ``` ## Document loaders ### Hugging Face dataset > [Hugging Face Hub](https://huggingface.co/docs/hub/index) is home to over 75,000 > [datasets](https://huggingface.co/docs/hub/index#datasets) in more than 100 languages > that can be used for a broad range of tasks across NLP, Computer Vision, and Audio. > They used for a diverse range of tasks such as translation, automatic speech > recognition, and image classification. We need to install `datasets` python package.  ```bash pip theme={null} pip install datasets ``` ```bash uv theme={null} uv add datasets ```  See a [usage example](/oss/python/integrations/document\\_loaders/hugging\\_face\\_dataset). ```python theme={null} from langchain\\_community.document\\_loaders.hugging\\_face\\_dataset import HuggingFaceDatasetLoader ``` ### Hugging Face model loader > Load model information from `Hugging Face Hub`, including README content. > > This loader interfaces with the `Hugging Face Models API` to fetch > and load model metadata and README files. > The API allows you to search and filter models based on > specific criteria such as model tags, authors, and more. ```python theme={null} from langchain\\_community.document\\_loaders import HuggingFaceModelLoader ``` ### Image captions It uses the Hugging Face models to generate image captions. We need to install several python packages.  ```bash pip theme={null} pip install transformers pillow ``` ```bash uv theme={null} uv add transformers pillow ```  See a [usage example](/oss/python/integrations/document\\_loaders/image\\_captions). ```python theme={null} from langchain\\_community.document\\_loaders import ImageCaptionLoader ``` ## Tools ### Hugging Face Hub Tools > [Hugging Face Tools](https://huggingface.co/docs/transformers/v4.29.0/en/custom\\_tools) > support text I/O and are loaded using the `load\\_huggingface\\_tool` function. We need to install several python packages.  ```bash pip theme={null} pip install transformers huggingface\\_hub ``` ```bash uv theme={null} uv add transformers huggingface\\_hub ```  See a [usage example](/oss/python/integrations/tools/huggingface\\_tools). ```python theme={null} from langchain\\_community.agent\\_toolkits.load\\_tools import load\\_huggingface\\_tool ``` ### Hugging Face Text-to-Speech Model Inference. > It is a wrapper around `OpenAI Text-to-Speech API`. ```python theme={null} from langchain\\_community.tools.audio import HuggingFaceTextToSpeechModelInference ``` \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/huggingface.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/integrations/providers/microsoft.md",
      "title": null,
      "raw_content": "# Microsoft This page covers all LangChain integrations with [Microsoft Azure](https://portal.azure.com) and other [Microsoft](https://www.microsoft.com) products. ## Chat models Microsoft offers three main options for accessing chat models through Azure: 1. [Azure OpenAI](https://learn.microsoft.com/en-us/azure/ai-services/openai/) - Provides access to OpenAI's powerful models like o3, 4.1, and other models through Microsoft Azure's secure enterprise platform. 2. [Azure AI](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/deploy-models) - Offers access to a variety of models from different providers including Anthropic, DeepSeek, Cohere, Phi and Mistral through a unified API. 3. [Azure ML](https://learn.microsoft.com/en-us/azure/machine-learning/) - Allows deployment and management of your own custom models or fine-tuned open-source models with Azure Machine Learning. ### Azure OpenAI > [Microsoft Azure](https://en.wikipedia.org/wiki/Microsoft\\_Azure), often referred to as `Azure` is a cloud computing platform run by `Microsoft`, which offers access, management, and development of applications and services through global data centers. It provides a range of capabilities, including software as a service (SaaS), platform as a service (PaaS), and infrastructure as a service (IaaS). `Microsoft Azure` supports many programming languages, tools, and frameworks, including Microsoft-specific and third-party software and systems. > [Azure OpenAI](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/) is an `Azure` service with powerful language models from OpenAI including the `GPT-3`, `Codex` and Embeddings model series for content generation, summarization, semantic search, and natural language to code translation.  ```bash pip theme={null} pip install langchain-openai ``` ```bash uv theme={null} uv add langchain-openai ```  Set the environment variables to get access to the `Azure OpenAI` service. ```python theme={null} import os os.environ[\"AZURE\\_OPENAI\\_ENDPOINT\"] = \"https:// [Azure AI Foundry](https://learn.microsoft.com/en-us/azure/developer/python/get-started) provides access to a wide range of models from various providers including Azure OpenAI, DeepSeek R1, Cohere, Phi and Mistral through the `AzureAIChatCompletionsModel` class.  ```bash pip theme={null} pip install -U langchain-azure-ai ``` ```bash uv theme={null} uv add langchain-azure-ai ```  Configure your API key and Endpoint. ```bash theme={null} export AZURE\\_AI\\_CREDENTIAL=your-api-key export AZURE\\_AI\\_ENDPOINT=your-endpoint ``` ```python theme={null} from langchain\\_azure\\_ai.chat\\_models import AzureAIChatCompletionsModel llm = AzureAIChatCompletionsModel( model\\_name=\"gpt-4o\", api\\_version=\"2024-05-01-preview\", ) ``` See a [usage example](/oss/python/integrations/chat/azure\\_ai) ### Azure ML Chat Online Endpoint See the documentation [here](/oss/python/integrations/chat/azureml\\_chat\\_endpoint) for accessing chat models hosted with [Azure Machine Learning](https://azure.microsoft.com/en-us/products/machine-learning/). ## LLMs ### Azure ML See a [usage example](/oss/python/integrations/llms/azure\\_ml). ```python theme={null} from langchain\\_community.llms.azureml\\_endpoint import AzureMLOnlineEndpoint ``` ### Azure OpenAI See a [usage example](/oss/python/integrations/llms/azure\\_openai). ```python theme={null} from langchain\\_openai import AzureOpenAI ``` ## Embedding Models Microsoft offers two main options for accessing embedding models through Azure: ### Azure OpenAI See a [usage example](/oss/python/integrations/text\\_embedding/azure\\_openai) ```python theme={null} from langchain\\_openai import AzureOpenAIEmbeddings ``` ### Azure AI  ```bash pip theme={null} pip install -U langchain-azure-ai ``` ```bash uv theme={null} uv add langchain-azure-ai ```  Configure your API key and Endpoint. ```bash theme={null} export AZURE\\_AI\\_CREDENTIAL=your-api-key export AZURE\\_AI\\_ENDPOINT=your-endpoint ``` ```python theme={null} from langchain\\_azure\\_ai.embeddings import AzureAIEmbeddingsModel embed\\_model = AzureAIEmbeddingsModel( model\\_name=\"text-embedding-ada-002\" ) ``` ## Document loaders ### Azure AI Data > [Azure AI Foundry (formerly Azure AI Studio](https://ai.azure.com/) provides the capability to upload data assets > to cloud storage and register existing data assets from the following sources: > > \\* `Microsoft OneLake` > \\* `Azure Blob Storage` > \\* `Azure Data Lake gen 2` First, you need to install several python packages.  ```bash pip theme={null} pip install azureml-fsspec, azure-ai-generative ``` ```bash uv theme={null} uv add azureml-fsspec, azure-ai-generative ```  See a [usage example](/oss/python/integrations/document\\_loaders/azure\\_ai\\_data). ```python theme={null} from langchain.document\\_loaders import AzureAIDataLoader ``` ### Azure AI Document Intelligence > [Azure AI Document Intelligence](https://aka.ms/doc-intelligence) (formerly known > as `Azure Form Recognizer`) is machine-learning > based service that extracts texts (including handwriting), tables, document structures, > and key-value-pairs > from digital or scanned PDFs, images, Office and HTML files. > > Document Intelligence supports `PDF`, `JPEG/JPG`, `PNG`, `BMP`, `TIFF`, `HEIF`, `DOCX`, `XLSX`, `PPTX` and `HTML`. First, you need to install a python package.  ```bash pip theme={null} pip install azure-ai-documentintelligence ``` ```bash uv theme={null} uv add azure-ai-documentintelligence ```  See a [usage example](/oss/python/integrations/document\\_loaders/azure\\_document\\_intelligence). ```python theme={null} from langchain.document\\_loaders import AzureAIDocumentIntelligenceLoader ``` ### Azure Blob Storage > [Azure Blob Storage](https://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction) is Microsoft's object storage solution for the cloud. Blob Storage is optimized for storing massive amounts of unstructured data. Unstructured data is data that doesn't adhere to a particular data model or definition, such as text or binary data. `Azure Blob Storage` is designed for: \\* Serving images or documents directly to a browser. \\* Storing files for distributed access. \\* Streaming video and audio. \\* Writing to log files. \\* Storing data for backup and restore, disaster recovery, and archiving. \\* Storing data for analysis by an on-premises or Azure-hosted service.  ```bash pip theme={null} pip install langchain-azure-storage ``` ```bash uv theme={null} uv add langchain-azure-storage ```  See [usage examples for the Azure Blob Storage Loader](/oss/python/integrations/document\\_loaders/azure\\_blob\\_storage). ```python theme={null} from langchain\\_azure\\_storage.document\\_loaders import AzureBlobStorageLoader ``` ### Microsoft OneDrive > [Microsoft OneDrive](https://en.wikipedia.org/wiki/OneDrive) (formerly `SkyDrive`) is a file-hosting service operated by Microsoft. First, you need to install a python package.  ```bash pip theme={null} pip install o365 ``` ```bash uv theme={null} uv add o365 ```  See a [usage example](/oss/python/integrations/document\\_loaders/microsoft\\_onedrive). ```python theme={null} from langchain\\_community.document\\_loaders import OneDriveLoader ``` ### Microsoft OneDrive File > [Microsoft OneDrive](https://en.wikipedia.org/wiki/OneDrive) (formerly `SkyDrive`) is a file-hosting service operated by Microsoft. First, you need to install a python package.  ```bash pip theme={null} pip install o365 ``` ```bash uv theme={null} uv add o365 ```  ```python theme={null} from langchain\\_community.document\\_loaders import OneDriveFileLoader ``` ### Microsoft Word > [Microsoft Word](https://www.microsoft.com/en-us/microsoft-365/word) is a word processor developed by Microsoft. See a [usage example](/oss/python/integrations/document\\_loaders/microsoft\\_word). ```python theme={null} from langchain\\_community.document\\_loaders import UnstructuredWordDocumentLoader ``` ### Microsoft Excel > [Microsoft Excel](https://en.wikipedia.org/wiki/Microsoft\\_Excel) is a spreadsheet editor developed by > Microsoft for Windows, macOS, Android, iOS and iPadOS. > It features calculation or computation capabilities, graphing tools, pivot tables, and a macro programming > language called Visual Basic for Applications (VBA). Excel forms part of the Microsoft 365 suite of software. The `UnstructuredExcelLoader` is used to load `Microsoft Excel` files. The loader works with both `.xlsx` and `.xls` files. The page content will be the raw text of the Excel file. If you use the loader in `\"elements\"` mode, an HTML representation of the Excel file will be available in the document metadata under the `text\\_as\\_html` key. See a [usage example](/oss/python/integrations/document\\_loaders/microsoft\\_excel). ```python theme={null} from langchain\\_community.document\\_loaders import UnstructuredExcelLoader ``` ### Microsoft SharePoint > [Microsoft SharePoint](https://en.wikipedia.org/wiki/SharePoint) is a website-based collaboration system > that uses workflow applications, \u201clist\u201d databases, and other web parts and security features to > empower business teams to work together developed by Microsoft. See a [usage example](/oss/python/integrations/document\\_loaders/microsoft\\_sharepoint). ```python theme={null} from langchain\\_community.document\\_loaders.sharepoint import SharePointLoader ``` ### Microsoft PowerPoint > [Microsoft PowerPoint](https://en.wikipedia.org/wiki/Microsoft\\_PowerPoint) is a presentation program by Microsoft. See a [usage example](/oss/python/integrations/document\\_loaders/microsoft\\_powerpoint). ```python theme={null} from langchain\\_community.document\\_loaders import UnstructuredPowerPointLoader ``` ### Microsoft OneNote First, let's install dependencies:  ```bash pip theme={null} pip install bs4 msal ``` ```bash uv theme={null} uv add bs4 msal ```  See a [usage example](/oss/python/integrations/document\\_loaders/microsoft\\_onenote). ```python theme={null} from langchain\\_community.document\\_loaders.onenote import OneNoteLoader ``` ### Playwright URL Loader > [Playwright](https://github.com/microsoft/playwright) is an open-source automation tool > developed by `Microsoft` that allows you to programmatically control and automate > web browsers. It is designed for end-to-end testing, scraping, and automating > tasks across various web browsers such as `Chromium`, `Firefox`, and `WebKit`. First, let's install dependencies:  ```bash pip theme={null} pip install playwright unstructured ``` ```bash uv theme={null} uv add playwright unstructured ```  See a [usage example](/oss/python/integrations/document\\_loaders/url/#playwright-url-loader). ```python theme={null} from langchain\\_community.document\\_loaders.onenote import OneNoteLoader ``` ## Memory ### Azure Cosmos DB Chat Message History > [Azure Cosmos DB](https://learn.microsoft.com/azure/cosmos-db/) provides chat message history storage for conversational AI applications, enabling you to persist and retrieve conversation history with low latency and high availability.  ```bash pip theme={null} pip install langchain-azure-ai ``` ```bash uv theme={null} uv add langchain-azure-ai ```  Configure your Azure Cosmos DB connection: ```python theme={null} from langchain\\_azure\\_ai.chat\\_message\\_histories import CosmosDBChatMessageHistory history = CosmosDBChatMessageHistory( cosmos\\_endpoint=\"https://.documents.azure.com:443/\", cosmos\\_database=\"\", cosmos\\_container=\"\", session\\_id=\"\", user\\_id=\"\", credential=\"\" # or use connection\\_string ) ``` ## Vector Stores ### Azure Cosmos DB AI agents can rely on Azure Cosmos DB as a unified [memory system](https://learn.microsoft.com/en-us/azure/cosmos-db/ai-agents#memory-can-make-or-break-agents) solution, enjoying speed, scale, and simplicity. This service successfully [enabled OpenAI's ChatGPT service](https://www.youtube.com/watch?v=6IIUtEFKJec\\&t) to scale dynamically with high reliability and low maintenance. Powered by an atom-record-sequence engine, it is the world's first globally distributed [NoSQL](https://learn.microsoft.com/en-us/azure/cosmos-db/distributed-nosql), [relational](https://learn.microsoft.com/en-us/azure/cosmos-db/distributed-relational), and [vector database](https://learn.microsoft.com/en-us/azure/cosmos-db/vector-database) service that offers a serverless mode. Below are two available Azure Cosmos DB APIs that can provide vector store functionalities. #### Azure Cosmos DB for MongoDB (vCore) > [Azure Cosmos DB for MongoDB vCore](https://learn.microsoft.com/en-us/azure/cosmos-db/mongodb/vcore/) makes it easy to create a database with full native MongoDB support. > You can apply your MongoDB experience and continue to use your favorite MongoDB drivers, SDKs, and tools by pointing your application to the API for MongoDB vCore account's connection string. > Use vector search in Azure Cosmos DB for MongoDB vCore to seamlessly integrate your AI-based applications with your data that's stored in Azure Cosmos DB. ##### Installation and Setup See [detailed configuration instructions](/oss/python/integrations/vectorstores/azure\\_cosmos\\_db\\_mongo\\_vcore). We need to install `langchain-azure-ai` and `pymongo` python packages.  ```bash pip theme={null} pip install langchain-azure-ai pymongo ``` ```bash uv theme={null} uv add langchain-azure-ai pymongo ```  ##### Deploy Azure Cosmos DB on Microsoft Azure Azure Cosmos DB for MongoDB vCore provides developers with a fully managed MongoDB-compatible database service for building modern applications with a familiar architecture. With Cosmos DB for MongoDB vCore, developers can enjoy the benefits of native Azure integrations, low total cost of ownership (TCO), and the familiar vCore architecture when migrating existing applications or building new ones. [Sign Up](https://azure.microsoft.com/en-us/free/) for free to get started today. See a [usage example](/oss/python/integrations/vectorstores/azure\\_cosmos\\_db\\_mongo\\_vcore). ```python theme={null} from langchain\\_azure\\_ai.vectorstores import AzureCosmosDBMongoVCoreVectorSearch ``` #### Azure Cosmos DB NoSQL > [Azure Cosmos DB for NoSQL](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/vector-search) now offers vector indexing and search in preview. > This feature is designed to handle high-dimensional vectors, enabling efficient and accurate vector search at any scale. You can now store vectors > directly in the documents alongside your data. This means that each document in your database can contain not only traditional schema-free data, > but also high-dimensional vectors as other properties of the documents. This colocation of data and vectors allows for efficient indexing and searching, > as the vectors are stored in the same logical unit as the data they represent. This simplifies data management, AI application architectures, and the > efficiency of vector-based operations. ##### Installation and Setup See [detail configuration instructions](/oss/python/integrations/vectorstores/azure\\_cosmos\\_db\\_no\\_sql). We need to install `langchain-azure-ai` and `azure-cosmos` python packages.  ```bash pip theme={null} pip install langchain-azure-ai azure-cosmos ``` ```bash uv theme={null} uv add langchain-azure-ai azure-cosmos ```  ##### Deploy Azure Cosmos DB on Microsoft Azure Azure Cosmos DB offers a solution for modern apps and intelligent workloads by being very responsive with dynamic and elastic autoscale. It is available in every Azure region and can automatically replicate data closer to users. It has SLA guaranteed low-latency and high availability. [Sign Up](https://learn.microsoft.com/en-us/azure/cosmos-db/nosql/quickstart-python?pivots=devcontainer-codespace) for free to get started today. See a [usage example](/oss/python/integrations/vectorstores/azure\\_cosmos\\_db\\_no\\_sql). ```python theme={null} from langchain\\_azure\\_ai.vectorstores import AzureCosmosDBNoSqlVectorSearch ``` ### Azure Database for PostgreSQL > [Azure Database for PostgreSQL - Flexible Server](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/service-overview) is a relational database service based on the open-source Postgres database engine. It's a fully managed database-as-a-service that can handle mission-critical workloads with predictable performance, security, high availability, and dynamic scalability. See [set up instructions](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/quickstart-create-server-portal) for Azure Database for PostgreSQL. Simply use the [connection string](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/connect-python?tabs=cmd%2Cpassword#add-authentication-code) from your Azure Portal. Since Azure Database for PostgreSQL is open-source Postgres, you can use the [LangChain's Postgres support](/oss/python/integrations/vectorstores/pgvector/) to connect to Azure Database for PostgreSQL. ### Azure SQL Database > [Azure SQL Database](https://learn.microsoft.com/azure/azure-sql/database/sql-database-paas-overview?view=azuresql) is a robust service that combines scalability, security, and high availability, providing all the benefits of a modern database solution. It also provides a dedicated Vector data type & built-in functions that simplifies the storage and querying of vector embeddings directly within a relational database. This eliminates the need for separate vector databases and related integrations, increasing the security of your solutions while reducing the overall complexity. By leveraging your current SQL Server databases for vector search, you can enhance data capabilities while minimizing expenses and avoiding the challenges of transitioning to new systems. ##### Installation and Setup See [detail configuration instructions](/oss/python/integrations/vectorstores/sqlserver). We need to install the `langchain-sqlserver` python package. ```bash theme={null} !pip install langchain-sqlserver==0.1.1 ``` ##### Deploy Azure SQL DB on Microsoft Azure [Sign Up](https://learn.microsoft.com/azure/azure-sql/database/free-offer?view=azuresql) for free to get started today. See a [usage example](/oss/python/integrations/vectorstores/sqlserver). ```python theme={null} from langchain\\_sqlserver import SQLServer\\_VectorStore ``` ### Azure AI Search [Azure AI Search](https://learn.microsoft.com/azure/search/search-what-is-azure-search) is a cloud search service that gives developers infrastructure, APIs, and tools for information retrieval of vector, keyword, and hybrid queries at scale. See [here](/oss/python/integrations/vectorstores/azuresearch) for usage examples. ```python theme={null} from langchain\\_community.vectorstores.azuresearch import AzureSearch ``` ## Retrievers ### Azure AI Search > [Azure AI Search](https://learn.microsoft.com/en-us/azure/search/search-what-is-azure-search) (formerly known as `Azure Search` or `Azure Cognitive Search` ) is a cloud search service that gives developers infrastructure, APIs, and tools for building a rich search experience over private, heterogeneous content in web, mobile, and enterprise applications. > Search is foundational to any app that surfaces text to users, where common scenarios include catalog or document search, online retail apps, or data exploration over proprietary content. When you create a search service, you'll work with the following capabilities: > > \\* A search engine for full text search over a search index containing user-owned content > \\* Rich indexing, with lexical analysis and optional AI enrichment for content extraction and transformation > \\* Rich query syntax for text search, fuzzy search, autocomplete, geo-search and more > \\* Programmability through REST APIs and client libraries in Azure SDKs > \\* Azure integration at the data layer, machine learning layer, and AI (AI Services) See [set up instructions](https://learn.microsoft.com/en-us/azure/search/search-create-service-portal). See a [usage example](/oss/python/integrations/retrievers/azure\\_ai\\_search). ```python theme={null} from langchain\\_community.retrievers import AzureAISearchRetriever ``` ## Vector Store ### Azure Database for PostgreSQL > [Azure Database for PostgreSQL - Flexible Server](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/service-overview) is a relational database service based on the open-source Postgres database engine. It's a fully managed database-as-a-service that can handle mission-critical workloads with predictable performance, security, high availability, and dynamic scalability. See [set up instructions](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/quickstart-create-server-portal) for Azure Database for PostgreSQL. You need to [enable pgvector extension](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/how-to-use-pgvector) in your database to use Postgres as a vector store. Once you have the extension enabled, you can use the [PGVector in LangChain](/oss/python/integrations/vectorstores/pgvector/) to connect to Azure Database for PostgreSQL. See a [usage example](/oss/python/integrations/vectorstores/pgvector/). Simply use the [connection string](https://learn.microsoft.com/en-us/azure/postgresql/flexible-server/connect-python?tabs=cmd%2Cpassword#add-authentication-code) from your Azure Portal. ## Tools ### Azure Container Apps dynamic sessions We need to get the `POOL\\_MANAGEMENT\\_ENDPOINT` environment variable from the Azure Container Apps service. See the instructions [here](/oss/python/integrations/tools/azure\\_dynamic\\_sessions/#setup). We need to install a python package.  ```bash pip theme={null} pip install langchain-azure-dynamic-sessions ``` ```bash uv theme={null} uv add langchain-azure-dynamic-sessions ```  See a [usage example](/oss/python/integrations/tools/azure\\_dynamic\\_sessions). ```python theme={null} from langchain\\_azure\\_dynamic\\_sessions import SessionsPythonREPLTool ``` ### Bing Search Follow the documentation [here](/oss/python/integrations/tools/bing\\_search) to get a detail explanations and instructions of this tool. The environment variable `BING\\_SUBSCRIPTION\\_KEY` and `BING\\_SEARCH\\_URL` are required from Bing Search resource. ```python theme={null} from langchain\\_community.tools.bing\\_search import BingSearchResults from langchain\\_community.utilities import BingSearchAPIWrapper api\\_wrapper = BingSearchAPIWrapper() tool = BingSearchResults(api\\_wrapper=api\\_wrapper) ``` ## Toolkits ### Azure AI Services We need to install several python packages.  ```bash pip theme={null} pip install azure-ai-formrecognizer azure-cognitiveservices-speech azure-ai-vision-imageanalysis ``` ```bash uv theme={null} uv add azure-ai-formrecognizer azure-cognitiveservices-speech azure-ai-vision-imageanalysis ```  See a [usage example](/oss/python/integrations/tools/azure\\_ai\\_services). ```python theme={null} from langchain\\_community.agent\\_toolkits import azure\\_ai\\_services ``` #### Azure AI Services individual tools The `azure\\_ai\\_services` toolkit includes the following tools: \\* Image Analysis: [AzureAiServicesImageAnalysisTool](https://python.langchain.com/api\\_reference/community/tools/langchain\\_community.tools.azure\\_ai\\_services.image\\_analysis.AzureAiServicesImageAnalysisTool.html) \\* Document Intelligence: [AzureAiServicesDocumentIntelligenceTool](https://python.langchain.com/api\\_reference/community/tools/langchain\\_community.tools.azure\\_ai\\_services.document\\_intelligence.AzureAiServicesDocumentIntelligenceTool.html) \\* Speech to Text: [AzureAiServicesSpeechToTextTool](https://python.langchain.com/api\\_reference/community/tools/langchain\\_community.tools.azure\\_ai\\_services.speech\\_to\\_text.AzureAiServicesSpeechToTextTool.html) \\* Text to Speech: [AzureAiServicesTextToSpeechTool](https://python.langchain.com/api\\_reference/community/tools/langchain\\_community.tools.azure\\_ai\\_services.text\\_to\\_speech.AzureAiServicesTextToSpeechTool.html) \\* Text Analytics for Health: [AzureAiServicesTextAnalyticsForHealthTool](https://python.langchain.com/api\\_reference/community/tools/langchain\\_community.tools.azure\\_ai\\_services.text\\_analytics\\_for\\_health.AzureAiServicesTextAnalyticsForHealthTool.html) ### Azure Cognitive Services We need to install several python packages.  ```bash pip theme={null} pip install azure-ai-formrecognizer azure-cognitiveservices-speech azure-ai-vision-imageanalysis ``` ```bash uv theme={null} uv add azure-ai-formrecognizer azure-cognitiveservices-speech azure-ai-vision-imageanalysis ```  See a [usage example](/oss/python/integrations/tools/azure\\_cognitive\\_services). ```python theme={null} from langchain\\_community.agent\\_toolkits import AzureCognitiveServicesToolkit ``` #### Azure AI Services individual tools The `azure\\_ai\\_services` toolkit includes the tools that queries the `Azure Cognitive Services`: \\* `AzureCogsFormRecognizerTool`: Form Recognizer API \\* `AzureCogsImageAnalysisTool`: Image Analysis API \\* `AzureCogsSpeech2TextTool`: Speech2Text API \\* `AzureCogsText2SpeechTool`: Text2Speech API \\* `AzureCogsTextAnalyticsHealthTool`: Text Analytics for Health API ```python theme={null} from langchain\\_community.tools.azure\\_cognitive\\_services import ( AzureCogsFormRecognizerTool, AzureCogsImageAnalysisTool, AzureCogsSpeech2TextTool, AzureCogsText2SpeechTool, AzureCogsTextAnalyticsHealthTool, ) ``` ### Microsoft Office 365 email and calendar We need to install `O365` python package.  ```bash pip theme={null} pip install O365 ``` ```bash uv theme={null} uv add O365 ```  See a [usage example](/oss/python/integrations/tools/office365). ```python theme={null} from langchain\\_community.agent\\_toolkits import O365Toolkit ``` #### Office 365 individual tools You can use individual tools from the Office 365 Toolkit: \\* `O365CreateDraftMessage`: creating a draft email in Office 365 \\* `O365SearchEmails`: searching email messages in Office 365 \\* `O365SearchEvents`: searching calendar events in Office 365 \\* `O365SendEvent`: sending calendar events in Office 365 \\* `O365SendMessage`: sending an email in Office 365 ```python theme={null} from langchain\\_community.tools.office365 import O365CreateDraftMessage from langchain\\_community.tools.office365 import O365SearchEmails from langchain\\_community.tools.office365 import O365SearchEvents from langchain\\_community.tools.office365 import O365SendEvent from langchain\\_community.tools.office365 import O365SendMessage ``` ### Microsoft Azure PowerBI We need to install `azure-identity` python package.  ```bash pip theme={null} pip install azure-identity ``` ```bash uv theme={null} uv add azure-identity ```  See a [usage example](/oss/python/integrations/tools/powerbi). ```python theme={null} from langchain\\_community.agent\\_toolkits import PowerBIToolkit from langchain\\_community.utilities.powerbi import PowerBIDataset ``` #### PowerBI individual tools You can use individual tools from the Azure PowerBI Toolkit: \\* `InfoPowerBITool`: getting metadata about a PowerBI Dataset \\* `ListPowerBITool`: getting tables names \\* `QueryPowerBITool`: querying a PowerBI Dataset ```python theme={null} from langchain\\_community.tools.powerbi.tool import InfoPowerBITool from langchain\\_community.tools.powerbi.tool import ListPowerBITool from langchain\\_community.tools.powerbi.tool import QueryPowerBITool ``` ### PlayWright Browser Toolkit > [Playwright](https://github.com/microsoft/playwright) is an open-source automation tool > developed by `Microsoft` that allows you to programmatically control and automate > web browsers. It is designed for end-to-end testing, scraping, and automating > tasks across various web browsers such as `Chromium`, `Firefox`, and `WebKit`. We need to install several python packages.  ```bash pip theme={null} pip install playwright lxml ``` ```bash uv theme={null} uv add playwright lxml ```  See a [usage example](/oss/python/integrations/tools/playwright). ```python theme={null} from langchain\\_community.agent\\_toolkits import PlayWrightBrowserToolkit ``` #### PlayWright Browser individual tools You can use individual tools from the PlayWright Browser Toolkit. ```python theme={null} from langchain\\_community.tools.playwright import ClickTool from langchain\\_community.tools.playwright import CurrentWebPageTool from langchain\\_community.tools.playwright import ExtractHyperlinksTool from langchain\\_community.tools.playwright import ExtractTextTool from langchain\\_community.tools.playwright import GetElementsTool from langchain\\_community.tools.playwright import NavigateTool from langchain\\_community.tools.playwright import NavigateBackTool ``` ## Graphs ### Azure Cosmos DB for Apache Gremlin We need to install a python package.  ```bash pip theme={null} pip install gremlinpython ``` ```bash uv theme={null} uv add gremlinpython ```  See a [usage example](/oss/python/integrations/graphs/azure\\_cosmosdb\\_gremlin). ```python theme={null} from langchain\\_community.graphs import GremlinGraph from langchain\\_community.graphs.graph\\_document import GraphDocument, Node, Relationship ``` ## Utilities ### Bing Search API > [Microsoft Bing](https://www.bing.com/), commonly referred to as `Bing` or `Bing Search`, > is a web search engine owned and operated by `Microsoft`. See a [usage example](/oss/python/integrations/tools/bing\\_search). ```python theme={null} from langchain\\_community.utilities import BingSearchAPIWrapper ``` ## More ### Microsoft Presidio > [Presidio](https://microsoft.github.io/presidio/) (Origin from Latin praesidium \u2018protection, garrison\u2019) > helps to ensure sensitive data is properly managed and governed. It provides fast identification and > anonymization modules for private entities in text and images such as credit card numbers, names, > locations, social security numbers, bitcoin wallets, US phone numbers, financial data and more. First, you need to install several python packages and download a `SpaCy` model.  ```bash pip theme={null} pip install langchain-experimental openai presidio-analyzer presidio-anonymizer spacy Faker python -m spacy download en\\_core\\_web\\_lg ``` ```bash uv theme={null} uv add langchain-experimental openai presidio-analyzer presidio-anonymizer spacy Faker python -m spacy download en\\_core\\_web\\_lg ```  See [usage examples](https://python.langchain.com/v0.1/docs/guides/productionization/safety/presidio\\_data\\_anonymization). ```python theme={null} from langchain\\_experimental.data\\_anonymizer import PresidioAnonymizer, PresidioReversibleAnonymizer ``` \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/microsoft.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/integrations/providers/openai.md",
      "title": null,
      "raw_content": "# OpenAI This page covers all LangChain integrations with [OpenAI](https://en.wikipedia.org/wiki/OpenAI) ## Model interfaces   OpenAI chat models.   Wrapper for OpenAI chat models hosted on Azure.   (Legacy) OpenAI text completion models.   Wrapper for (legacy) OpenAI text completion models hosted on Azure.   OpenAI embedding models.   Wrapper for OpenAI embedding models hosted on Azure.   ## Tools and toolkits   Text-to-image generation using OpenAI's Dall-E models.   ## Retrievers   Retrieve real-time information; e.g., sports scores, stock prices, the latest news, etc.   ## Document loaders   Load `conversations.json` from your ChatGPT data export folder.   ## Middleware Middleware specifically designed for OpenAI models. Learn more about [middleware](/oss/python/langchain/middleware/overview). | Middleware | Description | | ----------------------------------------- | --------------------------------------------------------- | | [Content moderation](#content-moderation) | Moderate agent traffic using OpenAI's moderation endpoint | ### Content moderation Moderate agent traffic (user input, model output, and tool results) using OpenAI's moderation endpoint to detect and handle unsafe content. Content moderation is useful for the following: \\* Applications requiring content safety and compliance \\* Filtering harmful, hateful, or inappropriate content \\* Customer-facing agents that need safety guardrails \\* Meeting platform moderation requirements  Learn more about [OpenAI's moderation models](https://platform.openai.com/docs/guides/moderation) and categories.  \\*\\*API reference:\\*\\* [`OpenAIModerationMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.OpenAIModerationMiddleware) ```python theme={null} from langchain\\_openai import ChatOpenAI from langchain\\_openai.middleware import OpenAIModerationMiddleware from langchain.agents import create\\_agent agent = create\\_agent( model=ChatOpenAI(model=\"gpt-4o\"), tools=[search\\_tool, database\\_tool], middleware=[ OpenAIModerationMiddleware( model=\"omni-moderation-latest\", check\\_input=True, check\\_output=True, exit\\_behavior=\"end\", ), ], ) ```   OpenAI moderation model to use. Options: `'omni-moderation-latest'`, `'omni-moderation-2024-09-26'`, `'text-moderation-latest'`, `'text-moderation-stable'`   Whether to check user input messages before the model is called   Whether to check model output messages after the model is called   Whether to check tool result messages before the model is called   How to handle violations when content is flagged. Options: \\* `'end'` - End agent execution immediately with a violation message \\* `'error'` - Raise `OpenAIModerationError` exception \\* `'replace'` - Replace the flagged content with the violation message and continue   Custom template for violation messages. Supports template variables: \\* `{categories}` - Comma-separated list of flagged categories \\* `{category\\_scores}` - JSON string of category scores \\* `{original\\_content}` - The original flagged content Default: `\"I'm sorry, but I can't comply with that request. It was flagged for {categories}.\"`   Optional pre-configured OpenAI client to reuse. If not provided, a new client will be created.   Optional pre-configured AsyncOpenAI client to reuse. If not provided, a new async client will be created.    The middleware integrates OpenAI's moderation endpoint to check content at different stages: \\*\\*Moderation stages:\\*\\* \\* `check\\_input` - User messages before model call \\* `check\\_output` - AI messages after model call \\* `check\\_tool\\_results` - Tool outputs before model call \\*\\*Exit behaviors:\\*\\* \\* `'end'` (default) - Stop execution with violation message \\* `'error'` - Raise exception for application handling \\* `'replace'` - Replace flagged content and continue ```python theme={null} from langchain\\_openai import ChatOpenAI from langchain\\_openai.middleware import OpenAIModerationMiddleware from langchain.agents import create\\_agent # Basic moderation agent = create\\_agent( model=ChatOpenAI(model=\"gpt-4o\"), tools=[search\\_tool, customer\\_data\\_tool], middleware=[ OpenAIModerationMiddleware( model=\"omni-moderation-latest\", check\\_input=True, check\\_output=True, ), ], ) # Strict moderation with custom message agent\\_strict = create\\_agent( model=ChatOpenAI(model=\"gpt-4o\"), tools=[search\\_tool, customer\\_data\\_tool], middleware=[ OpenAIModerationMiddleware( model=\"omni-moderation-latest\", check\\_input=True, check\\_output=True, check\\_tool\\_results=True, exit\\_behavior=\"error\", violation\\_message=( \"Content policy violation detected: {categories}. \" \"Please rephrase your request.\" ), ), ], ) # Moderation with replacement behavior agent\\_replace = create\\_agent( model=ChatOpenAI(model=\"gpt-4o\"), tools=[search\\_tool], middleware=[ OpenAIModerationMiddleware( check\\_input=True, exit\\_behavior=\"replace\", violation\\_message=\"[Content removed due to safety policies]\", ), ], ) ```  ## Other   Adapt LangChain models to OpenAI APIs.   Detect text that could be hateful, violent, etc.   \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/openai.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/integrations/providers/overview.md",
      "title": null,
      "raw_content": "# Integration packages {/\\* Do not manually edit \\*/} LangChain Python offers an extensive ecosystem with 1000+ integrations across chat & embedding models, tools & toolkits, document loaders, vector stores, and more.   To see a full list of integrations by component type, refer to the categories in the sidebar. ## Popular providers | Provider | Package | Downloads | Latest version | JS/TS support | | :------------------------------------------------------------------ | :-------------------------------------------------------------------------------------------------------------------- | :-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------- | | [OpenAI](/oss/python/integrations/providers/openai/) | [`langchain-openai`](https://reference.langchain.com/python/integrations/langchain\\_openai/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/openai) | | [Google (Vertex AI)](/oss/python/integrations/providers/google) | [`langchain-google-vertexai`](https://reference.langchain.com/python/integrations/langchain\\_google\\_vertexai/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/google-vertexai) | | [Anthropic (Claude)](/oss/python/integrations/providers/anthropic/) | [`langchain-anthropic`](https://reference.langchain.com/python/integrations/langchain\\_anthropic/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/anthropic) | | [AWS](/oss/python/integrations/providers/aws/) | [`langchain-aws`](https://reference.langchain.com/python/integrations/langchain\\_aws/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/aws) | | [Google (GenAI)](/oss/python/integrations/providers/google) | [`langchain-google-genai`](https://reference.langchain.com/python/integrations/langchain\\_google\\_genai/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/google-genai) | | [Groq](/oss/python/integrations/providers/groq/) | [`langchain-groq`](https://reference.langchain.com/python/integrations/langchain\\_groq/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/groq) | | [Ollama](/oss/python/integrations/providers/ollama/) | [`langchain-ollama`](https://reference.langchain.com/python/integrations/langchain\\_ollama/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/ollama) | | [Chroma](/oss/python/integrations/providers/chroma/) | [`langchain-chroma`](https://reference.langchain.com/python/integrations/langchain\\_chroma/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/community) | | [Huggingface](/oss/python/integrations/providers/huggingface/) | [`langchain-huggingface`](https://reference.langchain.com/python/integrations/langchain\\_huggingface/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/community) | | [Pinecone](/oss/python/integrations/providers/pinecone/) | [`langchain-pinecone`](https://reference.langchain.com/python/integrations/langchain\\_pinecone/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/pinecone) | | [Cohere](/oss/python/integrations/providers/cohere/) | [`langchain-cohere`](https://reference.langchain.com/python/integrations/langchain\\_cohere/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/cohere) | | [Postgres](/oss/python/integrations/providers/pgvector) | [`langchain-postgres`](https://reference.langchain.com/python/integrations/langchain\\_postgres/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/community) | | [Fireworks](/oss/python/integrations/providers/fireworks/) | [`langchain-fireworks`](https://reference.langchain.com/python/integrations/langchain\\_fireworks/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/community) | | [MistralAI](/oss/python/integrations/providers/mistralai/) | [`langchain-mistralai`](https://reference.langchain.com/python/integrations/langchain\\_mistralai/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/mistralai) | | [Databricks](/oss/python/integrations/providers/databricks/) | [`databricks-langchain`](https://pypi.org/project/databricks-langchain/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/community) | | [Perplexity](/oss/python/integrations/providers/perplexity/) | [`langchain-perplexity`](https://reference.langchain.com/python/integrations/langchain\\_perplexity/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/community) | | [IBM](/oss/python/integrations/providers/ibm/) | [`langchain-ibm`](https://reference.langchain.com/python/integrations/langchain\\_ibm/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/ibm) | | [Nvidia AI Endpoints](/oss/python/integrations/providers/nvidia) | [`langchain-nvidia-ai-endpoints`](https://reference.langchain.com/python/integrations/langchain\\_nvidia\\_ai\\_endpoints/) |  |  | \u274c | | [MongoDB](/oss/python/integrations/providers/mongodb\\_atlas) | [`langchain-mongodb`](https://reference.langchain.com/python/integrations/langchain\\_mongodb/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/mongodb) | | [Deepseek](/oss/python/integrations/providers/deepseek/) | [`langchain-deepseek`](https://reference.langchain.com/python/integrations/langchain\\_deepseek/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/deepseek) | | [Qdrant](/oss/python/integrations/providers/qdrant/) | [`langchain-qdrant`](https://reference.langchain.com/python/integrations/langchain\\_qdrant/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/qdrant) | | [Milvus](/oss/python/integrations/providers/milvus/) | [`langchain-milvus`](https://reference.langchain.com/python/integrations/langchain\\_milvus/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/community) | | [Tavily](/oss/python/integrations/providers/tavily/) | [`langchain-tavily`](https://pypi.org/project/langchain-tavily/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/tavily) | | [Elasticsearch](/oss/python/integrations/providers/elasticsearch/) | [`langchain-elasticsearch`](https://reference.langchain.com/python/integrations/langchain\\_elasticsearch/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/community) | | [Together](/oss/python/integrations/providers/together/) | [`langchain-together`](https://reference.langchain.com/python/integrations/langchain\\_together/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/community) | | [Redis](/oss/python/integrations/providers/redis/) | [`langchain-redis`](https://reference.langchain.com/python/integrations/langchain\\_redis/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/redis) | | [LiteLLM](/oss/python/integrations/providers/litellm/) | [`langchain-litellm`](https://pypi.org/project/langchain-litellm/) |  |  | N/A | | [xAI (Grok)](/oss/python/integrations/providers/xai/) | [`langchain-xai`](https://reference.langchain.com/python/integrations/langchain\\_xai/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/xai) | | [DataStax Astra DB](/oss/python/integrations/providers/astradb/) | [`langchain-astradb`](https://reference.langchain.com/python/integrations/langchain\\_astradb/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/community) | | [Azure AI](/oss/python/integrations/providers/azure\\_ai) | [`langchain-azure-ai`](https://reference.langchain.com/python/integrations/langchain\\_azure\\_ai/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/openai) | | [MCP Toolbox (Google)](/oss/python/integrations/providers/toolbox/) | [`toolbox-langchain`](https://pypi.org/project/toolbox-langchain/) |  |  | \u274c | | [Google (Community)](/oss/python/integrations/providers/google) | [`langchain-google-community`](https://reference.langchain.com/python/integrations/langchain\\_google\\_community/) |  |  | \u274c | | [Unstructured](/oss/python/integrations/providers/unstructured/) | [`langchain-unstructured`](https://reference.langchain.com/python/integrations/langchain\\_unstructured/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/community) | | [Neo4J](/oss/python/integrations/providers/neo4j/) | [`langchain-neo4j`](https://reference.langchain.com/python/integrations/langchain\\_neo4j/) |  |  | [\u2705](https://www.npmjs.com/package/@langchain/community) | | [Graph RAG](/oss/python/integrations/providers/graph\\_rag) | [`langchain-graph-retriever`](https://pypi.org/project/langchain-graph-retriever/) |  |  | \u274c | ## All providers [See all providers](/oss/python/integrations/providers/all\\_providers) or search for a provider using the search field. Community integrations can be found in [`langchain-community`](https://github.com/langchain-ai/langchain-community).  If you'd like to contribute an integration, see the [contributing guide](/oss/python/contributing).  \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/providers/overview.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/integrations/retrievers/index.md",
      "title": null,
      "raw_content": "# Retrievers A [retriever](/oss/python/langchain/retrieval#building-blocks) is an interface that returns documents given an unstructured query. It is more general than a vector store. A retriever does not need to be able to store documents, only to return (or retrieve) them. Retrievers can be created from vector stores, but are also broad enough to include [Wikipedia search](/oss/python/integrations/retrievers/wikipedia/) and [Amazon Kendra](/oss/python/integrations/retrievers/amazon\\_kendra\\_retriever/). Retrievers accept a string query as input and return a list of [Documents](https://python.langchain.com/api\\_reference/core/documents/langchain\\_core.documents.base.Document.html) as output. Note that all [vector stores](/oss/python/integrations/vectorstores) can be cast to retrievers. Refer to the vector store [integration docs](/oss/python/integrations/vectorstores/) for available vector stores. This page lists custom retrievers, implemented via subclassing BaseRetriever. ## Bring-your-own documents The below retrievers allow you to index and search a custom corpus of documents. | Retriever | Self-host | Cloud offering | Package | | ---------------------------------------------------------------------------------------- | --------- | -------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | | [`AmazonKnowledgeBasesRetriever`](/oss/python/integrations/retrievers/bedrock) | \u274c | \u2705 | [`langchain-aws`](https://python.langchain.com/api\\_reference/aws/retrievers/langchain\\_aws.retrievers.bedrock.AmazonKnowledgeBasesRetriever.html) | | [`AzureAISearchRetriever`](/oss/python/integrations/retrievers/azure\\_ai\\_search) | \u274c | \u2705 | [`langchain-community`](https://python.langchain.com/api\\_reference/community/retrievers/langchain\\_community.retrievers.azure\\_ai\\_search.AzureAISearchRetriever.html) | | [`ElasticsearchRetriever`](/oss/python/integrations/retrievers/elasticsearch\\_retriever) | \u2705 | \u2705 | [`langchain-elasticsearch`](https://python.langchain.com/api\\_reference/elasticsearch/retrievers/langchain\\_elasticsearch.retrievers.ElasticsearchRetriever.html) | | [`VertexAISearchRetriever`](/oss/python/integrations/retrievers/google\\_vertex\\_ai\\_search) | \u274c | \u2705 | [`langchain-google-community`](https://python.langchain.com/api\\_reference/google\\_community/vertex\\_ai\\_search/langchain\\_google\\_community.vertex\\_ai\\_search.VertexAISearchRetriever.html) | ## External index The below retrievers will search over an external index (e.g., constructed from Internet data or similar). | Retriever | Source | Package | | ------------------------------------------------------------------------ | ----------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------- | | [`ArxivRetriever`](/oss/python/integrations/retrievers/arxiv) | Scholarly articles on [arxiv.org](https://arxiv.org/) | [`langchain-community`](https://python.langchain.com/api\\_reference/community/retrievers/langchain\\_community.retrievers.arxiv.ArxivRetriever.html) | | [`TavilySearchAPIRetriever`](/oss/python/integrations/retrievers/tavily) | Internet search | [`langchain-community`](https://python.langchain.com/api\\_reference/community/retrievers/langchain\\_community.retrievers.tavily\\_search\\_api.TavilySearchAPIRetriever.html) | | [`WikipediaRetriever`](/oss/python/integrations/retrievers/wikipedia) | [Wikipedia](https://www.wikipedia.org/) articles | [`langchain-community`](https://python.langchain.com/api\\_reference/community/retrievers/langchain\\_community.retrievers.wikipedia.WikipediaRetriever.html) | ## All retrievers > \\*\\*Note:\\*\\* The descriptions in the table below are truncated for readability.   \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/retrievers/index.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/integrations/splitters/index.md",
      "title": null,
      "raw_content": "# Text splitters \\*\\*Text splitters\\*\\* break large docs into smaller chunks that will be retrievable individually and fit within model context window limit. There are several strategies for splitting documents, each with its own advantages.  For most use cases, start with the [RecursiveCharacterTextSplitter](/oss/python/integrations/splitters/recursive\\_text\\_splitter). It provides a solid balance between keeping context intact and managing chunk size. This default strategy works well out of the box, and you should only consider adjusting it if you need to fine-tune performance for your specific application.  ## Text structure-based Text is naturally organized into hierarchical units such as paragraphs, sentences, and words. We can leverage this inherent structure to inform our splitting strategy, creating split that maintain natural language flow, maintain semantic coherence within split, and adapts to varying levels of text granularity. LangChain's `RecursiveCharacterTextSplitter` implements this concept: \\* The [RecursiveCharacterTextSplitter](/oss/python/integrations/splitters/recursive\\_text\\_splitter) attempts to keep larger units (e.g., paragraphs) intact. \\* If a unit exceeds the chunk size, it moves to the next level (e.g., sentences). \\* This process continues down to the word level if necessary. Example usage: ```python theme={null} from langchain\\_text\\_splitters import RecursiveCharacterTextSplitter text\\_splitter = RecursiveCharacterTextSplitter(chunk\\_size=100, chunk\\_overlap=0) texts = text\\_splitter.split\\_text(document) ``` \\*\\*Available text splitters\\*\\*: \\* [Recursively split text](/oss/python/integrations/splitters/recursive\\_text\\_splitter) ## Length-based An intuitive strategy is to split documents based on their length. This simple yet effective approach ensures that each chunk doesn't exceed a specified size limit. Key benefits of length-based splitting: \\* Straightforward implementation \\* Consistent chunk sizes \\* Easily adaptable to different model requirements Types of length-based splitting: \\* Token-based: Splits text based on the number of tokens, which is useful when working with language models. \\* Character-based: Splits text based on the number of characters, which can be more consistent across different types of text. Example implementation using LangChain's CharacterTextSplitter with token-based splitting: ```python theme={null} from langchain\\_text\\_splitters import CharacterTextSplitter text\\_splitter = CharacterTextSplitter.from\\_tiktoken\\_encoder( encoding\\_name=\"cl100k\\_base\", chunk\\_size=100, chunk\\_overlap=0 ) texts = text\\_splitter.split\\_text(document) ``` \\*\\*Available text splitters\\*\\*: \\* [Split by tokens](/oss/python/integrations/splitters/split\\_by\\_token) \\* [Split by characters](/oss/python/integrations/splitters/character\\_text\\_splitter) ## Document structure-based Some documents have an inherent structure, such as HTML, Markdown, or JSON files. In these cases, it's beneficial to split the document based on its structure, as it often naturally groups semantically related text. Key benefits of structure-based splitting: \\* Preserves the logical organization of the document \\* Maintains context within each chunk \\* Can be more effective for downstream tasks like retrieval or summarization Examples of structure-based splitting: \\* Markdown: Split based on headers (e.g., #, ##, ###) \\* HTML: Split using tags \\* JSON: Split by object or array elements \\* Code: Split by functions, classes, or logical blocks \\*\\*Available text splitters\\*\\*: \\* [Split Markdown](/oss/python/integrations/splitters/markdown\\_header\\_metadata\\_splitter) \\* [Split JSON](/oss/python/integrations/splitters/recursive\\_json\\_splitter) \\* [Split code](/oss/python/integrations/splitters/code\\_splitter) \\* [Split HTML](/oss/python/integrations/splitters/split\\_html) \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/integrations/splitters/index.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/integrations/stores/index.md",
      "title": null,
      "raw_content": "# Key-value stores ## Overview LangChain provides a key-value store interface for storing and retrieving data by key. The key-value store interface in LangChain is primarily used for caching [embeddings](/oss/python/integrations/text\\_embedding). ## Interface All [`BaseStores`](https://python.langchain.com/api\\_reference/core/stores/langchain\\_core.stores.BaseStore.html) support the following interface: \\* `mget(key: Sequence[str]) -> List[Optional[bytes]]`: get the contents of multiple keys, returning `None` if the key does not exist \\* `mset(key\\_value\\_pairs: Sequence[Tuple[str, bytes]]) -> None`: set the contents of multiple keys \\* `mdelete(key: Sequence[str]) -> None`: delete multiple keys \\* `yield\\_keys(prefix: Optional[str] = None) -> Iterator[str]`: yield all keys in the store, optionally filtering by a prefix  Base stores are designed to work \\*\\*multiple\\*\\* key-value pairs at once for efficiency. This saves on network round-trips and may allow for more efficient batch operations in the underlying store.  ## Built-in stores for local development   ## Custom stores You can also implement your own custom store by extending the [`BaseStore`](https://reference.langchain.com/python/langgraph/store/#langgraph.store.base.BaseStore) class. See the [store interface documentation](https://python.langchain.com/api\\_reference/core/stores/langchain\\_core.stores.BaseStore.html) for more details. ## All key-value stores   \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/stores/index.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/integrations/text_embedding/index.md",
      "title": null,
      "raw_content": "# Embedding models ## Overview  This overview covers \\*\\*text-based embedding models\\*\\*. LangChain does not currently support multimodal embeddings. See [top embedding models](#top-integrations).  Embedding models transform raw text\u2014such as a sentence, paragraph, or tweet\u2014into a fixed-length vector of numbers that captures its \\*\\*semantic meaning\\*\\*. These vectors allow machines to compare and search text based on meaning rather than exact words. In practice, this means that texts with similar ideas are placed close together in the vector space. For example, instead of matching only the phrase \\*\"machine learning\"\\*, embeddings can surface documents that discuss related concepts even when different wording is used. ### How it works 1. \\*\\*Vectorization\\*\\* \u2014 The model encodes each input string as a high-dimensional vector. 2. \\*\\*Similarity scoring\\*\\* \u2014 Vectors are compared using mathematical metrics to measure how closely related the underlying texts are. ### Similarity metrics Several metrics are commonly used to compare embeddings: \\* \\*\\*Cosine similarity\\*\\* \u2014 measures the angle between two vectors. \\* \\*\\*Euclidean distance\\*\\* \u2014 measures the straight-line distance between points. \\* \\*\\*Dot product\\*\\* \u2014 measures how much one vector projects onto another. Here's an example of computing cosine similarity between two vectors: ```python theme={null} import numpy as np def cosine\\_similarity(vec1, vec2): dot = np.dot(vec1, vec2) return dot / (np.linalg.norm(vec1) \\* np.linalg.norm(vec2)) similarity = cosine\\_similarity(query\\_embedding, document\\_embedding) print(\"Cosine Similarity:\", similarity) ``` ## Interface LangChain provides a standard interface for text embedding models (e.g., OpenAI, Cohere, Hugging Face) via the [Embeddings](https://reference.langchain.com/python/langchain\\_core/embeddings/#langchain\\_core.embeddings.embeddings.Embeddings) interface. Two main methods are available: \\* `embed\\_documents(texts: List[str]) \u2192 List[List[float]]`: Embeds a list of documents. \\* `embed\\_query(text: str) \u2192 List[float]`: Embeds a single query.  The interface allows queries and documents to be embedded with different strategies, though most providers handle them the same way in practice.  ## Top integrations | Model | Package | | ---------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------ | | [`OpenAIEmbeddings`](/oss/python/integrations/text\\_embedding/openai) | [`langchain-openai`](https://python.langchain.com/api\\_reference/openai/chat\\_models/langchain\\_openai.chat\\_models.base.ChatOpenAI.html) | | [`AzureOpenAIEmbeddings`](/oss/python/integrations/text\\_embedding/azure\\_openai) | [`langchain-openai`](https://python.langchain.com/api\\_reference/openai/embeddings/langchain\\_openai.embeddings.azure.AzureOpenAIEmbeddings.html) | | [`GoogleGenerativeAIEmbeddings`](/oss/python/integrations/text\\_embedding/google\\_generative\\_ai) | [`langchain-google-genai`](https://python.langchain.com/api\\_reference/google\\_genai/embeddings/langchain\\_google\\_genai.embeddings.GoogleGenerativeAIEmbeddings.html) | | [`OllamaEmbeddings`](/oss/python/integrations/text\\_embedding/ollama) | [`langchain-ollama`](https://python.langchain.com/api\\_reference/ollama/embeddings/langchain\\_ollama.embeddings.OllamaEmbeddings.html) | | [`TogetherEmbeddings`](/oss/python/integrations/text\\_embedding/together) | [`langchain-together`](https://python.langchain.com/api\\_reference/together/embeddings/langchain\\_together.embeddings.TogetherEmbeddings.html) | | [`FireworksEmbeddings`](/oss/python/integrations/text\\_embedding/fireworks) | [`langchain-fireworks`](https://python.langchain.com/api\\_reference/fireworks/embeddings/langchain\\_fireworks.embeddings.FireworksEmbeddings.html) | | [`MistralAIEmbeddings`](/oss/python/integrations/text\\_embedding/mistralai) | [`langchain-mistralai`](https://python.langchain.com/api\\_reference/mistralai/embeddings/langchain\\_mistralai.embeddings.MistralAIEmbeddings.html) | | [`CohereEmbeddings`](/oss/python/integrations/text\\_embedding/cohere) | [`langchain-cohere`](https://python.langchain.com/api\\_reference/community/llms/langchain\\_community.llms.cohere.Cohere.html) | | [`NomicEmbeddings`](/oss/python/integrations/text\\_embedding/nomic) | [`langchain-nomic`](https://python.langchain.com/api\\_reference/nomic/embeddings/langchain\\_nomic.embeddings.NomicEmbeddings.html) | | [`FakeEmbeddings`](/oss/python/integrations/text\\_embedding/fake) | [`langchain-core`](https://python.langchain.com/api\\_reference/core/embeddings/langchain\\_core.embeddings.fake.FakeEmbeddings.html) | | [`DatabricksEmbeddings`](/oss/python/integrations/text\\_embedding/databricks) | [`databricks-langchain`](https://api-docs.databricks.com/python/databricks-ai-bridge/latest/databricks\\_langchain.html#databricks\\_langchain.DatabricksEmbeddings) | | [`WatsonxEmbeddings`](/oss/python/integrations/text\\_embedding/ibm\\_watsonx) | [`langchain-ibm`](https://python.langchain.com/api\\_reference/ibm/embeddings/langchain\\_ibm.embeddings.WatsonxEmbeddings.html) | | [`NVIDIAEmbeddings`](/oss/python/integrations/text\\_embedding/nvidia\\_ai\\_endpoints) | [`langchain-nvidia`](https://python.langchain.com/api\\_reference/nvidia\\_ai\\_endpoints/embeddings/langchain\\_nvidia\\_ai\\_endpoints.embeddings.NVIDIAEmbeddings.html) | | [`AimlapiEmbeddings`](/oss/python/integrations/text\\_embedding/aimlapi) | [`langchain-aimlapi`](https://python.langchain.com/api\\_reference/aimlapi/embeddings/langchain\\_aimlapi.embeddings.AimlapiEmbeddings.html) | ## Caching Embeddings can be stored or temporarily cached to avoid needing to recompute them. Caching embeddings can be done using a `CacheBackedEmbeddings`. This wrapper stores embeddings in a key-value store, where the text is hashed and the hash is used as the key in the cache. The main supported way to initialize a `CacheBackedEmbeddings` is `from\\_bytes\\_store`. It takes the following parameters: \\* \\*\\*`underlying\\_embedder`\\*\\*: The embedder to use for embedding. \\* \\*\\*`document\\_embedding\\_cache`\\*\\*: Any [`ByteStore`](/oss/python/integrations/stores/) for caching document embeddings. \\* \\*\\*`batch\\_size`\\*\\*: (optional, defaults to `None`) The number of documents to embed between store updates. \\* \\*\\*`namespace`\\*\\*: (optional, defaults to `\"\"`) The namespace to use for the document cache. Helps avoid collisions (e.g., set it to the embedding model name). \\* \\*\\*`query\\_embedding\\_cache`\\*\\*: (optional, defaults to `None`) A [`ByteStore`](/oss/python/integrations/stores/) for caching query embeddings, or `True` to reuse the same store as `document\\_embedding\\_cache`.  - Always set the `namespace` parameter to avoid collisions when using different embedding models. - `CacheBackedEmbeddings` does not cache query embeddings by default. To enable this, specify a `query\\_embedding\\_cache`.  ```python theme={null} import time from langchain\\_classic.embeddings import CacheBackedEmbeddings # [!code highlight] from langchain\\_classic.storage import LocalFileStore # [!code highlight] from langchain\\_core.vectorstores import InMemoryVectorStore # Create your underlying embeddings model underlying\\_embeddings = ... # e.g., OpenAIEmbeddings(), HuggingFaceEmbeddings(), etc. # Store persists embeddings to the local filesystem # This isn't for production use, but is useful for local store = LocalFileStore(\"./cache/\") # [!code highlight] cached\\_embedder = CacheBackedEmbeddings.from\\_bytes\\_store( underlying\\_embeddings, store, namespace=underlying\\_embeddings.model ) # Example: caching a query embedding tic = time.time() print(cached\\_embedder.embed\\_query(\"Hello, world!\")) print(f\"First call took: {time.time() - tic:.2f} seconds\") # Subsequent calls use the cache tic = time.time() print(cached\\_embedder.embed\\_query(\"Hello, world!\")) print(f\"Second call took: {time.time() - tic:.2f} seconds\") ``` In production, you would typically use a more robust persistent store, such as a database or cloud storage. Please see [stores integrations](/oss/python/integrations/stores/) for options. ## All embedding models   \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/text\\_embedding/index.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/integrations/tools/index.md",
      "title": null,
      "raw_content": "# Tools and toolkits [Tools](/oss/python/langchain/tools) are utilities designed to be called by a model: their inputs are designed to be generated by models, and their outputs are designed to be passed back to models. A toolkit is a collection of tools meant to be used together. ## Search The following table shows tools that execute online searches in some shape or form: | Tool/Toolkit | Free/Paid | Return Data | | ----------------------------------------------------------------- | ---------------------------- | ----------------------------------------------------- | | [Bing Search](/oss/python/integrations/tools/bing\\_search) | Paid | URL, Snippet, Title | | [Brave Search](/oss/python/integrations/tools/brave\\_search) | Free | URL, Snippet, Title | | [DuckDuckgoSearch](/oss/python/integrations/tools/ddg) | Free | URL, Snippet, Title | | [Exa Search](/oss/python/integrations/tools/exa\\_search) | 1000 free searches/month | URL, Author, Title, Published Date | | [Google Search](/oss/python/integrations/tools/google\\_search) | Paid | URL, Snippet, Title | | [Google Serper](/oss/python/integrations/tools/google\\_serper) | Free | URL, Snippet, Title, Search Rank, Site Links | | [Jina Search](/oss/python/integrations/tools/jina\\_search) | 1M Response Tokens Free | URL, Snippet, Title, Page Content | | [Mojeek Search](/oss/python/integrations/tools/mojeek\\_search) | Paid | URL, Snippet, Title | | [Parallel Search](/oss/python/integrations/tools/parallel\\_search) | Paid | URL, Title, Excerpts | | [SearchApi](/oss/python/integrations/tools/searchapi) | 100 Free Searches on Sign Up | URL, Snippet, Title, Search Rank, Site Links, Authors | | [SearxNG Search](/oss/python/integrations/tools/searx\\_search) | Free | URL, Snippet, Title, Category | | [SerpAPI](/oss/python/integrations/tools/serpapi) | 100 Free Searches/Month | Answer | | [Tavily Search](/oss/python/integrations/tools/tavily\\_search) | 1000 free searches/month | URL, Content, Title, Images, Answer | | [You.com Search](/oss/python/integrations/tools/you) | Free for 60 days | URL, Title, Page Content | ## Code Interpreter The following table shows tools that can be used as code interpreters: | Tool/Toolkit | Supported Languages | Sandbox Lifetime | Supports File Uploads | Return Types | Supports Self-Hosting | | ---------------------------------------------------------------------------------------------- | ----------------------------- | ------------------- | --------------------- | ------------ | --------------------- | | [Azure Container Apps dynamic sessions](/oss/python/integrations/tools/azure\\_dynamic\\_sessions) | Python | 1 Hour | \u2705 | Text, Images | \u274c | | [Bearly Code Interpreter](/oss/python/integrations/tools/bearly) | Python | Resets on Execution | \u2705 | Text | \u274c | | [Riza Code Interpreter](/oss/python/integrations/tools/riza) | Python, JavaScript, PHP, Ruby | Resets on Execution | \u2705 | Text | \u2705 | ## Productivity The following table shows tools that can be used to automate tasks in productivity tools: | Tool/Toolkit | Pricing | | ------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------ | | [Github Toolkit](/oss/python/integrations/tools/github) | Free | | [Gitlab Toolkit](/oss/python/integrations/tools/gitlab) | Free for personal project | | [Gmail Toolkit](/oss/python/integrations/tools/google\\_gmail) | Free, with limit of 250 quota units per user per second | | [Infobip Tool](/oss/python/integrations/tools/infobip) | Free trial, with variable pricing after | | [Jira Toolkit](/oss/python/integrations/tools/jira) | Free, with [rate limits](https://developer.atlassian.com/cloud/jira/platform/rate-limiting/) | | [Office365 Toolkit](/oss/python/integrations/tools/office365) | Free with Office365, includes [rate limits](https://learn.microsoft.com/en-us/graph/throttling-limits) | | [Slack Toolkit](/oss/python/integrations/tools/slack) | Free | | [Twilio Tool](/oss/python/integrations/tools/twilio) | Free trial, with [pay-as-you-go pricing](https://www.twilio.com/en-us/pricing) after | ## Web Browsing The following table shows tools that can be used to automate tasks in web browsers: | Tool/Toolkit | Pricing | Supports Interacting with the Browser | | --------------------------------------------------------------------------------------------------- | ----------------------------------------------------------- | ------------------------------------- | | [AgentQL Toolkit](/oss/python/integrations/tools/agentql) | Free trial, with pay-as-you-go and flat rate plans after | \u2705 | | [Hyperbrowser Browser Agent Tools](/oss/python/integrations/tools/hyperbrowser\\_browser\\_agent\\_tools) | Free trial, with flat rate plans and pre-paid credits after | \u2705 | | [Hyperbrowser Web Scraping Tools](/oss/python/integrations/tools/hyperbrowser\\_web\\_scraping\\_tools) | Free trial, with flat rate plans and pre-paid credits after | \u274c | | [MultiOn Toolkit](/oss/python/integrations/tools/multion) | 40 free requests/day | \u2705 | | [Oxylabs Web Scraper API](/oss/python/integrations/tools/oxylabs) | Free trial, with flat rate plans and pre-paid credits after | \u274c | | [PlayWright Browser Toolkit](/oss/python/integrations/tools/playwright) | Free | \u2705 | | [Requests Toolkit](/oss/python/integrations/tools/requests) | Free | \u274c | ## Database The following table shows tools that can be used to automate tasks in databases: | Tool/Toolkit | Allowed Operations | | ------------------------------------------------------------------------------- | ------------------------------- | | [Cassandra Database Toolkit](/oss/python/integrations/tools/cassandra\\_database) | SELECT and schema introspection | | [MCP Toolbox](/oss/python/integrations/tools/mcp\\_toolbox) | Any SQL operation | | [SQLDatabase Toolkit](/oss/python/integrations/tools/sql\\_database) | Any SQL operation | | [Spark SQL Toolkit](/oss/python/integrations/tools/spark\\_sql) | Any SQL operation | ## Finance The following table shows tools that can be used to execute financial transactions such as payments, purchases, and more: | Tool/Toolkit | Pricing | Capabilities | | --------------------------------------------- | ------- | --------------------------------------------------------------------------------- | | [GOAT](/oss/python/integrations/tools/goat) | Free | Create and receive payments, purchase physical goods, make investments, and more. | | [Privy](/oss/python/integrations/tools/privy) | Free | Create wallets with configurable permissions and execute transactions with speed. | ## Integration Platforms The following platforms provide access to multiple tools and services through a unified interface: | Tool/Toolkit | Number of Integrations | Pricing | Key Features | | --------------------------------------------------- | ---------------------- | ------------------- | ---------------------------------------------------------- | | [Composio](/oss/python/integrations/tools/composio) | 500+ | Free tier available | OAuth handling, event-driven workflows, multi-user support | ## All tools and toolkits    If you'd like to contribute an integration, see [Contributing integrations](/oss/python/contributing#add-a-new-integration).  \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/tools/index.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/integrations/vectorstores/index.md",
      "title": null,
      "raw_content": "# Vector stores ## Overview A vector store stores [embedded](/oss/python/integrations/text\\_embedding) data and performs similarity search. ```mermaid theme={null} flowchart LR subgraph \"\ud83d\udce5 Indexing phase (store)\" A[\ud83d\udcc4 Documents] --> B[\ud83d\udd22 Embedding model] B --> C[\ud83d\udd18 Embedding vectors] C --> D[(Vector store)] end subgraph \"\ud83d\udce4 Query phase (retrieval)\" E[\u2753 Query text] --> F[\ud83d\udd22 Embedding model] F --> G[\ud83d\udd18 Query vector] G --> H[\ud83d\udd0d Similarity search] H --> D D --> I[\ud83d\udcc4 Top-k results] end ``` ### Interface LangChain provides a unified interface for vector stores, allowing you to: \\* `add\\_documents` - Add documents to the store. \\* `delete` - Remove stored documents by ID. \\* `similarity\\_search` - Query for semantically similar documents. This abstraction lets you switch between different implementations without altering your application logic. ### Initialization To initialize a vector store, provide it with an embedding model: ```python theme={null} from langchain\\_core.vectorstores import InMemoryVectorStore vector\\_store = InMemoryVectorStore(embedding=SomeEmbeddingModel()) ``` ### Adding documents Add [`Document`](https://reference.langchain.com/python/langchain\\_core/documents/#langchain\\_core.documents.base.Document) objects (holding `page\\_content` and optional metadata) like so: ```python theme={null} vector\\_store.add\\_documents(documents=[doc1, doc2], ids=[\"id1\", \"id2\"]) ``` ### Deleting documents Delete by specifying IDs: ```python theme={null} vector\\_store.delete(ids=[\"id1\"]) ``` ### Similarity search Issue a semantic query using `similarity\\_search`, which returns the closest embedded documents: ```python theme={null} similar\\_docs = vector\\_store.similarity\\_search(\"your query here\") ``` Many vector stores support parameters like: \\* `k` \u2014 number of results to return \\* `filter` \u2014 conditional filtering based on metadata ### Similarity metrics & indexing Embedding similarity may be computed using: \\* \\*\\*Cosine similarity\\*\\* \\* \\*\\*Euclidean distance\\*\\* \\* \\*\\*Dot product\\*\\* Efficient search often employs indexing methods such as HNSW (Hierarchical Navigable Small World), though specifics depend on the vector store. ### Metadata filtering Filtering by metadata (e.g., source, date) can refine search results: ```python theme={null} vector\\_store.similarity\\_search( \"query\", k=3, filter={\"source\": \"tweets\"} ) ```  Support for metadata-based filtering varies between implementations. Check the documentation of your chosen vector store for details.  ## Top integrations \\*\\*Select embedding model:\\*\\*    ```bash pip theme={null} pip install -qU langchain-openai ``` ```bash uv theme={null} uv add langchain-openai ```  ```python theme={null} import getpass import os if not os.environ.get(\"OPENAI\\_API\\_KEY\"): os.environ[\"OPENAI\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \") from langchain\\_openai import OpenAIEmbeddings embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\") ```   ```bash theme={null} pip install -qU langchain-azure-ai ``` ```python theme={null} import getpass import os if not os.environ.get(\"AZURE\\_OPENAI\\_API\\_KEY\"): os.environ[\"AZURE\\_OPENAI\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for Azure: \") from langchain\\_openai import AzureOpenAIEmbeddings embeddings = AzureOpenAIEmbeddings( azure\\_endpoint=os.environ[\"AZURE\\_OPENAI\\_ENDPOINT\"], azure\\_deployment=os.environ[\"AZURE\\_OPENAI\\_DEPLOYMENT\\_NAME\"], openai\\_api\\_version=os.environ[\"AZURE\\_OPENAI\\_API\\_VERSION\"], ) ```   ```bash theme={null} pip install -qU langchain-google-genai ``` ```python theme={null} import getpass import os if not os.environ.get(\"GOOGLE\\_API\\_KEY\"): os.environ[\"GOOGLE\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \") from langchain\\_google\\_genai import GoogleGenerativeAIEmbeddings embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\") ```   ```bash theme={null} pip install -qU langchain-google-vertexai ``` ```python theme={null} from langchain\\_google\\_vertexai import VertexAIEmbeddings embeddings = VertexAIEmbeddings(model=\"text-embedding-005\") ```   ```bash theme={null} pip install -qU langchain-aws ``` ```python theme={null} from langchain\\_aws import BedrockEmbeddings embeddings = BedrockEmbeddings(model\\_id=\"amazon.titan-embed-text-v2:0\") ```   ```bash theme={null} pip install -qU langchain-huggingface ``` ```python theme={null} from langchain\\_huggingface import HuggingFaceEmbeddings embeddings = HuggingFaceEmbeddings(model\\_name=\"sentence-transformers/all-mpnet-base-v2\") ```   ```bash theme={null} pip install -qU langchain-ollama ``` ```python theme={null} from langchain\\_ollama import OllamaEmbeddings embeddings = OllamaEmbeddings(model=\"llama3\") ```   ```bash theme={null} pip install -qU langchain-cohere ``` ```python theme={null} import getpass import os if not os.environ.get(\"COHERE\\_API\\_KEY\"): os.environ[\"COHERE\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for Cohere: \") from langchain\\_cohere import CohereEmbeddings embeddings = CohereEmbeddings(model=\"embed-english-v3.0\") ```   ```bash theme={null} pip install -qU langchain-mistralai ``` ```python theme={null} import getpass import os if not os.environ.get(\"MISTRALAI\\_API\\_KEY\"): os.environ[\"MISTRALAI\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for MistralAI: \") from langchain\\_mistralai import MistralAIEmbeddings embeddings = MistralAIEmbeddings(model=\"mistral-embed\") ```   ```bash theme={null} pip install -qU langchain-nomic ``` ```python theme={null} import getpass import os if not os.environ.get(\"NOMIC\\_API\\_KEY\"): os.environ[\"NOMIC\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for Nomic: \") from langchain\\_nomic import NomicEmbeddings embeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\") ```   ```bash theme={null} pip install -qU langchain-nvidia-ai-endpoints ``` ```python theme={null} import getpass import os if not os.environ.get(\"NVIDIA\\_API\\_KEY\"): os.environ[\"NVIDIA\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \") from langchain\\_nvidia\\_ai\\_endpoints import NVIDIAEmbeddings embeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\") ```   ```bash theme={null} pip install -qU langchain-voyageai ``` ```python theme={null} import getpass import os if not os.environ.get(\"VOYAGE\\_API\\_KEY\"): os.environ[\"VOYAGE\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \") from langchain-voyageai import VoyageAIEmbeddings embeddings = VoyageAIEmbeddings(model=\"voyage-3\") ```   ```bash theme={null} pip install -qU langchain-ibm ``` ```python theme={null} import getpass import os if not os.environ.get(\"WATSONX\\_APIKEY\"): os.environ[\"WATSONX\\_APIKEY\"] = getpass.getpass(\"Enter API key for IBM watsonx: \") from langchain\\_ibm import WatsonxEmbeddings embeddings = WatsonxEmbeddings( model\\_id=\"ibm/slate-125m-english-rtrvr\", url=\"https://us-south.ml.cloud.ibm.com\", project\\_id=\"\", ) ```   ```bash theme={null} pip install -qU langchain-core ``` ```python theme={null} from langchain\\_core.embeddings import DeterministicFakeEmbedding embeddings = DeterministicFakeEmbedding(size=4096) ```   ```bash theme={null} pip install -qU langchain-xai ``` ```python theme={null} import getpass import os if not os.environ.get(\"XAI\\_API\\_KEY\"): os.environ[\"XAI\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for xAI: \") from langchain.chat\\_models import init\\_chat\\_model model = init\\_chat\\_model(\"grok-2\", model\\_provider=\"xai\") ```   ```bash theme={null} pip install -qU langchain-perplexity ``` ```python theme={null} import getpass import os if not os.environ.get(\"PPLX\\_API\\_KEY\"): os.environ[\"PPLX\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for Perplexity: \") from langchain.chat\\_models import init\\_chat\\_model model = init\\_chat\\_model(\"llama-3.1-sonar-small-128k-online\", model\\_provider=\"perplexity\") ```   ```bash theme={null} pip install -qU langchain-deepseek ``` ```python theme={null} import getpass import os if not os.environ.get(\"DEEPSEEK\\_API\\_KEY\"): os.environ[\"DEEPSEEK\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for DeepSeek: \") from langchain.chat\\_models import init\\_chat\\_model model = init\\_chat\\_model(\"deepseek-chat\", model\\_provider=\"deepseek\") ```   \\*\\*Select vector store:\\*\\*    ```bash pip theme={null} pip install -qU langchain-core ``` ```bash uv theme={null} uv add langchain-core ```  ```python theme={null} from langchain\\_core.vectorstores import InMemoryVectorStore vector\\_store = InMemoryVectorStore(embeddings) ```    ```bash pip theme={null} pip install -qU langchain-astradb ``` ```bash uv theme={null} uv add langchain-astradb ```  ```python theme={null} from langchain\\_astradb import AstraDBVectorStore vector\\_store = AstraDBVectorStore( embedding=embeddings, api\\_endpoint=ASTRA\\_DB\\_API\\_ENDPOINT, collection\\_name=\"astra\\_vector\\_langchain\", token=ASTRA\\_DB\\_APPLICATION\\_TOKEN, namespace=ASTRA\\_DB\\_NAMESPACE, ) ```    ```bash pip theme={null} pip install -qU langchain-azure-ai azure-cosmos ``` ```bash uv theme={null} uv add langchain-azure-ai ```  ```python theme={null} from langchain\\_azure\\_ai.vectorstores.azure\\_cosmos\\_db\\_no\\_sql import ( AzureCosmosDBNoSqlVectorSearch, ) vector\\_search = AzureCosmosDBNoSqlVectorSearch.from\\_documents( documents=docs, embedding=openai\\_embeddings, cosmos\\_client=cosmos\\_client, database\\_name=database\\_name, container\\_name=container\\_name, vector\\_embedding\\_policy=vector\\_embedding\\_policy, full\\_text\\_policy=full\\_text\\_policy, indexing\\_policy=indexing\\_policy, cosmos\\_container\\_properties=cosmos\\_container\\_properties, cosmos\\_database\\_properties={}, full\\_text\\_search\\_enabled=True, ) ```    ```bash pip theme={null} pip install -qU langchain-azure-ai pymongo ``` ```bash uv theme={null} uv add pymongo ```  ```python theme={null} from langchain\\_azure\\_ai.vectorstores.azure\\_cosmos\\_db\\_mongo\\_vcore import ( AzureCosmosDBMongoVCoreVectorSearch, ) vectorstore = AzureCosmosDBMongoVCoreVectorSearch.from\\_documents( docs, openai\\_embeddings, collection=collection, index\\_name=INDEX\\_NAME, ) ```    ```bash pip theme={null} pip install -qU langchain-chroma ``` ```bash uv theme={null} uv add langchain-chroma ```  ```python theme={null} from langchain\\_chroma import Chroma vector\\_store = Chroma( collection\\_name=\"example\\_collection\", embedding\\_function=embeddings, persist\\_directory=\"./chroma\\_langchain\\_db\", # Where to save data locally, remove if not necessary ) ```   ```bash theme={null} pip install -qU langchain-community ``` ```python theme={null} import faiss from langchain\\_community.docstore.in\\_memory import InMemoryDocstore from langchain\\_community.vectorstores import FAISS embedding\\_dim = len(embeddings.embed\\_query(\"hello world\")) index = faiss.IndexFlatL2(embedding\\_dim) vector\\_store = FAISS( embedding\\_function=embeddings, index=index, docstore=InMemoryDocstore(), index\\_to\\_docstore\\_id={}, ) ```    ```bash pip theme={null} pip install -qU langchain-milvus ``` ```bash uv theme={null} uv add langchain-milvus ```  ```python theme={null} from langchain\\_milvus import Milvus URI = \"./milvus\\_example.db\" vector\\_store = Milvus( embedding\\_function=embeddings, connection\\_args={\"uri\": URI}, index\\_params={\"index\\_type\": \"FLAT\", \"metric\\_type\": \"L2\"}, ) ```   ```bash theme={null} pip install -qU langchain-mongodb ``` ```python theme={null} from langchain\\_mongodb import MongoDBAtlasVectorSearch vector\\_store = MongoDBAtlasVectorSearch( embedding=embeddings, collection=MONGODB\\_COLLECTION, index\\_name=ATLAS\\_VECTOR\\_SEARCH\\_INDEX\\_NAME, relevance\\_score\\_fn=\"cosine\", ) ```    ```bash pip theme={null} pip install -qU langchain-postgres ``` ```bash uv theme={null} uv add langchain-postgres ```  ```python theme={null} from langchain\\_postgres import PGVector vector\\_store = PGVector( embeddings=embeddings, collection\\_name=\"my\\_docs\", connection=\"postgresql+psycopg://...\" ) ```    ```bash pip theme={null} pip install -qU langchain-postgres ``` ```bash uv theme={null} uv add langchain-postgres ```  ```python theme={null} from langchain\\_postgres import PGEngine, PGVectorStore $engine = PGEngine.from\\_connection\\_string( url=\"postgresql+psycopg://...\" ) vector\\_store = PGVectorStore.create\\_sync( engine=pg\\_engine, table\\_name='test\\_table', embedding\\_service=embedding ) ```    ```bash pip theme={null} pip install -qU langchain-pinecone ``` ```bash uv theme={null} uv add langchain-pinecone ```  ```python theme={null} from langchain\\_pinecone import PineconeVectorStore from pinecone import Pinecone pc = Pinecone(api\\_key=...) index = pc.Index(index\\_name) vector\\_store = PineconeVectorStore(embedding=embeddings, index=index) ```    ```bash pip theme={null} pip install -qU langchain-qdrant ``` ```bash uv theme={null} uv add langchain-qdrant ```  ```python theme={null} from qdrant\\_client.models import Distance, VectorParams from langchain\\_qdrant import QdrantVectorStore from qdrant\\_client import QdrantClient client = QdrantClient(\":memory:\") vector\\_size = len(embeddings.embed\\_query(\"sample text\")) if not client.collection\\_exists(\"test\"): client.create\\_collection( collection\\_name=\"test\", vectors\\_config=VectorParams(size=vector\\_size, distance=Distance.COSINE) ) vector\\_store = QdrantVectorStore( client=client, collection\\_name=\"test\", embedding=embeddings, ) ```   | Vectorstore | Delete by ID | Filtering | Search by Vector | Search with score | Async | Passes Standard Tests | Multi Tenancy | IDs in add Documents | | ---------------------------------------------------------------------------------------------------------------------------------------------------- | ------------ | --------- | ---------------- | ----------------- | ----- | --------------------- | ------------- | -------------------- | | [`AstraDBVectorStore`](/oss/python/integrations/vectorstores/astradb) | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | | [`AzureCosmosDBNoSqlVectorStore`](/oss/python/integrations/vectorstores/azure\\_cosmos\\_db\\_no\\_sql) | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u2705 | \u2705 | \u2705 | | [`AzureCosmosDBMongoVCoreVectorStore`](/oss/python/integrations/vectorstores/azure\\_cosmos\\_db\\_mongo\\_vcore) | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u2705 | \u2705 | \u2705 | | [`Chroma`](/oss/python/integrations/vectorstores/chroma) | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | | [`Clickhouse`](/oss/python/integrations/vectorstores/clickhouse) | \u2705 | \u2705 | \u274c | \u2705 | \u274c | \u274c | \u274c | \u2705 | | [`CouchbaseSearchVectorStore`](/oss/python/integrations/vectorstores/couchbase) | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u2705 | \u2705 | | [`DatabricksVectorSearch`](/oss/python/integrations/vectorstores/databricks\\_vector\\_search) | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u274c | \u2705 | | [`ElasticsearchStore`](/oss/python/integrations/vectorstores/elasticsearch) | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u274c | \u2705 | | [`FAISS`](/oss/python/integrations/vectorstores/faiss) | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u274c | \u2705 | | [`InMemoryVectorStore`](https://python.langchain.com/api\\_reference/core/vectorstores/langchain\\_core.vectorstores.in\\_memory.InMemoryVectorStore.html) | \u2705 | \u2705 | \u274c | \u2705 | \u2705 | \u274c | \u274c | \u2705 | | [`Milvus`](/oss/python/integrations/vectorstores/milvus) | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | | [`Moorcheh`](/oss/python/integrations/vectorstores/moorcheh) | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | | [`MongoDBAtlasVectorSearch`](/oss/python/integrations/vectorstores/mongodb\\_atlas) | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | | [`openGauss`](/oss/python/integrations/vectorstores/opengauss) | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u2705 | \u274c | \u2705 | | [`PGVector`](/oss/python/integrations/vectorstores/pgvector) | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u274c | \u2705 | | [`PGVectorStore`](/oss/python/integrations/vectorstores/pgvectorstore) | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u2705 | | [`PineconeVectorStore`](/oss/python/integrations/vectorstores/pinecone) | \u2705 | \u2705 | \u2705 | \u274c | \u2705 | \u274c | \u274c | \u2705 | | [`QdrantVectorStore`](/oss/python/integrations/vectorstores/qdrant) | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u2705 | \u2705 | | [`Weaviate`](/oss/python/integrations/vectorstores/weaviate) | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u2705 | \u2705 | | [`SQLServer`](/oss/python/integrations/vectorstores/sqlserver) | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u274c | \u274c | \u2705 | | [`ZeusDB`](/oss/python/integrations/vectorstores/zeusdb) | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u2705 | | [`Oracle AI Vector Search`](/oss/python/integrations/vectorstores/oracle) | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u2705 | \u274c | \u2705 | ## All vector stores   \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/integrations/vectorstores/index.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/agents.md",
      "title": null,
      "raw_content": "# Agents Agents combine language models with [tools](/oss/python/langchain/tools) to create systems that can reason about tasks, decide which tools to use, and iteratively work towards solutions. [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent) provides a production-ready agent implementation. [An LLM Agent runs tools in a loop to achieve a goal](https://simonwillison.net/2025/Sep/18/agents/). An agent runs until a stop condition is met - i.e., when the model emits a final output or an iteration limit is reached. ```mermaid theme={null} %%{ init: { \"fontFamily\": \"monospace\", \"flowchart\": { \"curve\": \"curve\" }, \"themeVariables\": {\"edgeLabelBackground\": \"transparent\"} } }%% graph TD %% Outside the agent QUERY([input]) LLM{model} TOOL(tools) ANSWER([output]) %% Main flows (no inline labels) QUERY --> LLM LLM --\"action\"--> TOOL TOOL --\"observation\"--> LLM LLM --\"finish\"--> ANSWER classDef blueHighlight fill:#0a1c25,stroke:#0a455f,color:#bae6fd; classDef greenHighlight fill:#0b1e1a,stroke:#0c4c39,color:#9ce4c4; class QUERY blueHighlight; class ANSWER blueHighlight; ```  [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent) builds a \\*\\*graph\\*\\*-based agent runtime using [LangGraph](/oss/python/langgraph/overview). A graph consists of nodes (steps) and edges (connections) that define how your agent processes information. The agent moves through this graph, executing nodes like the model node (which calls the model), the tools node (which executes tools), or middleware. Learn more about the [Graph API](/oss/python/langgraph/graph-api).  ## Core components ### Model The [model](/oss/python/langchain/models) is the reasoning engine of your agent. It can be specified in multiple ways, supporting both static and dynamic model selection. #### Static model Static models are configured once when creating the agent and remain unchanged throughout execution. This is the most common and straightforward approach. To initialize a static model from a model identifier string: ```python wrap theme={null} from langchain.agents import create\\_agent agent = create\\_agent( \"gpt-5\", tools=tools ) ```  Model identifier strings support automatic inference (e.g., `\"gpt-5\"` will be inferred as `\"openai:gpt-5\"`). Refer to the [reference](https://reference.langchain.com/python/langchain/models/#langchain.chat\\_models.init\\_chat\\_model\\(model\\)) to see a full list of model identifier string mappings.  For more control over the model configuration, initialize a model instance directly using the provider package. In this example, we use [`ChatOpenAI`](https://reference.langchain.com/python/integrations/langchain\\_openai/ChatOpenAI). See [Chat models](/oss/python/integrations/chat) for other available chat model classes. ```python wrap theme={null} from langchain.agents import create\\_agent from langchain\\_openai import ChatOpenAI model = ChatOpenAI( model=\"gpt-5\", temperature=0.1, max\\_tokens=1000, timeout=30 # ... (other params) ) agent = create\\_agent(model, tools=tools) ``` Model instances give you complete control over configuration. Use them when you need to set specific [parameters](/oss/python/langchain/models#parameters) like `temperature`, `max\\_tokens`, `timeouts`, `base\\_url`, and other provider-specific settings. Refer to the [reference](/oss/python/integrations/providers/all\\_providers) to see available params and methods on your model. #### Dynamic model Dynamic models are selected at runtime based on the current state and context. This enables sophisticated routing logic and cost optimization. To use a dynamic model, create middleware using the [`@wrap\\_model\\_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap\\_model\\_call) decorator that modifies the model in the request: ```python theme={null} from langchain\\_openai import ChatOpenAI from langchain.agents import create\\_agent from langchain.agents.middleware import wrap\\_model\\_call, ModelRequest, ModelResponse basic\\_model = ChatOpenAI(model=\"gpt-4o-mini\") advanced\\_model = ChatOpenAI(model=\"gpt-4o\") @wrap\\_model\\_call def dynamic\\_model\\_selection(request: ModelRequest, handler) -> ModelResponse: \"\"\"Choose model based on conversation complexity.\"\"\" message\\_count = len(request.state[\"messages\"]) if message\\_count > 10: # Use an advanced model for longer conversations model = advanced\\_model else: model = basic\\_model return handler(request.override(model=model)) agent = create\\_agent( model=basic\\_model, # Default model tools=tools, middleware=[dynamic\\_model\\_selection] ) ```  Pre-bound models (models with [`bind\\_tools`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.language\\_models.chat\\_models.BaseChatModel.bind\\_tools) already called) are not supported when using structured output. If you need dynamic model selection with structured output, ensure the models passed to the middleware are not pre-bound.   For model configuration details, see [Models](/oss/python/langchain/models). For dynamic model selection patterns, see [Dynamic model in middleware](/oss/python/langchain/middleware#dynamic-model).  ### Tools Tools give agents the ability to take actions. Agents go beyond simple model-only tool binding by facilitating: \\* Multiple tool calls in sequence (triggered by a single prompt) \\* Parallel tool calls when appropriate \\* Dynamic tool selection based on previous results \\* Tool retry logic and error handling \\* State persistence across tool calls For more information, see [Tools](/oss/python/langchain/tools). #### Defining tools Pass a list of tools to the agent.  Tools can be specified as plain Python functions or coroutines. The [tool decorator](/oss/python/langchain/tools#create-tools) can be used to customize tool names, descriptions, argument schemas, and other properties.  ```python wrap theme={null} from langchain.tools import tool from langchain.agents import create\\_agent @tool def search(query: str) -> str: \"\"\"Search for information.\"\"\" return f\"Results for: {query}\" @tool def get\\_weather(location: str) -> str: \"\"\"Get weather information for a location.\"\"\" return f\"Weather in {location}: Sunny, 72\u00b0F\" agent = create\\_agent(model, tools=[search, get\\_weather]) ``` If an empty tool list is provided, the agent will consist of a single LLM node without tool-calling capabilities. #### Tool error handling To customize how tool errors are handled, use the [`@wrap\\_tool\\_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap\\_tool\\_call) decorator to create middleware: ```python wrap theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import wrap\\_tool\\_call from langchain.messages import ToolMessage @wrap\\_tool\\_call def handle\\_tool\\_errors(request, handler): \"\"\"Handle tool execution errors with custom messages.\"\"\" try: return handler(request) except Exception as e: # Return a custom error message to the model return ToolMessage( content=f\"Tool error: Please check your input and try again. ({str(e)})\", tool\\_call\\_id=request.tool\\_call[\"id\"] ) agent = create\\_agent( model=\"gpt-4o\", tools=[search, get\\_weather], middleware=[handle\\_tool\\_errors] ) ``` The agent will return a [`ToolMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage) with the custom error message when a tool fails: ```python theme={null} [ ... ToolMessage( content=\"Tool error: Please check your input and try again. (division by zero)\", tool\\_call\\_id=\"...\" ), ... ] ``` #### Tool use in the ReAct loop Agents follow the ReAct (\"Reasoning + Acting\") pattern, alternating between brief reasoning steps with targeted tool calls and feeding the resulting observations into subsequent decisions until they can deliver a final answer.  \\*\\*Prompt:\\*\\* Identify the current most popular wireless headphones and verify availability. ``` ================================ Human Message ================================= Find the most popular wireless headphones right now and check if they're in stock ``` \\* \\*\\*Reasoning\\*\\*: \"Popularity is time-sensitive, I need to use the provided search tool.\" \\* \\*\\*Acting\\*\\*: Call `search\\_products(\"wireless headphones\")` ``` ================================== Ai Message ================================== Tool Calls: search\\_products (call\\_abc123) Call ID: call\\_abc123 Args: query: wireless headphones ``` ``` ================================= Tool Message ================================= Found 5 products matching \"wireless headphones\". Top 5 results: WH-1000XM5, ... ``` \\* \\*\\*Reasoning\\*\\*: \"I need to confirm availability for the top-ranked item before answering.\" \\* \\*\\*Acting\\*\\*: Call `check\\_inventory(\"WH-1000XM5\")` ``` ================================== Ai Message ================================== Tool Calls: check\\_inventory (call\\_def456) Call ID: call\\_def456 Args: product\\_id: WH-1000XM5 ``` ``` ================================= Tool Message ================================= Product WH-1000XM5: 10 units in stock ``` \\* \\*\\*Reasoning\\*\\*: \"I have the most popular model and its stock status. I can now answer the user's question.\" \\* \\*\\*Acting\\*\\*: Produce final answer ``` ================================== Ai Message ================================== I found wireless headphones (model WH-1000XM5) with 10 units in stock... ```   To learn more about tools, see [Tools](/oss/python/langchain/tools).  ### System prompt You can shape how your agent approaches tasks by providing a prompt. The [`system\\_prompt`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent\\(system\\_prompt\\)) parameter can be provided as a string: ```python wrap theme={null} agent = create\\_agent( model, tools, system\\_prompt=\"You are a helpful assistant. Be concise and accurate.\" ) ``` When no [`system\\_prompt`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent\\(system\\_prompt\\)) is provided, the agent will infer its task from the messages directly. #### Dynamic system prompt For more advanced use cases where you need to modify the system prompt based on runtime context or agent state, you can use [middleware](/oss/python/langchain/middleware). The [`@dynamic\\_prompt`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.dynamic\\_prompt) decorator creates middleware that generates system prompts based on the model request: ```python wrap theme={null} from typing import TypedDict from langchain.agents import create\\_agent from langchain.agents.middleware import dynamic\\_prompt, ModelRequest class Context(TypedDict): user\\_role: str @dynamic\\_prompt def user\\_role\\_prompt(request: ModelRequest) -> str: \"\"\"Generate system prompt based on user role.\"\"\" user\\_role = request.runtime.context.get(\"user\\_role\", \"user\") base\\_prompt = \"You are a helpful assistant.\" if user\\_role == \"expert\": return f\"{base\\_prompt} Provide detailed technical responses.\" elif user\\_role == \"beginner\": return f\"{base\\_prompt} Explain concepts simply and avoid jargon.\" return base\\_prompt agent = create\\_agent( model=\"gpt-4o\", tools=[web\\_search], middleware=[user\\_role\\_prompt], context\\_schema=Context ) # The system prompt will be set dynamically based on context result = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"Explain machine learning\"}]}, context={\"user\\_role\": \"expert\"} ) ```  For more details on message types and formatting, see [Messages](/oss/python/langchain/messages). For comprehensive middleware documentation, see [Middleware](/oss/python/langchain/middleware).  ## Invocation You can invoke an agent by passing an update to its [`State`](/oss/python/langgraph/graph-api#state). All agents include a [sequence of messages](/oss/python/langgraph/use-graph-api#messagesstate) in their state; to invoke the agent, pass a new message: ```python theme={null} result = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in San Francisco?\"}]} ) ``` For streaming steps and / or tokens from the agent, refer to the [streaming](/oss/python/langchain/streaming) guide. Otherwise, the agent follows the LangGraph [Graph API](/oss/python/langgraph/use-graph-api) and supports all associated methods, such as `stream` and `invoke`. ## Advanced concepts ### Structured output In some situations, you may want the agent to return an output in a specific format. LangChain provides strategies for structured output via the `response\\_format` parameter. #### ToolStrategy `ToolStrategy` uses artificial tool calling to generate structured output. This works with any model that supports tool calling: ```python wrap theme={null} from pydantic import BaseModel from langchain.agents import create\\_agent from langchain.agents.structured\\_output import ToolStrategy class ContactInfo(BaseModel): name: str email: str phone: str agent = create\\_agent( model=\"gpt-4o-mini\", tools=[search\\_tool], response\\_format=ToolStrategy(ContactInfo) ) result = agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}] }) result[\"structured\\_response\"] # ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567') ``` #### ProviderStrategy `ProviderStrategy` uses the model provider's native structured output generation. This is more reliable but only works with providers that support native structured output (e.g., OpenAI): ```python wrap theme={null} from langchain.agents.structured\\_output import ProviderStrategy agent = create\\_agent( model=\"gpt-4o\", response\\_format=ProviderStrategy(ContactInfo) ) ```  As of `langchain 1.0`, simply passing a schema (e.g., `response\\_format=ContactInfo`) is no longer supported. You must explicitly use `ToolStrategy` or `ProviderStrategy`.   To learn about structured output, see [Structured output](/oss/python/langchain/structured-output).  ### Memory Agents maintain conversation history automatically through the message state. You can also configure the agent to use a custom state schema to remember additional information during the conversation. Information stored in the state can be thought of as the [short-term memory](/oss/python/langchain/short-term-memory) of the agent: Custom state schemas must extend [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState) as a `TypedDict`. There are two ways to define custom state: 1. Via [middleware](/oss/python/langchain/middleware) (preferred) 2. Via [`state\\_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state\\_schema) on [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent) #### Defining state via middleware Use middleware to define custom state when your custom state needs to be accessed by specific middleware hooks and tools attached to said middleware. ```python theme={null} from langchain.agents import AgentState from langchain.agents.middleware import AgentMiddleware from typing import Any class CustomState(AgentState): user\\_preferences: dict class CustomMiddleware(AgentMiddleware): state\\_schema = CustomState tools = [tool1, tool2] def before\\_model(self, state: CustomState, runtime) -> dict[str, Any] | None: ... agent = create\\_agent( model, tools=tools, middleware=[CustomMiddleware()] ) # The agent can now track additional state beyond messages result = agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}], \"user\\_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"}, }) ``` #### Defining state via `state\\_schema` Use the [`state\\_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state\\_schema) parameter as a shortcut to define custom state that is only used in tools. ```python theme={null} from langchain.agents import AgentState class CustomState(AgentState): user\\_preferences: dict agent = create\\_agent( model, tools=[tool1, tool2], state\\_schema=CustomState ) # The agent can now track additional state beyond messages result = agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}], \"user\\_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"}, }) ```  As of `langchain 1.0`, custom state schemas \\*\\*must\\*\\* be `TypedDict` types. Pydantic models and dataclasses are no longer supported. See the [v1 migration guide](/oss/python/migrate/langchain-v1#state-type-restrictions) for more details.   Defining custom state via middleware is preferred over defining it via [`state\\_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state\\_schema) on [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent) because it allows you to keep state extensions conceptually scoped to the relevant middleware and tools. [`state\\_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state\\_schema) is still supported for backwards compatibility on [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent).   To learn more about memory, see [Memory](/oss/python/concepts/memory). For information on implementing long-term memory that persists across sessions, see [Long-term memory](/oss/python/langchain/long-term-memory).  ### Streaming We've seen how the agent can be called with `invoke` to get a final response. If the agent executes multiple steps, this may take a while. To show intermediate progress, we can stream back messages as they occur. ```python theme={null} for chunk in agent.stream({ \"messages\": [{\"role\": \"user\", \"content\": \"Search for AI news and summarize the findings\"}] }, stream\\_mode=\"values\"): # Each chunk contains the full state at that point latest\\_message = chunk[\"messages\"][-1] if latest\\_message.content: print(f\"Agent: {latest\\_message.content}\") elif latest\\_message.tool\\_calls: print(f\"Calling tools: {[tc['name'] for tc in latest\\_message.tool\\_calls]}\") ```  For more details on streaming, see [Streaming](/oss/python/langchain/streaming).  ### Middleware [Middleware](/oss/python/langchain/middleware) provides powerful extensibility for customizing agent behavior at different stages of execution. You can use middleware to: \\* Process state before the model is called (e.g., message trimming, context injection) \\* Modify or validate the model's response (e.g., guardrails, content filtering) \\* Handle tool execution errors with custom logic \\* Implement dynamic model selection based on state or context \\* Add custom logging, monitoring, or analytics Middleware integrates seamlessly into the agent's execution, allowing you to intercept and modify data flow at key points without changing the core agent logic.  For comprehensive middleware documentation including decorators like [`@before\\_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.before\\_model), [`@after\\_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.after\\_model), and [`@wrap\\_tool\\_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap\\_tool\\_call), see [Middleware](/oss/python/langchain/middleware).  \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/agents.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/component-architecture.md",
      "title": null,
      "raw_content": "# Component architecture LangChain's power comes from how its components work together to create sophisticated AI applications. This page provides diagrams showcasing the relationships between different components. ## Core component ecosystem The diagram below shows how LangChain's major components connect to form complete AI applications: ```mermaid theme={null} graph TD %% Input processing subgraph \"\ud83d\udce5 Input processing\" A[Text input] --> B[Document loaders] B --> C[Text splitters] C --> D[Documents] end %% Embedding & storage subgraph \"\ud83d\udd22 Embedding & storage\" D --> E[Embedding models] E --> F[Vectors] F --> G[(Vector stores)] end %% Retrieval subgraph \"\ud83d\udd0d Retrieval\" H[User Query] --> I[Embedding models] I --> J[Query vector] J --> K[Retrievers] K --> G G --> L[Relevant context] end %% Generation subgraph \"\ud83e\udd16 Generation\" M[Chat models] --> N[Tools] N --> O[Tool results] O --> M L --> M M --> P[AI response] end %% Orchestration subgraph \"\ud83c\udfaf Orchestration\" Q[Agents] --> M Q --> N Q --> K Q --> R[Memory] end ``` ### How components connect Each component layer builds on the previous ones: 1. \\*\\*Input processing\\*\\* \u2013 Transform raw data into structured documents 2. \\*\\*Embedding & storage\\*\\* \u2013 Convert text into searchable vector representations 3. \\*\\*Retrieval\\*\\* \u2013 Find relevant information based on user queries 4. \\*\\*Generation\\*\\* \u2013 Use AI models to create responses, optionally with tools 5. \\*\\*Orchestration\\*\\* \u2013 Coordinate everything through agents and memory systems ## Component categories LangChain organizes components into these main categories: | Category | Purpose | Key Components | Use Cases | | -------------------------------------------------------------------- | --------------------------- | ----------------------------------- | -------------------------------------------------- | | \\*\\*[Models](/oss/python/langchain/models)\\*\\* | AI reasoning and generation | Chat models, LLMs, Embedding models | Text generation, reasoning, semantic understanding | | \\*\\*[Tools](/oss/python/langchain/tools)\\*\\* | External capabilities | APIs, databases, etc. | Web search, data access, computations | | \\*\\*[Agents](/oss/python/langchain/agents)\\*\\* | Orchestration and reasoning | ReAct agents, tool calling agents | Nondeterministic workflows, decision making | | \\*\\*[Memory](/oss/python/langchain/short-term-memory)\\*\\* | Context preservation | Message history, custom state | Conversations, stateful interactions | | \\*\\*[Retrievers](/oss/python/integrations/retrievers)\\*\\* | Information access | Vector retrievers, web retrievers | RAG, knowledge base search | | \\*\\*[Document processing](/oss/python/integrations/document\\_loaders)\\*\\* | Data ingestion | Loaders, splitters, transformers | PDF processing, web scraping | | \\*\\*[Vector Stores](/oss/python/integrations/vectorstores)\\*\\* | Semantic search | Chroma, Pinecone, FAISS | Similarity search, embeddings storage | ## Common patterns ### RAG (Retrieval-Augmented Generation) ```mermaid theme={null} graph LR A[User question] --> B[Retriever] B --> C[Relevant docs] C --> D[Chat model] A --> D D --> E[Informed response] ``` ### Agent with tools ```mermaid theme={null} graph LR A[User request] --> B[Agent] B --> C{Need tool?} C -->|Yes| D[Call tool] D --> E[Tool result] E --> B C -->|No| F[Final answer] ``` ### Multi-agent system ```mermaid theme={null} graph LR A[Complex Task] --> B[Supervisor agent] B --> C[Specialist agent 1] B --> D[Specialist agent 2] C --> E[Results] D --> E E --> B B --> F[Coordinated response] ``` ## Learn more Now that you understand how components relate to each other, explore specific areas: \\* [Building your first RAG system](/oss/python/langchain/knowledge-base) \\* [Creating agents](/oss/python/langchain/agents) \\* [Working with tools](/oss/python/langchain/tools) \\* [Setting up memory](/oss/python/langchain/short-term-memory) \\* [Browse integrations](/oss/python/integrations/providers/overview) \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/component-architecture.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/context-engineering.md",
      "title": "Context engineering in agents",
      "raw_content": "# Context engineering in agents\n\n## Overview\n\nThe hard part of building agents (or any LLM application) is making them reliable enough. While they may work for a prototype, they often fail in real-world use cases.\n\n### Why do agents fail?\n\nWhen agents fail, it's usually because the LLM call inside the agent took the wrong action / didn't do what we expected. LLMs fail for one of two reasons:\n\n1. The underlying LLM is not capable enough\n2. The \"right\" context was not passed to the LLM\n\nMore often than not - it's actually the second reason that causes agents to not be reliable.\n\n**Context engineering** is providing the right information and tools in the right format so the LLM can accomplish a task. This is the number one job of AI Engineers. This lack of \"right\" context is the number one blocker for more reliable agents, and LangChain's agent abstractions are uniquely designed to facilitate context engineering.\n\n New to context engineering? Start with the [conceptual overview](/oss/python/concepts/context) to understand the different types of context and when to use them. \n\n### The agent loop\n\nA typical agent loop consists of two main steps:\n\n1. **Model call** - calls the LLM with a prompt and available tools, returns either a response or a request to execute tools\n2. **Tool execution** - executes the tools that the LLM requested, returns tool results\n\n ![Image 1: Core agent loop diagram](https://mintcdn.com/langchain-5e9cc07a/Tazq8zGc0yYUYrDl/oss/images/core_agent_loop.png?fit=max&auto=format&n=Tazq8zGc0yYUYrDl&q=85&s=ac72e48317a9ced68fd1be64e89ec063)\n\nThis loop continues until the LLM decides to finish.\n\n### What you can control\n\nTo build reliable agents, you need to control what happens at each step of the agent loop, as well as what happens between steps.\n\n| Context Type                                  | What You Control                                                                     | Transient or Persistent |\n| --------------------------------------------- | ------------------------------------------------------------------------------------ | ----------------------- |\n| **[Model Context](#model-context)**           | What goes into model calls (instructions, message history, tools, response format)   | Transient               |\n| **[Tool Context](#tool-context)**             | What tools can access and produce (reads/writes to state, store, runtime context)    | Persistent              |\n| **[Life-cycle Context](#life-cycle-context)** | What happens between model and tool calls (summarization, guardrails, logging, etc.) | Persistent              |\n\n  What the LLM sees for a single call. You can modify messages, tools, or prompts without changing what's saved in state.   What gets saved in state across turns. Life-cycle hooks and tool writes modify this permanently. \n\n### Data sources\n\nThroughout this process, your agent accesses (reads / writes) different sources of data:\n\n| Data Source         | Also Known As        | Scope               | Examples                                                                   |\n| ------------------- | -------------------- | ------------------- | -------------------------------------------------------------------------- |\n| **Runtime Context** | Static configuration | Conversation-scoped | User ID, API keys, database connections, permissions, environment settings |\n| **State**           | Short-term memory    | Conversation-scoped | Current messages, uploaded files, authentication status, tool results      |\n| **Store**           | Long-term memory     | Cross-conversation  | User preferences, extracted insights, memories, historical data            |\n\n### How it works\n\nLangChain [middleware](/oss/python/langchain/middleware) is the mechanism under the hood that makes context engineering practical for developers using LangChain.\n\nMiddleware allows you to hook into any step in the agent lifecycle and:\n\n* Update context\n* Jump to a different step in the agent lifecycle\n\nThroughout this guide, you'll see frequent use of the middleware API as a means to the context engineering end.\n\n## Model Context\n\nControl what goes into each model call - instructions, available tools, which model to use, and output format. These decisions directly impact reliability and cost.\n\n  Base instructions from the developer to the LLM.   The full list of messages (conversation history) sent to the LLM.   Utilities the agent has access to to take actions.   The actual model (including configuration) to be called.   Schema specification for the model's final response. \n\nAll of these types of model context can draw from **state** (short-term memory), **store** (long-term memory), or **runtime context** (static configuration).\n\n### System Prompt\n\nThe system prompt sets the LLM's behavior and capabilities. Different users, contexts, or conversation stages need different instructions. Successful agents draw on memories, preferences, and configuration to provide the right instructions for the current state of the conversation.\n\n  Access message count or conversation context from state: ```python theme={null} from langchain.agents import create_agent from langchain.agents.middleware import dynamic_prompt, ModelRequest @dynamic_prompt def state_aware_prompt(request: ModelRequest) -> str: # request.messages is a shortcut for request.state[\"messages\"] message_count = len(request.messages) base = \"You are a helpful assistant.\" if message_count > 10: base += \"\\nThis is a long conversation - be extra concise.\" return base agent = create_agent( model=\"gpt-4o\", tools=[...], middleware=[state_aware_prompt] ) ```   Access user preferences from long-term memory: ```python theme={null} from dataclasses import dataclass from langchain.agents import create_agent from langchain.agents.middleware import dynamic_prompt, ModelRequest from langgraph.store.memory import InMemoryStore @dataclass class Context: user_id: str @dynamic_prompt def store_aware_prompt(request: ModelRequest) -> str: user_id = request.runtime.context.user_id # Read from Store: get user preferences store = request.runtime.store user_prefs = store.get((\"preferences\",), user_id) base = \"You are a helpful assistant.\" if user_prefs: style = user_prefs.value.get(\"communication_style\", \"balanced\") base += f\"\\nUser prefers {style} responses.\" return base agent = create_agent( model=\"gpt-4o\", tools=[...], middleware=[store_aware_prompt], context_schema=Context, store=InMemoryStore() ) ```   Access user ID or configuration from Runtime Context: ```python theme={null} from dataclasses import dataclass from langchain.agents import create_agent from langchain.agents.middleware import dynamic_prompt, ModelRequest @dataclass class Context: user_role: str deployment_env: str @dynamic_prompt def context_aware_prompt(request: ModelRequest) -> str: # Read from Runtime Context: user role and environment user_role = request.runtime.context.user_role env = request.runtime.context.deployment_env base = \"You are a helpful assistant.\" if user_role == \"admin\": base += \"\\nYou have admin access. You can perform all operations.\" elif user_role == \"viewer\": base += \"\\nYou have read-only access. Guide users to read operations only.\" if env == \"production\": base += \"\\nBe extra careful with any data modifications.\" return base agent = create_agent( model=\"gpt-4o\", tools=[...], middleware=[context_aware_prompt], context_schema=Context ) ``` \n\n### Messages\n\nMessages make up the prompt that is sent to the LLM.\nIt's critical to manage the content of messages to ensure that the LLM has the right information to respond well.\n\n  Inject uploaded file context from State when relevant to current query: ```python theme={null} from langchain.agents import create_agent from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse from typing import Callable @wrap_model_call def inject_file_context( request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse] ) -> ModelResponse: \"\"\"Inject context about files user has uploaded this session.\"\"\" # Read from State: get uploaded files metadata uploaded_files = request.state.get(\"uploaded_files\", []) # [!code highlight] if uploaded_files: # Build context about available files file_descriptions = [] for file in uploaded_files: file_descriptions.append( f\"- {file['name']} ({file['type']}): {file['summary']}\" ) file_context = f\"\"\"Files you have access to in this conversation: {chr(10).join(file_descriptions)} Reference these files when answering questions.\"\"\" # Inject file context before recent messages messages = [ # [!code highlight] *request.messages, {\"role\": \"user\", \"content\": file_context}, ] request = request.override(messages=messages) # [!code highlight] return handler(request) agent = create_agent( model=\"gpt-4o\", tools=[...], middleware=[inject_file_context] ) ```   Inject user's email writing style from Store to guide drafting: ```python theme={null} from dataclasses import dataclass from langchain.agents import create_agent from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse from typing import Callable from langgraph.store.memory import InMemoryStore @dataclass class Context: user_id: str @wrap_model_call def inject_writing_style( request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse] ) -> ModelResponse: \"\"\"Inject user's email writing style from Store.\"\"\" user_id = request.runtime.context.user_id # [!code highlight] # Read from Store: get user's writing style examples store = request.runtime.store # [!code highlight] writing_style = store.get((\"writing_style\",), user_id) # [!code highlight] if writing_style: style = writing_style.value # Build style guide from stored examples style_context = f\"\"\"Your writing style: - Tone: {style.get('tone', 'professional')} - Typical greeting: \"{style.get('greeting', 'Hi')}\" - Typical sign-off: \"{style.get('sign_off', 'Best')}\" - Example email you've written: {style.get('example_email', '')}\"\"\" # Append at end - models pay more attention to final messages messages = [ *request.messages, {\"role\": \"user\", \"content\": style_context} ] request = request.override(messages=messages) # [!code highlight] return handler(request) agent = create_agent( model=\"gpt-4o\", tools=[...], middleware=[inject_writing_style], context_schema=Context, store=InMemoryStore() ) ```   Inject compliance rules from Runtime Context based on user's jurisdiction: ```python theme={null} from dataclasses import dataclass from langchain.agents import create_agent from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse from typing import Callable @dataclass class Context: user_jurisdiction: str industry: str compliance_frameworks: list[str] @wrap_model_call def inject_compliance_rules( request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse] ) -> ModelResponse: \"\"\"Inject compliance constraints from Runtime Context.\"\"\" # Read from Runtime Context: get compliance requirements jurisdiction = request.runtime.context.user_jurisdiction # [!code highlight] industry = request.runtime.context.industry # [!code highlight] frameworks = request.runtime.context.compliance_frameworks # [!code highlight] # Build compliance constraints rules = [] if \"GDPR\" in frameworks: rules.append(\"- Must obtain explicit consent before processing personal data\") rules.append(\"- Users have right to data deletion\") if \"HIPAA\" in frameworks: rules.append(\"- Cannot share patient health information without authorization\") rules.append(\"- Must use secure, encrypted communication\") if industry == \"finance\": rules.append(\"- Cannot provide financial advice without proper disclaimers\") if rules: compliance_context = f\"\"\"Compliance requirements for {jurisdiction}: {chr(10).join(rules)}\"\"\" # Append at end - models pay more attention to final messages messages = [ *request.messages, {\"role\": \"user\", \"content\": compliance_context} ] request = request.override(messages=messages) # [!code highlight] return handler(request) agent = create_agent( model=\"gpt-4o\", tools=[...], middleware=[inject_compliance_rules], context_schema=Context ) ``` \n\n **Transient vs Persistent Message Updates:** The examples above use `wrap_model_call` to make **transient** updates - modifying what messages are sent to the model for a single call without changing what's saved in state. For **persistent** updates that modify state (like the summarization example in [Life-cycle Context](#summarization)), use life-cycle hooks like `before_model` or `after_model` to permanently update the conversation history. See the [middleware documentation](/oss/python/langchain/middleware) for more details. \n\n### Tools\n\nTools let the model interact with databases, APIs, and external systems. How you define and select tools directly impacts whether the model can complete tasks effectively.\n\n#### Defining tools\n\nEach tool needs a clear name, description, argument names, and argument descriptions. These aren't just metadata\u2014they guide the model's reasoning about when and how to use the tool.\n\n```python  theme={null}\nfrom langchain.tools import tool\n\n@tool(parse_docstring=True)\ndef search_orders(\n    user_id: str,\n    status: str,\n    limit: int = 10\n) -> str:\n    \"\"\"Search for user orders by status.\n\n    Use this when the user asks about order history or wants to check\n    order status. Always filter by the provided status.\n\n    Args:\n        user_id: Unique identifier for the user\n        status: Order status: 'pending', 'shipped', or 'delivered'\n        limit: Maximum number of results to return\n    \"\"\"\n    # Implementation here\n    pass\n```\n\n#### Selecting tools\n\nNot every tool is appropriate for every situation. Too many tools may overwhelm the model (overload context) and increase errors; too few limit capabilities. Dynamic tool selection adapts the available toolset based on authentication state, user permissions, feature flags, or conversation stage.\n\n  Enable advanced tools only after certain conversation milestones: ```python theme={null} from langchain.agents import create_agent from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse from typing import Callable @wrap_model_call def state_based_tools( request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse] ) -> ModelResponse: \"\"\"Filter tools based on conversation State.\"\"\" # Read from State: check if user has authenticated state = request.state # [!code highlight] is_authenticated = state.get(\"authenticated\", False) # [!code highlight] message_count = len(state[\"messages\"]) # Only enable sensitive tools after authentication if not is_authenticated: tools = [t for t in request.tools if t.name.startswith(\"public_\")] request = request.override(tools=tools) # [!code highlight] elif message_count < 5: # Limit tools early in conversation tools = [t for t in request.tools if t.name != \"advanced_search\"] request = request.override(tools=tools) # [!code highlight] return handler(request) agent = create_agent( model=\"gpt-4o\", tools=[public_search, private_search, advanced_search], middleware=[state_based_tools] ) ```   Filter tools based on user preferences or feature flags in Store: ```python theme={null} from dataclasses import dataclass from langchain.agents import create_agent from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse from typing import Callable from langgraph.store.memory import InMemoryStore @dataclass class Context: user_id: str @wrap_model_call def store_based_tools( request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse] ) -> ModelResponse: \"\"\"Filter tools based on Store preferences.\"\"\" user_id = request.runtime.context.user_id # Read from Store: get user's enabled features store = request.runtime.store feature_flags = store.get((\"features\",), user_id) if feature_flags: enabled_features = feature_flags.value.get(\"enabled_tools\", []) # Only include tools that are enabled for this user tools = [t for t in request.tools if t.name in enabled_features] request = request.override(tools=tools) return handler(request) agent = create_agent( model=\"gpt-4o\", tools=[search_tool, analysis_tool, export_tool], middleware=[store_based_tools], context_schema=Context, store=InMemoryStore() ) ```   Filter tools based on user permissions from Runtime Context: ```python theme={null} from dataclasses import dataclass from langchain.agents import create_agent from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse from typing import Callable @dataclass class Context: user_role: str @wrap_model_call def context_based_tools( request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse] ) -> ModelResponse: \"\"\"Filter tools based on Runtime Context permissions.\"\"\" # Read from Runtime Context: get user role user_role = request.runtime.context.user_role if user_role == \"admin\": # Admins get all tools pass elif user_role == \"editor\": # Editors can't delete tools = [t for t in request.tools if t.name != \"delete_data\"] request = request.override(tools=tools) else: # Viewers get read-only tools tools = [t for t in request.tools if t.name.startswith(\"read_\")] request = request.override(tools=tools) return handler(request) agent = create_agent( model=\"gpt-4o\", tools=[read_data, write_data, delete_data], middleware=[context_based_tools], context_schema=Context ) ``` \n\nSee [Dynamically selecting tools](/oss/python/langchain/middleware#dynamically-selecting-tools) for more examples.\n\n### Model\n\nDifferent models have different strengths, costs, and context windows. Select the right model for the task at hand, which\nmight change during an agent run.\n\n  Use different models based on conversation length from State: ```python theme={null} from langchain.agents import create_agent from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse from langchain.chat_models import init_chat_model from typing import Callable # Initialize models once outside the middleware large_model = init_chat_model(\"claude-sonnet-4-5-20250929\") standard_model = init_chat_model(\"gpt-4o\") efficient_model = init_chat_model(\"gpt-4o-mini\") @wrap_model_call def state_based_model( request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse] ) -> ModelResponse: \"\"\"Select model based on State conversation length.\"\"\" # request.messages is a shortcut for request.state[\"messages\"] message_count = len(request.messages) # [!code highlight] if message_count > 20: # Long conversation - use model with larger context window model = large_model elif message_count > 10: # Medium conversation model = standard_model else: # Short conversation - use efficient model model = efficient_model request = request.override(model=model) # [!code highlight] return handler(request) agent = create_agent( model=\"gpt-4o-mini\", tools=[...], middleware=[state_based_model] ) ```   Use user's preferred model from Store: ```python theme={null} from dataclasses import dataclass from langchain.agents import create_agent from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse from langchain.chat_models import init_chat_model from typing import Callable from langgraph.store.memory import InMemoryStore @dataclass class Context: user_id: str # Initialize available models once MODEL_MAP = { \"gpt-4o\": init_chat_model(\"gpt-4o\"), \"gpt-4o-mini\": init_chat_model(\"gpt-4o-mini\"), \"claude-sonnet\": init_chat_model(\"claude-sonnet-4-5-20250929\"), } @wrap_model_call def store_based_model( request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse] ) -> ModelResponse: \"\"\"Select model based on Store preferences.\"\"\" user_id = request.runtime.context.user_id # Read from Store: get user's preferred model store = request.runtime.store user_prefs = store.get((\"preferences\",), user_id) if user_prefs: preferred_model = user_prefs.value.get(\"preferred_model\") if preferred_model and preferred_model in MODEL_MAP: request = request.override(model=MODEL_MAP[preferred_model]) return handler(request) agent = create_agent( model=\"gpt-4o\", tools=[...], middleware=[store_based_model], context_schema=Context, store=InMemoryStore() ) ```   Select model based on cost limits or environment from Runtime Context: ```python theme={null} from dataclasses import dataclass from langchain.agents import create_agent from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse from langchain.chat_models import init_chat_model from typing import Callable @dataclass class Context: cost_tier: str environment: str # Initialize models once outside the middleware premium_model = init_chat_model(\"claude-sonnet-4-5-20250929\") standard_model = init_chat_model(\"gpt-4o\") budget_model = init_chat_model(\"gpt-4o-mini\") @wrap_model_call def context_based_model( request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse] ) -> ModelResponse: \"\"\"Select model based on Runtime Context.\"\"\" # Read from Runtime Context: cost tier and environment cost_tier = request.runtime.context.cost_tier environment = request.runtime.context.environment if environment == \"production\" and cost_tier == \"premium\": # Production premium users get best model model = premium_model elif cost_tier == \"budget\": # Budget tier gets efficient model model = budget_model else: # Standard tier model = standard_model request = request.override(model=model) return handler(request) agent = create_agent( model=\"gpt-4o\", tools=[...], middleware=[context_based_model], context_schema=Context ) ``` \n\nSee [Dynamic model](/oss/python/langchain/agents#dynamic-model) for more examples.\n\n### Response Format\n\nStructured output transforms unstructured text into validated, structured data. When extracting specific fields or returning data for downstream systems, free-form text isn't sufficient.\n\n**How it works:** When you provide a schema as the response format, the model's final response is guaranteed to conform to that schema. The agent runs the model / tool calling loop until the model is done calling tools, then the final response is coerced into the provided format.\n\n#### Defining formats\n\nSchema definitions guide the model. Field names, types, and descriptions specify exactly what format the output should adhere to.\n\n```python  theme={null}\nfrom pydantic import BaseModel, Field\n\nclass CustomerSupportTicket(BaseModel):\n    \"\"\"Structured ticket information extracted from customer message.\"\"\"\n\n    category: str = Field(\n        description=\"Issue category: 'billing', 'technical', 'account', or 'product'\"\n    )\n    priority: str = Field(\n        description=\"Urgency level: 'low', 'medium', 'high', or 'critical'\"\n    )\n    summary: str = Field(\n        description=\"One-sentence summary of the customer's issue\"\n    )\n    customer_sentiment: str = Field(\n        description=\"Customer's emotional tone: 'frustrated', 'neutral', or 'satisfied'\"\n    )\n```\n\n#### Selecting formats\n\nDynamic response format selection adapts schemas based on user preferences, conversation stage, or role\u2014returning simple formats early and detailed formats as complexity increases.\n\n  Configure structured output based on conversation state: ```python theme={null} from langchain.agents import create_agent from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse from pydantic import BaseModel, Field from typing import Callable class SimpleResponse(BaseModel): \"\"\"Simple response for early conversation.\"\"\" answer: str = Field(description=\"A brief answer\") class DetailedResponse(BaseModel): \"\"\"Detailed response for established conversation.\"\"\" answer: str = Field(description=\"A detailed answer\") reasoning: str = Field(description=\"Explanation of reasoning\") confidence: float = Field(description=\"Confidence score 0-1\") @wrap_model_call def state_based_output( request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse] ) -> ModelResponse: \"\"\"Select output format based on State.\"\"\" # request.messages is a shortcut for request.state[\"messages\"] message_count = len(request.messages) # [!code highlight] if message_count < 3: # Early conversation - use simple format request = request.override(response_format=SimpleResponse) # [!code highlight] else: # Established conversation - use detailed format request = request.override(response_format=DetailedResponse) # [!code highlight] return handler(request) agent = create_agent( model=\"gpt-4o\", tools=[...], middleware=[state_based_output] ) ```   Configure output format based on user preferences in Store: ```python theme={null} from dataclasses import dataclass from langchain.agents import create_agent from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse from pydantic import BaseModel, Field from typing import Callable from langgraph.store.memory import InMemoryStore @dataclass class Context: user_id: str class VerboseResponse(BaseModel): \"\"\"Verbose response with details.\"\"\" answer: str = Field(description=\"Detailed answer\") sources: list[str] = Field(description=\"Sources used\") class ConciseResponse(BaseModel): \"\"\"Concise response.\"\"\" answer: str = Field(description=\"Brief answer\") @wrap_model_call def store_based_output( request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse] ) -> ModelResponse: \"\"\"Select output format based on Store preferences.\"\"\" user_id = request.runtime.context.user_id # Read from Store: get user's preferred response style store = request.runtime.store user_prefs = store.get((\"preferences\",), user_id) if user_prefs: style = user_prefs.value.get(\"response_style\", \"concise\") if style == \"verbose\": request = request.override(response_format=VerboseResponse) else: request = request.override(response_format=ConciseResponse) return handler(request) agent = create_agent( model=\"gpt-4o\", tools=[...], middleware=[store_based_output], context_schema=Context, store=InMemoryStore() ) ```   Configure output format based on Runtime Context like user role or environment: ```python theme={null} from dataclasses import dataclass from langchain.agents import create_agent from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse from pydantic import BaseModel, Field from typing import Callable @dataclass class Context: user_role: str environment: str class AdminResponse(BaseModel): \"\"\"Response with technical details for admins.\"\"\" answer: str = Field(description=\"Answer\") debug_info: dict = Field(description=\"Debug information\") system_status: str = Field(description=\"System status\") class UserResponse(BaseModel): \"\"\"Simple response for regular users.\"\"\" answer: str = Field(description=\"Answer\") @wrap_model_call def context_based_output( request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse] ) -> ModelResponse: \"\"\"Select output format based on Runtime Context.\"\"\" # Read from Runtime Context: user role and environment user_role = request.runtime.context.user_role environment = request.runtime.context.environment if user_role == \"admin\" and environment == \"production\": # Admins in production get detailed output request = request.override(response_format=AdminResponse) else: # Regular users get simple output request = request.override(response_format=UserResponse) return handler(request) agent = create_agent( model=\"gpt-4o\", tools=[...], middleware=[context_based_output], context_schema=Context ) ``` \n\n## Tool Context\n\nTools are special in that they both read and write context.\n\nIn the most basic case, when a tool executes, it receives the LLM's request parameters and returns a tool message back. The tool does its work and produces a result.\n\nTools can also fetch important information for the model that allows it to perform and complete tasks.\n\n### Reads\n\nMost real-world tools need more than just the LLM's parameters. They need user IDs for database queries, API keys for external services, or current session state to make decisions. Tools read from state, store, and runtime context to access this information.\n\n  Read from State to check current session information: ```python theme={null} from langchain.tools import tool, ToolRuntime from langchain.agents import create_agent @tool def check_authentication( runtime: ToolRuntime ) -> str: \"\"\"Check if user is authenticated.\"\"\" # Read from State: check current auth status current_state = runtime.state is_authenticated = current_state.get(\"authenticated\", False) if is_authenticated: return \"User is authenticated\" else: return \"User is not authenticated\" agent = create_agent( model=\"gpt-4o\", tools=[check_authentication] ) ```   Read from Store to access persisted user preferences: ```python theme={null} from dataclasses import dataclass from langchain.tools import tool, ToolRuntime from langchain.agents import create_agent from langgraph.store.memory import InMemoryStore @dataclass class Context: user_id: str @tool def get_preference( preference_key: str, runtime: ToolRuntime[Context] ) -> str: \"\"\"Get user preference from Store.\"\"\" user_id = runtime.context.user_id # Read from Store: get existing preferences store = runtime.store existing_prefs = store.get((\"preferences\",), user_id) if existing_prefs: value = existing_prefs.value.get(preference_key) return f\"{preference_key}: {value}\" if value else f\"No preference set for {preference_key}\" else: return \"No preferences found\" agent = create_agent( model=\"gpt-4o\", tools=[get_preference], context_schema=Context, store=InMemoryStore() ) ```   Read from Runtime Context for configuration like API keys and user IDs: ```python theme={null} from dataclasses import dataclass from langchain.tools import tool, ToolRuntime from langchain.agents import create_agent @dataclass class Context: user_id: str api_key: str db_connection: str @tool def fetch_user_data( query: str, runtime: ToolRuntime[Context] ) -> str: \"\"\"Fetch data using Runtime Context configuration.\"\"\" # Read from Runtime Context: get API key and DB connection user_id = runtime.context.user_id api_key = runtime.context.api_key db_connection = runtime.context.db_connection # Use configuration to fetch data results = perform_database_query(db_connection, query, api_key) return f\"Found {len(results)} results for user {user_id}\" agent = create_agent( model=\"gpt-4o\", tools=[fetch_user_data], context_schema=Context ) # Invoke with runtime context result = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"Get my data\"}]}, context=Context( user_id=\"user_123\", api_key=\"sk-...\", db_connection=\"postgresql://...\" ) ) ``` \n\n### Writes\n\nTool results can be used to help an agent complete a given task. Tools can both return results directly to the model\nand update the memory of the agent to make important context available to future steps.\n\n  Write to State to track session-specific information using Command: ```python theme={null} from langchain.tools import tool, ToolRuntime from langchain.agents import create_agent from langgraph.types import Command @tool def authenticate_user( password: str, runtime: ToolRuntime ) -> Command: \"\"\"Authenticate user and update State.\"\"\" # Perform authentication (simplified) if password == \"correct\": # Write to State: mark as authenticated using Command return Command( update={\"authenticated\": True}, ) else: return Command(update={\"authenticated\": False}) agent = create_agent( model=\"gpt-4o\", tools=[authenticate_user] ) ```   Write to Store to persist data across sessions: ```python theme={null} from dataclasses import dataclass from langchain.tools import tool, ToolRuntime from langchain.agents import create_agent from langgraph.store.memory import InMemoryStore @dataclass class Context: user_id: str @tool def save_preference( preference_key: str, preference_value: str, runtime: ToolRuntime[Context] ) -> str: \"\"\"Save user preference to Store.\"\"\" user_id = runtime.context.user_id # Read existing preferences store = runtime.store existing_prefs = store.get((\"preferences\",), user_id) # Merge with new preference prefs = existing_prefs.value if existing_prefs else {} prefs[preference_key] = preference_value # Write to Store: save updated preferences store.put((\"preferences\",), user_id, prefs) return f\"Saved preference: {preference_key} = {preference_value}\" agent = create_agent( model=\"gpt-4o\", tools=[save_preference], context_schema=Context, store=InMemoryStore() ) ``` \n\nSee [Tools](/oss/python/langchain/tools) for comprehensive examples of accessing state, store, and runtime context in tools.\n\n## Life-cycle Context\n\nControl what happens **between** the core agent steps - intercepting data flow to implement cross-cutting concerns like summarization, guardrails, and logging.\n\nAs you've seen in [Model Context](#model-context) and [Tool Context](#tool-context), [middleware](/oss/python/langchain/middleware) is the mechanism that makes context engineering practical. Middleware allows you to hook into any step in the agent lifecycle and either:\n\n1. **Update context** - Modify state and store to persist changes, update conversation history, or save insights\n2. **Jump in the lifecycle** - Move to different steps in the agent cycle based on context (e.g., skip tool execution if a condition is met, repeat model call with modified context)\n\n ![Image 2: Middleware hooks in the agent loop](https://mintcdn.com/langchain-5e9cc07a/RAP6mjwE5G00xYsA/oss/images/middleware_final.png?fit=max&auto=format&n=RAP6mjwE5G00xYsA&q=85&s=eb4404b137edec6f6f0c8ccb8323eaf1)\n\n### Example: Summarization\n\nOne of the most common life-cycle patterns is automatically condensing conversation history when it gets too long. Unlike the transient message trimming shown in [Model Context](#messages), summarization **persistently updates state** - permanently replacing old messages with a summary that's saved for all future turns.\n\nLangChain offers built-in middleware for this:\n\n```python  theme={null}\nfrom langchain.agents import create_agent\nfrom langchain.agents.middleware import SummarizationMiddleware\n\nagent = create_agent(\n    model=\"gpt-4o\",\n    tools=[...],\n    middleware=[\n        SummarizationMiddleware(\n            model=\"gpt-4o-mini\",\n            trigger={\"tokens\": 4000},\n            keep={\"messages\": 20},\n        ),\n    ],\n)\n```\n\nWhen the conversation exceeds the token limit, `SummarizationMiddleware` automatically:\n\n1. Summarizes older messages using a separate LLM call\n2. Replaces them with a summary message in State (permanently)\n3. Keeps recent messages intact for context\n\nThe summarized conversation history is permanently updated - future turns will see the summary instead of the original messages.\n\n For a complete list of built-in middleware, available hooks, and how to create custom middleware, see the [Middleware documentation](/oss/python/langchain/middleware). \n\n## Best practices\n\n1. **Start simple** - Begin with static prompts and tools, add dynamics only when needed\n2. **Test incrementally** - Add one context engineering feature at a time\n3. **Monitor performance** - Track model calls, token usage, and latency\n4. **Use built-in middleware** - Leverage [`SummarizationMiddleware`](/oss/python/langchain/middleware#summarization), [`LLMToolSelectorMiddleware`](/oss/python/langchain/middleware#llm-tool-selector), etc.\n5. **Document your context strategy** - Make it clear what context is being passed and why\n6. **Understand transient vs persistent**: Model context changes are transient (per-call), while life-cycle context changes persist to state\n\n## Related resources\n\n* [Context conceptual overview](/oss/python/concepts/context) - Understand context types and when to use them\n* [Middleware](/oss/python/langchain/middleware) - Complete middleware guide\n* [Tools](/oss/python/langchain/tools) - Tool creation and context access\n* [Memory](/oss/python/concepts/memory) - Short-term and long-term memory patterns\n* [Agents](/oss/python/langchain/agents) - Core agent concepts\n\n***\n\n [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/context-engineering.mdx) \n\n [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/deploy.md",
      "title": null,
      "raw_content": "# LangSmith Deployment When you're ready to deploy your LangChain agent to production, LangSmith provides a managed hosting platform designed for agent workloads. Traditional hosting platforms are built for stateless, short-lived web applications, while LangGraph is \\*\\*purpose-built for stateful, long-running agents\\*\\* that require persistent state and background execution. LangSmith handles the infrastructure, scaling, and operational concerns so you can deploy directly from your repository. ## Prerequisites Before you begin, ensure you have the following: \\* A [GitHub account](https://github.com/) \\* A [LangSmith account](https://smith.langchain.com/) (free to sign up) ## Deploy your agent ### 1. Create a repository on GitHub Your application's code must reside in a GitHub repository to be deployed on LangSmith. Both public and private repositories are supported. For this quickstart, first make sure your app is LangGraph-compatible by following the [local server setup guide](/oss/python/langchain/studio#setup-local-agent-server). Then, push your code to the repository. ### 2. Deploy to LangSmith   Log in to [LangSmith](https://smith.langchain.com/). In the left sidebar, select \\*\\*Deployments\\*\\*.   Click the \\*\\*+ New Deployment\\*\\* button. A pane will open where you can fill in the required fields.   If you are a first time user or adding a private repository that has not been previously connected, click the \\*\\*Add new account\\*\\* button and follow the instructions to connect your GitHub account.   Select your application's repository. Click \\*\\*Submit\\*\\* to deploy. This may take about 15 minutes to complete. You can check the status in the \\*\\*Deployment details\\*\\* view.   ### 3. Test your application in Studio Once your application is deployed: 1. Select the deployment you just created to view more details. 2. Click the \\*\\*Studio\\*\\* button in the top right corner. Studio will open to display your graph. ### 4. Get the API URL for your deployment 1. In the \\*\\*Deployment details\\*\\* view in LangGraph, click the \\*\\*API URL\\*\\* to copy it to your clipboard. 2. Click the `URL` to copy it to the clipboard. ### 5. Test the API You can now test the API:   1. Install LangGraph Python: ```shell theme={null} pip install langgraph-sdk ``` 2. Send a message to the agent: ```python theme={null} from langgraph\\_sdk import get\\_sync\\_client # or get\\_client for async client = get\\_sync\\_client(url=\"your-deployment-url\", api\\_key=\"your-langsmith-api-key\") for chunk in client.runs.stream( None, # Threadless run \"agent\", # Name of agent. Defined in langgraph.json. input={ \"messages\": [{ \"role\": \"human\", \"content\": \"What is LangGraph?\", }], }, stream\\_mode=\"updates\", ): print(f\"Receiving new event of type: {chunk.event}...\") print(chunk.data) print(\"\\n\\n\") ```   ```bash theme={null} curl -s --request POST \\ --url /runs/stream \\ --header 'Content-Type: application/json' \\ --header \"X-Api-Key:  \\ --data \"{ \\\"assistant\\_id\\\": \\\"agent\\\", `# Name of agent. Defined in langgraph.json.` \\\"input\\\": { \\\"messages\\\": [ { \\\"role\\\": \\\"human\\\", \\\"content\\\": \\\"What is LangGraph?\\\" } ] }, \\\"stream\\_mode\\\": \\\"updates\\\" }\" ```    LangSmith offers additional hosting options, including self-hosted and hybrid. For more information, please see the [Platform setup overview](/langsmith/platform-setup).  \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/deploy.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/get-help.md",
      "title": null,
      "raw_content": "# Get help Connect with the LangChain community, access learning resources, and get the support you need to build with confidence. ## Learning resources Start your journey or deepen your knowledge with our comprehensive learning materials. \\* \\*\\*[Chat LangChain](https://chat.langchain.com/)\\*\\*: Ask the docs anything about LangChain, powered by real-time docs \\* \\*\\*[API Reference](https://reference.langchain.com/python/)\\*\\*: Complete documentation for all LangChain packages ## Community support Get help from fellow developers and the LangChain team through our active community channels. \\* \\*\\*[Community Forum](https://forum.langchain.com/)\\*\\*: Ask questions, share solutions, and discuss best practices \\* \\*\\*[Community Slack](https://www.langchain.com/join-community)\\*\\*: Connect with other builders and get quick help ## Professional support For enterprise needs and critical applications, access dedicated support channels. \\* \\*\\*[Support portal](https://support.langchain.com/)\\*\\*: Submit tickets and track support requests \\* \\*\\*[LangSmith status](https://status.smith.langchain.com/)\\*\\*: Real-time status of LangSmith services and APIs ## Contribute Help us improve LangChain for everyone. Whether you're fixing bugs, adding features, or improving documentation, we welcome your contributions. \\* \\*\\*[Contributing Guide](/oss/python/contributing/overview)\\*\\*: Everything you need to know about contributing to LangChain ## Stay connected Follow us for the latest updates, announcements, and community highlights. \\* \\*\\*[X (Twitter)](https://twitter.com/langchainai)\\*\\*: Daily updates and community spotlights \\* \\*\\*[LinkedIn](https://www.linkedin.com/company/langchain/)\\*\\*: Professional network and company updates \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/get-help.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/guardrails.md",
      "title": null,
      "raw_content": "# Guardrails > Implement safety checks and content filtering for your agents Guardrails help you build safe, compliant AI applications by validating and filtering content at key points in your agent's execution. They can detect sensitive information, enforce content policies, validate outputs, and prevent unsafe behaviors before they cause problems. Common use cases include: \\* Preventing PII leakage \\* Detecting and blocking prompt injection attacks \\* Blocking inappropriate or harmful content \\* Enforcing business rules and compliance requirements \\* Validating output quality and accuracy You can implement guardrails using [middleware](/oss/python/langchain/middleware) to intercept execution at strategic points - before the agent starts, after it completes, or around model and tool calls.Guardrails can be implemented using two complementary approaches:   Use rule-based logic like regex patterns, keyword matching, or explicit checks. Fast, predictable, and cost-effective, but may miss nuanced violations.   Use LLMs or classifiers to evaluate content with semantic understanding. Catch subtle issues that rules miss, but are slower and more expensive.   LangChain provides both built-in guardrails (e.g., [PII detection](#pii-detection), [human-in-the-loop](#human-in-the-loop)) and a flexible middleware system for building custom guardrails using either approach. ## Built-in guardrails ### PII detection LangChain provides built-in middleware for detecting and handling Personally Identifiable Information (PII) in conversations. This middleware can detect common PII types like emails, credit cards, IP addresses, and more. PII detection middleware is helpful for cases such as health care and financial applications with compliance requirements, customer service agents that need to sanitize logs, and generally any application handling sensitive user data. The PII middleware supports multiple strategies for handling detected PII: | Strategy | Description | Example | | -------- | --------------------------------------- | --------------------- | | `redact` | Replace with `[REDACTED\\_TYPE]` | `[REDACTED\\_EMAIL]` | | `mask` | Partially obscure (e.g., last 4 digits) | `\\*\\*\\*\\*-\\*\\*\\*\\*-\\*\\*\\*\\*-1234` | | `hash` | Replace with deterministic hash | `a8f5f167...` | | `block` | Raise exception when detected | Error thrown | ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import PIIMiddleware agent = create\\_agent( model=\"gpt-4o\", tools=[customer\\_service\\_tool, email\\_tool], middleware=[ # Redact emails in user input before sending to model PIIMiddleware( \"email\", strategy=\"redact\", apply\\_to\\_input=True, ), # Mask credit cards in user input PIIMiddleware( \"credit\\_card\", strategy=\"mask\", apply\\_to\\_input=True, ), # Block API keys - raise error if detected PIIMiddleware( \"api\\_key\", detector=r\"sk-[a-zA-Z0-9]{32}\", strategy=\"block\", apply\\_to\\_input=True, ), ], ) # When user provides PII, it will be handled according to the strategy result = agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"My email is john.doe@example.com and card is 5105-1051-0510-5100\"}] }) ```  \\*\\*Built-in PII types:\\*\\* \\* `email` - Email addresses \\* `credit\\_card` - Credit card numbers (Luhn validated) \\* `ip` - IP addresses \\* `mac\\_address` - MAC addresses \\* `url` - URLs \\*\\*Configuration options:\\*\\* | Parameter | Description | Default | | ----------------------- | ---------------------------------------------------------------------- | ---------------------- | | `pii\\_type` | Type of PII to detect (built-in or custom) | Required | | `strategy` | How to handle detected PII (`\"block\"`, `\"redact\"`, `\"mask\"`, `\"hash\"`) | `\"redact\"` | | `detector` | Custom detector function or regex pattern | `None` (uses built-in) | | `apply\\_to\\_input` | Check user messages before model call | `True` | | `apply\\_to\\_output` | Check AI messages after model call | `False` | | `apply\\_to\\_tool\\_results` | Check tool result messages after execution | `False` |  See the [middleware documentation](/oss/python/langchain/middleware#pii-detection) for complete details on PII detection capabilities. ### Human-in-the-loop LangChain provides built-in middleware for requiring human approval before executing sensitive operations. This is one of the most effective guardrails for high-stakes decisions. Human-in-the-loop middleware is helpful for cases such as financial transactions and transfers, deleting or modifying production data, sending communications to external parties, and any operation with significant business impact. ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import HumanInTheLoopMiddleware from langgraph.checkpoint.memory import InMemorySaver from langgraph.types import Command agent = create\\_agent( model=\"gpt-4o\", tools=[search\\_tool, send\\_email\\_tool, delete\\_database\\_tool], middleware=[ HumanInTheLoopMiddleware( interrupt\\_on={ # Require approval for sensitive operations \"send\\_email\": True, \"delete\\_database\": True, # Auto-approve safe operations \"search\": False, } ), ], # Persist the state across interrupts checkpointer=InMemorySaver(), ) # Human-in-the-loop requires a thread ID for persistence config = {\"configurable\": {\"thread\\_id\": \"some\\_id\"}} # Agent will pause and wait for approval before executing sensitive tools result = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"Send an email to the team\"}]}, config=config ) result = agent.invoke( Command(resume={\"decisions\": [{\"type\": \"approve\"}]}), config=config # Same thread ID to resume the paused conversation ) ```  See the [human-in-the-loop documentation](/oss/python/langchain/human-in-the-loop) for complete details on implementing approval workflows.  ## Custom guardrails For more sophisticated guardrails, you can create custom middleware that runs before or after the agent executes. This gives you full control over validation logic, content filtering, and safety checks. ### Before agent guardrails Use \"before agent\" hooks to validate requests once at the start of each invocation. This is useful for session-level checks like authentication, rate limiting, or blocking inappropriate requests before any processing begins.  ```python title=\"Class syntax\" theme={null} from typing import Any from langchain.agents.middleware import AgentMiddleware, AgentState, hook\\_config from langgraph.runtime import Runtime class ContentFilterMiddleware(AgentMiddleware): \"\"\"Deterministic guardrail: Block requests containing banned keywords.\"\"\" def \\_\\_init\\_\\_(self, banned\\_keywords: list[str]): super().\\_\\_init\\_\\_() self.banned\\_keywords = [kw.lower() for kw in banned\\_keywords] @hook\\_config(can\\_jump\\_to=[\"end\"]) def before\\_agent(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None: # Get the first user message if not state[\"messages\"]: return None first\\_message = state[\"messages\"][0] if first\\_message.type != \"human\": return None content = first\\_message.content.lower() # Check for banned keywords for keyword in self.banned\\_keywords: if keyword in content: # Block execution before any processing return { \"messages\": [{ \"role\": \"assistant\", \"content\": \"I cannot process requests containing inappropriate content. Please rephrase your request.\" }], \"jump\\_to\": \"end\" } return None # Use the custom guardrail from langchain.agents import create\\_agent agent = create\\_agent( model=\"gpt-4o\", tools=[search\\_tool, calculator\\_tool], middleware=[ ContentFilterMiddleware( banned\\_keywords=[\"hack\", \"exploit\", \"malware\"] ), ], ) # This request will be blocked before any processing result = agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"How do I hack into a database?\"}] }) ``` ```python title=\"Decorator syntax\" theme={null} from typing import Any from langchain.agents.middleware import before\\_agent, AgentState, hook\\_config from langgraph.runtime import Runtime banned\\_keywords = [\"hack\", \"exploit\", \"malware\"] @before\\_agent(can\\_jump\\_to=[\"end\"]) def content\\_filter(state: AgentState, runtime: Runtime) -> dict[str, Any] | None: \"\"\"Deterministic guardrail: Block requests containing banned keywords.\"\"\" # Get the first user message if not state[\"messages\"]: return None first\\_message = state[\"messages\"][0] if first\\_message.type != \"human\": return None content = first\\_message.content.lower() # Check for banned keywords for keyword in banned\\_keywords: if keyword in content: # Block execution before any processing return { \"messages\": [{ \"role\": \"assistant\", \"content\": \"I cannot process requests containing inappropriate content. Please rephrase your request.\" }], \"jump\\_to\": \"end\" } return None # Use the custom guardrail from langchain.agents import create\\_agent agent = create\\_agent( model=\"gpt-4o\", tools=[search\\_tool, calculator\\_tool], middleware=[content\\_filter], ) # This request will be blocked before any processing result = agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"How do I hack into a database?\"}] }) ```  ### After agent guardrails Use \"after agent\" hooks to validate final outputs once before returning to the user. This is useful for model-based safety checks, quality validation, or final compliance scans on the complete agent response.  ```python title=\"Class syntax\" theme={null} from langchain.agents.middleware import AgentMiddleware, AgentState, hook\\_config from langgraph.runtime import Runtime from langchain.messages import AIMessage from langchain.chat\\_models import init\\_chat\\_model from typing import Any class SafetyGuardrailMiddleware(AgentMiddleware): \"\"\"Model-based guardrail: Use an LLM to evaluate response safety.\"\"\" def \\_\\_init\\_\\_(self): super().\\_\\_init\\_\\_() self.safety\\_model = init\\_chat\\_model(\"gpt-4o-mini\") @hook\\_config(can\\_jump\\_to=[\"end\"]) def after\\_agent(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None: # Get the final AI response if not state[\"messages\"]: return None last\\_message = state[\"messages\"][-1] if not isinstance(last\\_message, AIMessage): return None # Use a model to evaluate safety safety\\_prompt = f\"\"\"Evaluate if this response is safe and appropriate. Respond with only 'SAFE' or 'UNSAFE'. Response: {last\\_message.content}\"\"\" result = self.safety\\_model.invoke([{\"role\": \"user\", \"content\": safety\\_prompt}]) if \"UNSAFE\" in result.content: last\\_message.content = \"I cannot provide that response. Please rephrase your request.\" return None # Use the safety guardrail from langchain.agents import create\\_agent agent = create\\_agent( model=\"gpt-4o\", tools=[search\\_tool, calculator\\_tool], middleware=[SafetyGuardrailMiddleware()], ) result = agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"How do I make explosives?\"}] }) ``` ```python title=\"Decorator syntax\" theme={null} from langchain.agents.middleware import after\\_agent, AgentState, hook\\_config from langgraph.runtime import Runtime from langchain.messages import AIMessage from langchain.chat\\_models import init\\_chat\\_model from typing import Any safety\\_model = init\\_chat\\_model(\"gpt-4o-mini\") @after\\_agent(can\\_jump\\_to=[\"end\"]) def safety\\_guardrail(state: AgentState, runtime: Runtime) -> dict[str, Any] | None: \"\"\"Model-based guardrail: Use an LLM to evaluate response safety.\"\"\" # Get the final AI response if not state[\"messages\"]: return None last\\_message = state[\"messages\"][-1] if not isinstance(last\\_message, AIMessage): return None # Use a model to evaluate safety safety\\_prompt = f\"\"\"Evaluate if this response is safe and appropriate. Respond with only 'SAFE' or 'UNSAFE'. Response: {last\\_message.content}\"\"\" result = safety\\_model.invoke([{\"role\": \"user\", \"content\": safety\\_prompt}]) if \"UNSAFE\" in result.content: last\\_message.content = \"I cannot provide that response. Please rephrase your request.\" return None # Use the safety guardrail from langchain.agents import create\\_agent agent = create\\_agent( model=\"gpt-4o\", tools=[search\\_tool, calculator\\_tool], middleware=[safety\\_guardrail], ) result = agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"How do I make explosives?\"}] }) ```  ### Combine multiple guardrails You can stack multiple guardrails by adding them to the middleware array. They execute in order, allowing you to build layered protection: ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import PIIMiddleware, HumanInTheLoopMiddleware agent = create\\_agent( model=\"gpt-4o\", tools=[search\\_tool, send\\_email\\_tool], middleware=[ # Layer 1: Deterministic input filter (before agent) ContentFilterMiddleware(banned\\_keywords=[\"hack\", \"exploit\"]), # Layer 2: PII protection (before and after model) PIIMiddleware(\"email\", strategy=\"redact\", apply\\_to\\_input=True), PIIMiddleware(\"email\", strategy=\"redact\", apply\\_to\\_output=True), # Layer 3: Human approval for sensitive tools HumanInTheLoopMiddleware(interrupt\\_on={\"send\\_email\": True}), # Layer 4: Model-based safety check (after agent) SafetyGuardrailMiddleware(), ], ) ``` ## Additional resources \\* [Middleware documentation](/oss/python/langchain/middleware) - Complete guide to custom middleware \\* [Middleware API reference](https://reference.langchain.com/python/langchain/middleware/) - Complete guide to custom middleware \\* [Human-in-the-loop](/oss/python/langchain/human-in-the-loop) - Add human review for sensitive operations \\* [Testing agents](/oss/python/langchain/test) - Strategies for testing safety mechanisms \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/guardrails.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/human-in-the-loop.md",
      "title": null,
      "raw_content": "# Human-in-the-loop The Human-in-the-Loop (HITL) [middleware](/oss/python/langchain/middleware/built-in#human-in-the-loop) lets you add human oversight to agent tool calls. When a model proposes an action that might require review \u2014 for example, writing to a file or executing SQL \u2014 the middleware can pause execution and wait for a decision. It does this by checking each tool call against a configurable policy. If intervention is needed, the middleware issues an [interrupt](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) that halts execution. The graph state is saved using LangGraph's [persistence layer](/oss/python/langgraph/persistence), so execution can pause safely and resume later. A human decision then determines what happens next: the action can be approved as-is (`approve`), modified before running (`edit`), or rejected with feedback (`reject`). ## Interrupt decision types The [middleware](/oss/python/langchain/middleware/built-in#human-in-the-loop) defines three built-in ways a human can respond to an interrupt: | Decision Type | Description | Example Use Case | | ------------- | ------------------------------------------------------------------------- | --------------------------------------------------- | | \u2705 `approve` | The action is approved as-is and executed without changes. | Send an email draft exactly as written | | \u270f\ufe0f `edit` | The tool call is executed with modifications. | Change the recipient before sending an email | | \u274c `reject` | The tool call is rejected, with an explanation added to the conversation. | Reject an email draft and explain how to rewrite it | The available decision types for each tool depend on the policy you configure in `interrupt\\_on`. When multiple tool calls are paused at the same time, each action requires a separate decision. Decisions must be provided in the same order as the actions appear in the interrupt request.  When \\*\\*editing\\*\\* tool arguments, make changes conservatively. Significant modifications to the original arguments may cause the model to re-evaluate its approach and potentially execute the tool multiple times or take unexpected actions.  ## Configuring interrupts To use HITL, add the [middleware](/oss/python/langchain/middleware/built-in#human-in-the-loop) to the agent's `middleware` list when creating the agent. You configure it with a mapping of tool actions to the decision types that are allowed for each action. The middleware will interrupt execution when a tool call matches an action in the mapping. ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import HumanInTheLoopMiddleware # [!code highlight] from langgraph.checkpoint.memory import InMemorySaver # [!code highlight] agent = create\\_agent( model=\"gpt-4o\", tools=[write\\_file\\_tool, execute\\_sql\\_tool, read\\_data\\_tool], middleware=[ HumanInTheLoopMiddleware( # [!code highlight] interrupt\\_on={ \"write\\_file\": True, # All decisions (approve, edit, reject) allowed \"execute\\_sql\": {\"allowed\\_decisions\": [\"approve\", \"reject\"]}, # No editing allowed # Safe operation, no approval needed \"read\\_data\": False, }, # Prefix for interrupt messages - combined with tool name and args to form the full message # e.g., \"Tool execution pending approval: execute\\_sql with query='DELETE FROM...'\" # Individual tools can override this by specifying a \"description\" in their interrupt config description\\_prefix=\"Tool execution pending approval\", ), ], # Human-in-the-loop requires checkpointing to handle interrupts. # In production, use a persistent checkpointer like AsyncPostgresSaver. checkpointer=InMemorySaver(), # [!code highlight] ) ```  You must configure a checkpointer to persist the graph state across interrupts. In production, use a persistent checkpointer like [`AsyncPostgresSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.postgres.aio.AsyncPostgresSaver). For testing or prototyping, use [`InMemorySaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.memory.InMemorySaver). When invoking the agent, pass a `config` that includes the \\*\\*thread ID\\*\\* to associate execution with a conversation thread. See the [LangGraph interrupts documentation](/oss/python/langgraph/interrupts) for details.    Mapping of tool names to approval configs. Values can be `True` (interrupt with default config), `False` (auto-approve), or an `InterruptOnConfig` object.   Prefix for action request descriptions  \\*\\*`InterruptOnConfig` options:\\*\\*  List of allowed decisions: `'approve'`, `'edit'`, or `'reject'`   Static string or callable function for custom description   ## Responding to interrupts When you invoke the agent, it runs until it either completes or an interrupt is raised. An interrupt is triggered when a tool call matches the policy you configured in `interrupt\\_on`. In that case, the invocation result will include an `\\_\\_interrupt\\_\\_` field with the actions that require review. You can then present those actions to a reviewer and resume execution once decisions are provided. ```python theme={null} from langgraph.types import Command # Human-in-the-loop leverages LangGraph's persistence layer. # You must provide a thread ID to associate the execution with a conversation thread, # so the conversation can be paused and resumed (as is needed for human review). config = {\"configurable\": {\"thread\\_id\": \"some\\_id\"}} # [!code highlight] # Run the graph until the interrupt is hit. result = agent.invoke( { \"messages\": [ { \"role\": \"user\", \"content\": \"Delete old records from the database\", } ] }, config=config # [!code highlight] ) # The interrupt contains the full HITL request with action\\_requests and review\\_configs print(result['\\_\\_interrupt\\_\\_']) # > [ # > Interrupt( # > value={ # > 'action\\_requests': [ # > { # > 'name': 'execute\\_sql', # > 'arguments': {'query': 'DELETE FROM records WHERE created\\_at < NOW() - INTERVAL \\'30 days\\';'}, # > 'description': 'Tool execution pending approval\\n\\nTool: execute\\_sql\\nArgs: {...}' # > } # > ], # > 'review\\_configs': [ # > { # > 'action\\_name': 'execute\\_sql', # > 'allowed\\_decisions': ['approve', 'reject'] # > } # > ] # > } # > ) # > ] # Resume with approval decision agent.invoke( Command( # [!code highlight] resume={\"decisions\": [{\"type\": \"approve\"}]} # or \"edit\", \"reject\" [!code highlight] ), # [!code highlight] config=config # Same thread ID to resume the paused conversation ) ``` ### Decision types   Use `approve` to approve the tool call as-is and execute it without changes. ```python theme={null} agent.invoke( Command( # Decisions are provided as a list, one per action under review. # The order of decisions must match the order of actions # listed in the `\\_\\_interrupt\\_\\_` request. resume={ \"decisions\": [ { \"type\": \"approve\", } ] } ), config=config # Same thread ID to resume the paused conversation ) ```   Use `edit` to modify the tool call before execution. Provide the edited action with the new tool name and arguments. ```python theme={null} agent.invoke( Command( # Decisions are provided as a list, one per action under review. # The order of decisions must match the order of actions # listed in the `\\_\\_interrupt\\_\\_` request. resume={ \"decisions\": [ { \"type\": \"edit\", # Edited action with tool name and args \"edited\\_action\": { # Tool name to call. # Will usually be the same as the original action. \"name\": \"new\\_tool\\_name\", # Arguments to pass to the tool. \"args\": {\"key1\": \"new\\_value\", \"key2\": \"original\\_value\"}, } } ] } ), config=config # Same thread ID to resume the paused conversation ) ```  When \\*\\*editing\\*\\* tool arguments, make changes conservatively. Significant modifications to the original arguments may cause the model to re-evaluate its approach and potentially execute the tool multiple times or take unexpected actions.    Use `reject` to reject the tool call and provide feedback instead of execution. ```python theme={null} agent.invoke( Command( # Decisions are provided as a list, one per action under review. # The order of decisions must match the order of actions # listed in the `\\_\\_interrupt\\_\\_` request. resume={ \"decisions\": [ { \"type\": \"reject\", # An explanation about why the action was rejected \"message\": \"No, this is wrong because ..., instead do this ...\", } ] } ), config=config # Same thread ID to resume the paused conversation ) ``` The `message` is added to the conversation as feedback to help the agent understand why the action was rejected and what it should do instead. \\*\\*\\* ### Multiple decisions When multiple actions are under review, provide a decision for each action in the same order as they appear in the interrupt: ```python theme={null} { \"decisions\": [ {\"type\": \"approve\"}, { \"type\": \"edit\", \"edited\\_action\": { \"name\": \"tool\\_name\", \"args\": {\"param\": \"new\\_value\"} } }, { \"type\": \"reject\", \"message\": \"This action is not allowed\" } ] } ```   ## Execution lifecycle The middleware defines an `after\\_model` hook that runs after the model generates a response but before any tool calls are executed: 1. The agent invokes the model to generate a response. 2. The middleware inspects the response for tool calls. 3. If any calls require human input, the middleware builds a `HITLRequest` with `action\\_requests` and `review\\_configs` and calls [interrupt](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt). 4. The agent waits for human decisions. 5. Based on the `HITLResponse` decisions, the middleware executes approved or edited calls, synthesizes [ToolMessage](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage)'s for rejected calls, and resumes execution. ## Custom HITL logic For more specialized workflows, you can build custom HITL logic directly using the [interrupt](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) primitive and [middleware](/oss/python/langchain/middleware) abstraction. Review the [execution lifecycle](#execution-lifecycle) above to understand how to integrate interrupts into the agent's operation. \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/human-in-the-loop.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/install.md",
      "title": null,
      "raw_content": "# Install LangChain To install the LangChain package:  ```bash pip theme={null} pip install -U langchain # Requires Python 3.10+ ``` ```bash uv theme={null} uv add langchain # Requires Python 3.10+ ```  LangChain provides integrations to hundreds of LLMs and thousands of other integrations. These live in independent provider packages. For example:  ```bash pip theme={null} # Installing the OpenAI integration pip install -U langchain-openai # Installing the Anthropic integration pip install -U langchain-anthropic ``` ```bash uv theme={null} # Installing the OpenAI integration uv add langchain-openai # Installing the Anthropic integration uv add langchain-anthropic ```   See the [Integrations tab](/oss/python/integrations/providers/overview) for a full list of available integrations.  \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/install.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/knowledge-base.md",
      "title": null,
      "raw_content": "# Build a semantic search engine with LangChain ## Overview This tutorial will familiarize you with LangChain's [document loader](/oss/python/langchain/retrieval#document-loaders), [embedding](/oss/python/langchain/retrieval#embedding-models), and [vector store](/oss/python/langchain/retrieval#vector-store) abstractions. These abstractions are designed to support retrieval of data-- from (vector) databases and other sources -- for integration with LLM workflows. They are important for applications that fetch data to be reasoned over as part of model inference, as in the case of retrieval-augmented generation, or [RAG](/oss/python/langchain/retrieval). Here we will build a search engine over a PDF document. This will allow us to retrieve passages in the PDF that are similar to an input query. The guide also includes a minimal RAG implementation on top of the search engine. ### Concepts This guide focuses on retrieval of text data. We will cover the following concepts: \\* [Documents and document loaders](/oss/python/integrations/document\\_loaders); \\* [Text splitters](/oss/python/integrations/splitters); \\* [Embeddings](/oss/python/integrations/text\\_embedding); \\* [Vector stores](/oss/python/integrations/vectorstores) and [retrievers](/oss/python/integrations/retrievers). ## Setup ### Installation This tutorial requires the `langchain-community` and `pypdf` packages:  ```bash pip theme={null} pip install langchain-community pypdf ``` ```bash conda theme={null} conda install langchain-community pypdf -c conda-forge ```  For more details, see our [Installation guide](/oss/python/langchain/install). ### LangSmith Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more and more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com). After you sign up at the link above, make sure to set your environment variables to start logging traces: ```shell theme={null} export LANGSMITH\\_TRACING=\"true\" export LANGSMITH\\_API\\_KEY=\"...\" ``` Or, if in a notebook, you can set them with: ```python theme={null} import getpass import os os.environ[\"LANGSMITH\\_TRACING\"] = \"true\" os.environ[\"LANGSMITH\\_API\\_KEY\"] = getpass.getpass() ``` ## 1. Documents and Document Loaders LangChain implements a [Document](https://reference.langchain.com/python/langchain\\_core/documents/#langchain\\_core.documents.base.Document) abstraction, which is intended to represent a unit of text and associated metadata. It has three attributes: \\* `page\\_content`: a string representing the content; \\* `metadata`: a dict containing arbitrary metadata; \\* `id`: (optional) a string identifier for the document. The `metadata` attribute can capture information about the source of the document, its relationship to other documents, and other information. Note that an individual [`Document`](https://reference.langchain.com/python/langchain\\_core/documents/#langchain\\_core.documents.base.Document) object often represents a chunk of a larger document. We can generate sample documents when desired: ```python theme={null} from langchain\\_core.documents import Document documents = [ Document( page\\_content=\"Dogs are great companions, known for their loyalty and friendliness.\", metadata={\"source\": \"mammal-pets-doc\"}, ), Document( page\\_content=\"Cats are independent pets that often enjoy their own space.\", metadata={\"source\": \"mammal-pets-doc\"}, ), ] ``` However, the LangChain ecosystem implements [document loaders](/oss/python/langchain/retrieval#document-loaders) that [integrate with hundreds of common sources](/oss/python/integrations/document\\_loaders/). This makes it easy to incorporate data from these sources into your AI application. ### Loading documents Let's load a PDF into a sequence of [`Document`](https://reference.langchain.com/python/langchain\\_core/documents/#langchain\\_core.documents.base.Document) objects. [Here is a sample PDF](https://github.com/langchain-ai/langchain/blob/v0.3/docs/docs/example\\_data/nke-10k-2023.pdf) -- a 10-k filing for Nike from 2023. We can consult the LangChain documentation for [available PDF document loaders](/oss/python/integrations/document\\_loaders/#pdfs). ```python theme={null} from langchain\\_community.document\\_loaders import PyPDFLoader file\\_path = \"../example\\_data/nke-10k-2023.pdf\" loader = PyPDFLoader(file\\_path) docs = loader.load() print(len(docs)) ``` ```output theme={null} 107 ``` `PyPDFLoader` loads one [`Document`](https://reference.langchain.com/python/langchain\\_core/documents/#langchain\\_core.documents.base.Document) object per PDF page. For each, we can easily access: \\* The string content of the page; \\* Metadata containing the file name and page number. ```python theme={null} print(f\"{docs[0].page\\_content[:200]}\\n\") print(docs[0].metadata) ``` ```output theme={null} Table of Contents UNITED STATES SECURITIES AND EXCHANGE COMMISSION Washington, D.C. 20549 FORM 10-K (Mark One) \u2611 ANNUAL REPORT PURSUANT TO SECTION 13 OR 15(D) OF THE SECURITIES EXCHANGE ACT OF 1934 FO {'source': '../example\\_data/nke-10k-2023.pdf', 'page': 0} ``` ### Splitting For both information retrieval and downstream question-answering purposes, a page may be too coarse a representation. Our goal in the end will be to retrieve [`Document`](https://reference.langchain.com/python/langchain\\_core/documents/#langchain\\_core.documents.base.Document) objects that answer an input query, and further splitting our PDF will help ensure that the meanings of relevant portions of the document are not \"washed out\" by surrounding text. We can use [text splitters](/oss/python/langchain/retrieval#text\\_splitters) for this purpose. Here we will use a simple text splitter that partitions based on characters. We will split our documents into chunks of 1000 characters with 200 characters of overlap between chunks. The overlap helps mitigate the possibility of separating a statement from important context related to it. We use the `RecursiveCharacterTextSplitter`, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases. We set `add\\_start\\_index=True` so that the character index where each split Document starts within the initial Document is preserved as metadata attribute \u201cstart\\\\_index\u201d. ```python theme={null} from langchain\\_text\\_splitters import RecursiveCharacterTextSplitter text\\_splitter = RecursiveCharacterTextSplitter( chunk\\_size=1000, chunk\\_overlap=200, add\\_start\\_index=True ) all\\_splits = text\\_splitter.split\\_documents(docs) print(len(all\\_splits)) ``` ```output theme={null} 514 ``` ## 2. Embeddings Vector search is a common way to store and search over unstructured data (such as unstructured text). The idea is to store numeric vectors that are associated with the text. Given a query, we can [embed](/oss/python/langchain/retrieval#embedding\\_models) it as a vector of the same dimension and use vector similarity metrics (such as cosine similarity) to identify related text. LangChain supports embeddings from [dozens of providers](/oss/python/integrations/text\\_embedding/). These models specify how text should be converted into a numeric vector. Let's select a model:   ```shell theme={null} pip install -U \"langchain-openai\" ``` ```python theme={null} import getpass import os if not os.environ.get(\"OPENAI\\_API\\_KEY\"): os.environ[\"OPENAI\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \") from langchain\\_openai import OpenAIEmbeddings embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\") ```   ```shell theme={null} pip install -U \"langchain-openai\" ``` ```python theme={null} import getpass import os if not os.environ.get(\"AZURE\\_OPENAI\\_API\\_KEY\"): os.environ[\"AZURE\\_OPENAI\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for Azure: \") from langchain\\_openai import AzureOpenAIEmbeddings embeddings = AzureOpenAIEmbeddings( azure\\_endpoint=os.environ[\"AZURE\\_OPENAI\\_ENDPOINT\"], azure\\_deployment=os.environ[\"AZURE\\_OPENAI\\_DEPLOYMENT\\_NAME\"], openai\\_api\\_version=os.environ[\"AZURE\\_OPENAI\\_API\\_VERSION\"], ) ```   ```shell theme={null} pip install -qU langchain-google-genai ``` ```python theme={null} import getpass import os if not os.environ.get(\"GOOGLE\\_API\\_KEY\"): os.environ[\"GOOGLE\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \") from langchain\\_google\\_genai import GoogleGenerativeAIEmbeddings embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\") ```   ```shell theme={null} pip install -qU langchain-google-vertexai ``` ```python theme={null} from langchain\\_google\\_vertexai import VertexAIEmbeddings embeddings = VertexAIEmbeddings(model=\"text-embedding-005\") ```   ```shell theme={null} pip install -qU langchain-aws ``` ```python theme={null} from langchain\\_aws import BedrockEmbeddings embeddings = BedrockEmbeddings(model\\_id=\"amazon.titan-embed-text-v2:0\") ```   ```shell theme={null} pip install -qU langchain-huggingface ``` ```python theme={null} from langchain\\_huggingface import HuggingFaceEmbeddings embeddings = HuggingFaceEmbeddings(model\\_name=\"sentence-transformers/all-mpnet-base-v2\") ```   ```shell theme={null} pip install -qU langchain-ollama ``` ```python theme={null} from langchain\\_ollama import OllamaEmbeddings embeddings = OllamaEmbeddings(model=\"llama3\") ```   ```shell theme={null} pip install -qU langchain-cohere ``` ```python theme={null} import getpass import os if not os.environ.get(\"COHERE\\_API\\_KEY\"): os.environ[\"COHERE\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for Cohere: \") from langchain\\_cohere import CohereEmbeddings embeddings = CohereEmbeddings(model=\"embed-english-v3.0\") ```   ```shell theme={null} pip install -qU langchain-mistralai ``` ```python theme={null} import getpass import os if not os.environ.get(\"MISTRALAI\\_API\\_KEY\"): os.environ[\"MISTRALAI\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for MistralAI: \") from langchain\\_mistralai import MistralAIEmbeddings embeddings = MistralAIEmbeddings(model=\"mistral-embed\") ```   ```shell theme={null} pip install -qU langchain-nomic ``` ```python theme={null} import getpass import os if not os.environ.get(\"NOMIC\\_API\\_KEY\"): os.environ[\"NOMIC\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for Nomic: \") from langchain\\_nomic import NomicEmbeddings embeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\") ```   ```shell theme={null} pip install -qU langchain-nvidia-ai-endpoints ``` ```python theme={null} import getpass import os if not os.environ.get(\"NVIDIA\\_API\\_KEY\"): os.environ[\"NVIDIA\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \") from langchain\\_nvidia\\_ai\\_endpoints import NVIDIAEmbeddings embeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\") ```   ```shell theme={null} pip install -qU langchain-voyageai ``` ```python theme={null} import getpass import os if not os.environ.get(\"VOYAGE\\_API\\_KEY\"): os.environ[\"VOYAGE\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \") from langchain-voyageai import VoyageAIEmbeddings embeddings = VoyageAIEmbeddings(model=\"voyage-3\") ```   ```shell theme={null} pip install -qU langchain-ibm ``` ```python theme={null} import getpass import os if not os.environ.get(\"WATSONX\\_APIKEY\"): os.environ[\"WATSONX\\_APIKEY\"] = getpass.getpass(\"Enter API key for IBM watsonx: \") from langchain\\_ibm import WatsonxEmbeddings embeddings = WatsonxEmbeddings( model\\_id=\"ibm/slate-125m-english-rtrvr\", url=\"https://us-south.ml.cloud.ibm.com\", project\\_id=\"\", ) ```   ```shell theme={null} pip install -qU langchain-core ``` ```python theme={null} from langchain\\_core.embeddings import DeterministicFakeEmbedding embeddings = DeterministicFakeEmbedding(size=4096) ```   ```shell theme={null} pip install -qU langchain-isaacus ``` ```python theme={null} import getpass import os if not os.environ.get(\"ISAACUS\\_API\\_KEY\"): os.environ[\"ISAACUS\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for Isaacus: \") from langchain\\_isaacus import IsaacusEmbeddings embeddings = IsaacusEmbeddings(model=\"kanon-2-embedder\") ```   ```python theme={null} vector\\_1 = embeddings.embed\\_query(all\\_splits[0].page\\_content) vector\\_2 = embeddings.embed\\_query(all\\_splits[1].page\\_content) assert len(vector\\_1) == len(vector\\_2) print(f\"Generated vectors of length {len(vector\\_1)}\\n\") print(vector\\_1[:10]) ``` ```output theme={null} Generated vectors of length 1536 [-0.008586574345827103, -0.03341241180896759, -0.008936782367527485, -0.0036674530711025, 0.010564599186182022, 0.009598285891115665, -0.028587326407432556, -0.015824200585484505, 0.0030416189692914486, -0.012899317778646946] ``` Armed with a model for generating text embeddings, we can next store them in a special data structure that supports efficient similarity search. ## 3. Vector stores LangChain [VectorStore](https://reference.langchain.com/python/langchain\\_core/vectorstores/?h=#langchain\\_core.vectorstores.base.VectorStore) objects contain methods for adding text and [`Document`](https://reference.langchain.com/python/langchain\\_core/documents/#langchain\\_core.documents.base.Document) objects to the store, and querying them using various similarity metrics. They are often initialized with [embedding](/oss/python/langchain/retrieval#embedding\\_models) models, which determine how text data is translated to numeric vectors. LangChain includes a suite of [integrations](/oss/python/integrations/vectorstores) with different vector store technologies. Some vector stores are hosted by a provider (e.g., various cloud providers) and require specific credentials to use; some (such as [Postgres](/oss/python/integrations/vectorstores/pgvector)) run in separate infrastructure that can be run locally or via a third-party; others can run in-memory for lightweight workloads. Let's select a vector store:   ```shell theme={null} pip install -U \"langchain-core\" ``` ```python theme={null} from langchain\\_core.vectorstores import InMemoryVectorStore vector\\_store = InMemoryVectorStore(embeddings) ```   ```shell theme={null} pip install -U \"langchain-astradb\" ``` ```python theme={null} from langchain\\_astradb import AstraDBVectorStore vector\\_store = AstraDBVectorStore( embedding=embeddings, api\\_endpoint=ASTRA\\_DB\\_API\\_ENDPOINT, collection\\_name=\"astra\\_vector\\_langchain\", token=ASTRA\\_DB\\_APPLICATION\\_TOKEN, namespace=ASTRA\\_DB\\_NAMESPACE, ) ```   ```shell theme={null} pip install -qU langchain-chroma ``` ```python theme={null} from langchain\\_chroma import Chroma vector\\_store = Chroma( collection\\_name=\"example\\_collection\", embedding\\_function=embeddings, persist\\_directory=\"./chroma\\_langchain\\_db\", # Where to save data locally, remove if not necessary ) ```   ```shell theme={null} pip install -qU langchain-community faiss-cpu ``` ```python theme={null} import faiss from langchain\\_community.docstore.in\\_memory import InMemoryDocstore from langchain\\_community.vectorstores import FAISS embedding\\_dim = len(embeddings.embed\\_query(\"hello world\")) index = faiss.IndexFlatL2(embedding\\_dim) vector\\_store = FAISS( embedding\\_function=embeddings, index=index, docstore=InMemoryDocstore(), index\\_to\\_docstore\\_id={}, ) ```   ```shell theme={null} pip install -qU langchain-milvus ``` ```python theme={null} from langchain\\_milvus import Milvus URI = \"./milvus\\_example.db\" vector\\_store = Milvus( embedding\\_function=embeddings, connection\\_args={\"uri\": URI}, index\\_params={\"index\\_type\": \"FLAT\", \"metric\\_type\": \"L2\"}, ) ```   ```shell theme={null} pip install -qU langchain-mongodb ``` ```python theme={null} from langchain\\_mongodb import MongoDBAtlasVectorSearch vector\\_store = MongoDBAtlasVectorSearch( embedding=embeddings, collection=MONGODB\\_COLLECTION, index\\_name=ATLAS\\_VECTOR\\_SEARCH\\_INDEX\\_NAME, relevance\\_score\\_fn=\"cosine\", ) ```   ```shell theme={null} pip install -qU langchain-postgres ``` ```python theme={null} from langchain\\_postgres import PGVector vector\\_store = PGVector( embeddings=embeddings, collection\\_name=\"my\\_docs\", connection=\"postgresql+psycopg://...\", ) ```   ```shell theme={null} pip install -qU langchain-postgres ``` ```python theme={null} from langchain\\_postgres import PGEngine, PGVectorStore pg\\_engine = PGEngine.from\\_connection\\_string( url=\"postgresql+psycopg://...\" ) vector\\_store = PGVectorStore.create\\_sync( engine=pg\\_engine, table\\_name='test\\_table', embedding\\_service=embedding ) ```   ```shell theme={null} pip install -qU langchain-pinecone ``` ```python theme={null} from langchain\\_pinecone import PineconeVectorStore from pinecone import Pinecone pc = Pinecone(api\\_key=...) index = pc.Index(index\\_name) vector\\_store = PineconeVectorStore(embedding=embeddings, index=index) ```   ```shell theme={null} pip install -qU langchain-qdrant ``` ```python theme={null} from qdrant\\_client.models import Distance, VectorParams from langchain\\_qdrant import QdrantVectorStore from qdrant\\_client import QdrantClient client = QdrantClient(\":memory:\") vector\\_size = len(embeddings.embed\\_query(\"sample text\")) if not client.collection\\_exists(\"test\"): client.create\\_collection( collection\\_name=\"test\", vectors\\_config=VectorParams(size=vector\\_size, distance=Distance.COSINE) ) vector\\_store = QdrantVectorStore( client=client, collection\\_name=\"test\", embedding=embeddings, ) ```   Having instantiated our vector store, we can now index the documents. ```python theme={null} ids = vector\\_store.add\\_documents(documents=all\\_splits) ``` Note that most vector store implementations will allow you to connect to an existing vector store-- e.g., by providing a client, index name, or other information. See the documentation for a specific [integration](/oss/python/integrations/vectorstores) for more detail. Once we've instantiated a [`VectorStore`](https://reference.langchain.com/python/langchain\\_core/vectorstores/?h=#langchain\\_core.vectorstores.base.VectorStore) that contains documents, we can query it. [VectorStore](https://reference.langchain.com/python/langchain\\_core/vectorstores/?h=#langchain\\_core.vectorstores.base.VectorStore) includes methods for querying: \\* Synchronously and asynchronously; \\* By string query and by vector; \\* With and without returning similarity scores; \\* By similarity and [maximum marginal relevance](https://reference.langchain.com/python/langchain\\_core/vectorstores/?h=#langchain\\_core.vectorstores.base.VectorStore.max\\_marginal\\_relevance\\_search) (to balance similarity with query to diversity in retrieved results). The methods will generally include a list of [Document](https://reference.langchain.com/python/langchain\\_core/documents/#langchain\\_core.documents.base.Document) objects in their outputs. \\*\\*Usage\\*\\* Embeddings typically represent text as a \"dense\" vector such that texts with similar meanings are geometrically close. This lets us retrieve relevant information just by passing in a question, without knowledge of any specific key-terms used in the document. Return documents based on similarity to a string query: ```python theme={null} results = vector\\_store.similarity\\_search( \"How many distribution centers does Nike have in the US?\" ) print(results[0]) ``` ```output theme={null} page\\_content='direct to consumer operations sell products through the following number of retail stores in the United States: U.S. RETAIL STORES NUMBER NIKE Brand factory stores 213 NIKE Brand in-line stores (including employee-only stores) 74 Converse stores (including factory stores) 82 TOTAL 369 In the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information. 2023 FORM 10-K 2' metadata={'page': 4, 'source': '../example\\_data/nke-10k-2023.pdf', 'start\\_index': 3125} ``` Async query: ```python theme={null} results = await vector\\_store.asimilarity\\_search(\"When was Nike incorporated?\") print(results[0]) ``` ```output theme={null} page\\_content='Table of Contents PART I ITEM 1. BUSINESS GENERAL NIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\" \"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise. Our principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is the largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores and sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales' metadata={'page': 3, 'source': '../example\\_data/nke-10k-2023.pdf', 'start\\_index': 0} ``` Return scores: ```python theme={null} # Note that providers implement different scores; the score here # is a distance metric that varies inversely with similarity. results = vector\\_store.similarity\\_search\\_with\\_score(\"What was Nike's revenue in 2023?\") doc, score = results[0] print(f\"Score: {score}\\n\") print(doc) ``` ```output theme={null} Score: 0.23699893057346344 page\\_content='Table of Contents FISCAL 2023 NIKE BRAND REVENUE HIGHLIGHTS The following tables present NIKE Brand revenues disaggregated by reportable operating segment, distribution channel and major product line: FISCAL 2023 COMPARED TO FISCAL 2022 \u2022NIKE, Inc. Revenues were $51.2 billion in fiscal 2023, which increased 10% and 16% compared to fiscal 2022 on a reported and currency-neutral basis, respectively. The increase was due to higher revenues in North America, Europe, Middle East & Africa (\"EMEA\"), APLA and Greater China, which contributed approximately 7, 6, 2 and 1 percentage points to NIKE, Inc. Revenues, respectively. \u2022NIKE Brand revenues, which represented over 90% of NIKE, Inc. Revenues, increased 10% and 16% on a reported and currency-neutral basis, respectively. This increase was primarily due to higher revenues in Men's, the Jordan Brand, Women's and Kids' which grew 17%, 35%,11% and 10%, respectively, on a wholesale equivalent basis.' metadata={'page': 35, 'source': '../example\\_data/nke-10k-2023.pdf', 'start\\_index': 0} ``` Return documents based on similarity to an embedded query: ```python theme={null} embedding = embeddings.embed\\_query(\"How were Nike's margins impacted in 2023?\") results = vector\\_store.similarity\\_search\\_by\\_vector(embedding) print(results[0]) ``` ```output theme={null} page\\_content='Table of Contents GROSS MARGIN FISCAL 2023 COMPARED TO FISCAL 2022 For fiscal 2023, our consolidated gross profit increased 4% to $22,292 million compared to $21,479 million for fiscal 2022. Gross margin decreased 250 basis points to 43.5% for fiscal 2023 compared to 46.0% for fiscal 2022 due to the following: \\*Wholesale equivalent The decrease in gross margin for fiscal 2023 was primarily due to: \u2022Higher NIKE Brand product costs, on a wholesale equivalent basis, primarily due to higher input costs and elevated inbound freight and logistics costs as well as product mix; \u2022Lower margin in our NIKE Direct business, driven by higher promotional activity to liquidate inventory in the current period compared to lower promotional activity in the prior period resulting from lower available inventory supply; \u2022Unfavorable changes in net foreign currency exchange rates, including hedges; and \u2022Lower off-price margin, on a wholesale equivalent basis. This was partially offset by:' metadata={'page': 36, 'source': '../example\\_data/nke-10k-2023.pdf', 'start\\_index': 0} ``` Learn more: \\* [API Reference](https://reference.langchain.com/python/langchain\\_core/vectorstores/?h=#langchain\\_core.vectorstores.base.VectorStore) \\* [Integration-specific docs](/oss/python/integrations/vectorstores) ## 4. Retrievers LangChain [`VectorStore`](https://reference.langchain.com/python/langchain\\_core/vectorstores/?h=#langchain\\_core.vectorstores.base.VectorStore) objects do not subclass [Runnable](https://reference.langchain.com/python/langchain\\_core/runnables/#langchain\\_core.runnables.Runnable). LangChain [Retrievers](https://reference.langchain.com/python/langchain\\_core/retrievers/#langchain\\_core.retrievers.BaseRetriever) are Runnables, so they implement a standard set of methods (e.g., synchronous and asynchronous `invoke` and `batch` operations). Although we can construct retrievers from vector stores, retrievers can interface with non-vector store sources of data, as well (such as external APIs). We can create a simple version of this ourselves, without subclassing `Retriever`. If we choose what method we wish to use to retrieve documents, we can create a runnable easily. Below we will build one around the `similarity\\_search` method: ```python theme={null} from typing import List from langchain\\_core.documents import Document from langchain\\_core.runnables import chain @chain def retriever(query: str) -> List[Document]: return vector\\_store.similarity\\_search(query, k=1) retriever.batch( [ \"How many distribution centers does Nike have in the US?\", \"When was Nike incorporated?\", ], ) ``` ```output theme={null} [[Document(metadata={'page': 4, 'source': '../example\\_data/nke-10k-2023.pdf', 'start\\_index': 3125}, page\\_content='direct to consumer operations sell products through the following number of retail stores in the United States:\\nU.S. RETAIL STORES NUMBER\\nNIKE Brand factory stores 213 \\nNIKE Brand in-line stores (including employee-only stores) 74 \\nConverse stores (including factory stores) 82 \\nTOTAL 369 \\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\n2023 FORM 10-K 2')], [Document(metadata={'page': 3, 'source': '../example\\_data/nke-10k-2023.pdf', 'start\\_index': 0}, page\\_content='Table of Contents\\nPART I\\nITEM 1. BUSINESS\\nGENERAL\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales')]] ``` Vectorstores implement an `as\\_retriever` method that will generate a Retriever, specifically a [VectorStoreRetriever](https://python.langchain.com/api\\_reference/core/vectorstores/langchain\\_core.vectorstores.base.VectorStoreRetriever.html). These retrievers include specific `search\\_type` and `search\\_kwargs` attributes that identify what methods of the underlying vector store to call, and how to parameterize them. For instance, we can replicate the above with the following: ```python theme={null} retriever = vector\\_store.as\\_retriever( search\\_type=\"similarity\", search\\_kwargs={\"k\": 1}, ) retriever.batch( [ \"How many distribution centers does Nike have in the US?\", \"When was Nike incorporated?\", ], ) ``` ```output theme={null} [[Document(metadata={'page': 4, 'source': '../example\\_data/nke-10k-2023.pdf', 'start\\_index': 3125}, page\\_content='direct to consumer operations sell products through the following number of retail stores in the United States:\\nU.S. RETAIL STORES NUMBER\\nNIKE Brand factory stores 213 \\nNIKE Brand in-line stores (including employee-only stores) 74 \\nConverse stores (including factory stores) 82 \\nTOTAL 369 \\nIn the United States, NIKE has eight significant distribution centers. Refer to Item 2. Properties for further information.\\n2023 FORM 10-K 2')], [Document(metadata={'page': 3, 'source': '../example\\_data/nke-10k-2023.pdf', 'start\\_index': 0}, page\\_content='Table of Contents\\nPART I\\nITEM 1. BUSINESS\\nGENERAL\\nNIKE, Inc. was incorporated in 1967 under the laws of the State of Oregon. As used in this Annual Report on Form 10-K (this \"Annual Report\"), the terms \"we,\" \"us,\" \"our,\"\\n\"NIKE\" and the \"Company\" refer to NIKE, Inc. and its predecessors, subsidiaries and affiliates, collectively, unless the context indicates otherwise.\\nOur principal business activity is the design, development and worldwide marketing and selling of athletic footwear, apparel, equipment, accessories and services. NIKE is\\nthe largest seller of athletic footwear and apparel in the world. We sell our products through NIKE Direct operations, which are comprised of both NIKE-owned retail stores\\nand sales through our digital platforms (also referred to as \"NIKE Brand Digital\"), to retail accounts and to a mix of independent distributors, licensees and sales')]] ``` `VectorStoreRetriever` supports search types of `\"similarity\"` (default), `\"mmr\"` (maximum marginal relevance, described above), and `\"similarity\\_score\\_threshold\"`. We can use the latter to threshold documents output by the retriever by similarity score. Retrievers can easily be incorporated into more complex applications, such as [retrieval-augmented generation (RAG)](/oss/python/langchain/retrieval) applications that combine a given question with retrieved context into a prompt for a LLM. To learn more about building such an application, check out the [RAG tutorial](/oss/python/langchain/rag) tutorial. ## Next steps You've now seen how to build a semantic search engine over a PDF document. For more on document loaders: \\* [Overview](/oss/python/langchain/retrieval#document\\_loaders) \\* [Available integrations](/oss/python/integrations/document\\_loaders/) For more on embeddings: \\* [Overview](/oss/python/langchain/retrieval#embedding\\_models/) \\* [Available integrations](/oss/python/integrations/text\\_embedding/) For more on vector stores: \\* [Overview](/oss/python/langchain/retrieval#vectorstores/) \\* [Available integrations](/oss/python/integrations/vectorstores/) For more on RAG, see: \\* [Build a Retrieval Augmented Generation (RAG) App](/oss/python/langchain/rag/) \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/knowledge-base.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/long-term-memory.md",
      "title": null,
      "raw_content": "# Long-term memory ## Overview LangChain agents use [LangGraph persistence](/oss/python/langgraph/persistence#memory-store) to enable long-term memory. This is a more advanced topic and requires knowledge of LangGraph to use. ## Memory storage LangGraph stores long-term memories as JSON documents in a [store](/oss/python/langgraph/persistence#memory-store). Each memory is organized under a custom `namespace` (similar to a folder) and a distinct `key` (like a file name). Namespaces often include user or org IDs or other labels that makes it easier to organize information. This structure enables hierarchical organization of memories. Cross-namespace searching is then supported through content filters. ```python theme={null} from langgraph.store.memory import InMemoryStore def embed(texts: list[str]) -> list[list[float]]: # Replace with an actual embedding function or LangChain embeddings object return [[1.0, 2.0] \\* len(texts)] # InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production use. store = InMemoryStore(index={\"embed\": embed, \"dims\": 2}) # [!code highlight] user\\_id = \"my-user\" application\\_context = \"chitchat\" namespace = (user\\_id, application\\_context) # [!code highlight] store.put( # [!code highlight] namespace, \"a-memory\", { \"rules\": [ \"User likes short, direct language\", \"User only speaks English & python\", ], \"my-key\": \"my-value\", }, ) # get the \"memory\" by ID item = store.get(namespace, \"a-memory\") # [!code highlight] # search for \"memories\" within this namespace, filtering on content equivalence, sorted by vector similarity items = store.search( # [!code highlight] namespace, filter={\"my-key\": \"my-value\"}, query=\"language preferences\" ) ``` For more information about the memory store, see the [Persistence](/oss/python/langgraph/persistence#memory-store) guide. ## Read long-term memory in tools ```python A tool the agent can use to look up user information theme={null} from dataclasses import dataclass from langchain\\_core.runnables import RunnableConfig from langchain.agents import create\\_agent from langchain.tools import tool, ToolRuntime from langgraph.store.memory import InMemoryStore @dataclass class Context: user\\_id: str # InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production. store = InMemoryStore() # [!code highlight] # Write sample data to the store using the put method store.put( # [!code highlight] (\"users\",), # Namespace to group related data together (users namespace for user data) \"user\\_123\", # Key within the namespace (user ID as key) { \"name\": \"John Smith\", \"language\": \"English\", } # Data to store for the given user ) @tool def get\\_user\\_info(runtime: ToolRuntime[Context]) -> str: \"\"\"Look up user info.\"\"\" # Access the store - same as that provided to `create\\_agent` store = runtime.store # [!code highlight] user\\_id = runtime.context.user\\_id # Retrieve data from store - returns StoreValue object with value and metadata user\\_info = store.get((\"users\",), user\\_id) # [!code highlight] return str(user\\_info.value) if user\\_info else \"Unknown user\" agent = create\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=[get\\_user\\_info], # Pass store to agent - enables agent to access store when running tools store=store, # [!code highlight] context\\_schema=Context ) # Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"look up user information\"}]}, context=Context(user\\_id=\"user\\_123\") # [!code highlight] ) ```  ## Write long-term memory from tools ```python Example of a tool that updates user information theme={null} from dataclasses import dataclass from typing\\_extensions import TypedDict from langchain.agents import create\\_agent from langchain.tools import tool, ToolRuntime from langgraph.store.memory import InMemoryStore # InMemoryStore saves data to an in-memory dictionary. Use a DB-backed store in production. store = InMemoryStore() # [!code highlight] @dataclass class Context: user\\_id: str # TypedDict defines the structure of user information for the LLM class UserInfo(TypedDict): name: str # Tool that allows agent to update user information (useful for chat applications) @tool def save\\_user\\_info(user\\_info: UserInfo, runtime: ToolRuntime[Context]) -> str: \"\"\"Save user info.\"\"\" # Access the store - same as that provided to `create\\_agent` store = runtime.store # [!code highlight] user\\_id = runtime.context.user\\_id # [!code highlight] # Store data in the store (namespace, key, data) store.put((\"users\",), user\\_id, user\\_info) # [!code highlight] return \"Successfully saved user info.\" agent = create\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=[save\\_user\\_info], store=store, # [!code highlight] context\\_schema=Context ) # Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"My name is John Smith\"}]}, # user\\_id passed in context to identify whose information is being updated context=Context(user\\_id=\"user\\_123\") # [!code highlight] ) # You can access the store directly to get the value store.get((\"users\",), \"user\\_123\").value ``` \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/long-term-memory.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/mcp.md",
      "title": null,
      "raw_content": "# Model Context Protocol (MCP) [Model Context Protocol (MCP)](https://modelcontextprotocol.io/introduction) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchain-mcp-adapters) library. ## Install Install the `langchain-mcp-adapters` library to use MCP tools in LangGraph:  ```bash pip theme={null} pip install langchain-mcp-adapters ``` ```bash uv theme={null} uv add langchain-mcp-adapters ```  ## Transport types MCP supports different transport mechanisms for client-server communication: \\* \\*\\*stdio\\*\\* \u2013 Client launches server as a subprocess and communicates via standard input/output. Best for local tools and simple setups. \\* \\*\\*Streamable HTTP\\*\\* \u2013 Server runs as an independent process handling HTTP requests. Supports remote connections and multiple clients. \\* \\*\\*Server-Sent Events (SSE)\\*\\* \u2013 a variant of streamable HTTP optimized for real-time streaming communication. ## Use MCP tools `langchain-mcp-adapters` enables agents to use tools defined across one or more MCP server. ```python Accessing multiple MCP servers icon=\"server\" theme={null} from langchain\\_mcp\\_adapters.client import MultiServerMCPClient # [!code highlight] from langchain.agents import create\\_agent client = MultiServerMCPClient( # [!code highlight] { \"math\": { \"transport\": \"stdio\", # Local subprocess communication \"command\": \"python\", # Absolute path to your math\\_server.py file \"args\": [\"/path/to/math\\_server.py\"], }, \"weather\": { \"transport\": \"streamable\\_http\", # HTTP-based remote server # Ensure you start your weather server on port 8000 \"url\": \"http://localhost:8000/mcp\", } } ) tools = await client.get\\_tools() # [!code highlight] agent = create\\_agent( \"claude-sonnet-4-5-20250929\", tools # [!code highlight] ) math\\_response = await agent.ainvoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what's (3 + 5) x 12?\"}]} ) weather\\_response = await agent.ainvoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in nyc?\"}]} ) ```  `MultiServerMCPClient` is \\*\\*stateless by default\\*\\*. Each tool invocation creates a fresh MCP `ClientSession`, executes the tool, and then cleans up.  ## Custom MCP servers To create your own MCP servers, you can use the `mcp` library. This library provides a simple way to define [tools](https://modelcontextprotocol.io/docs/learn/server-concepts#tools-ai-actions) and run them as servers.  ```bash pip theme={null} pip install mcp ``` ```bash uv theme={null} uv add mcp ```  Use the following reference implementations to test your agent with MCP tool servers. ```python title=\"Math server (stdio transport)\" icon=\"floppy-disk\" theme={null} from mcp.server.fastmcp import FastMCP mcp = FastMCP(\"Math\") @mcp.tool() def add(a: int, b: int) -> int: \"\"\"Add two numbers\"\"\" return a + b @mcp.tool() def multiply(a: int, b: int) -> int: \"\"\"Multiply two numbers\"\"\" return a \\* b if \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\": mcp.run(transport=\"stdio\") ``` ```python title=\"Weather server (streamable HTTP transport)\" icon=\"wifi\" theme={null} from mcp.server.fastmcp import FastMCP mcp = FastMCP(\"Weather\") @mcp.tool() async def get\\_weather(location: str) -> str: \"\"\"Get weather for location.\"\"\" return \"It's always sunny in New York\" if \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\": mcp.run(transport=\"streamable-http\") ``` ## Stateful tool usage For stateful servers that maintain context between tool calls, use `client.session()` to create a persistent `ClientSession`. ```python Using MCP ClientSession for stateful tool usage theme={null} from langchain\\_mcp\\_adapters.tools import load\\_mcp\\_tools client = MultiServerMCPClient({...}) async with client.session(\"math\") as session: tools = await load\\_mcp\\_tools(session) ``` ## Additional resources \\* [MCP documentation](https://modelcontextprotocol.io/introduction) \\* [MCP Transport documentation](https://modelcontextprotocol.io/docs/concepts/transports) \\* [`langchain-mcp-adapters`](https://github.com/langchain-ai/langchain-mcp-adapters) \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/mcp.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/messages.md",
      "title": null,
      "raw_content": "# Messages Messages are the fundamental unit of context for models in LangChain. They represent the input and output of models, carrying both the content and metadata needed to represent the state of a conversation when interacting with an LLM. Messages are objects that contain: \\* [\\*\\*Role\\*\\*](#message-types) - Identifies the message type (e.g. `system`, `user`) \\* [\\*\\*Content\\*\\*](#message-content) - Represents the actual content of the message (like text, images, audio, documents, etc.) \\* [\\*\\*Metadata\\*\\*](#message-metadata) - Optional fields such as response information, message IDs, and token usage LangChain provides a standard message type that works across all model providers, ensuring consistent behavior regardless of the model being called. ## Basic usage The simplest way to use messages is to create message objects and pass them to a model when [invoking](/oss/python/langchain/models#invocation). ```python theme={null} from langchain.chat\\_models import init\\_chat\\_model from langchain.messages import HumanMessage, AIMessage, SystemMessage model = init\\_chat\\_model(\"gpt-5-nano\") system\\_msg = SystemMessage(\"You are a helpful assistant.\") human\\_msg = HumanMessage(\"Hello, how are you?\") # Use with chat models messages = [system\\_msg, human\\_msg] response = model.invoke(messages) # Returns AIMessage ``` ### Text prompts Text prompts are strings - ideal for straightforward generation tasks where you don't need to retain conversation history. ```python theme={null} response = model.invoke(\"Write a haiku about spring\") ``` \\*\\*Use text prompts when:\\*\\* \\* You have a single, standalone request \\* You don't need conversation history \\* You want minimal code complexity ### Message prompts Alternatively, you can pass in a list of messages to the model by providing a list of message objects. ```python theme={null} from langchain.messages import SystemMessage, HumanMessage, AIMessage messages = [ SystemMessage(\"You are a poetry expert\"), HumanMessage(\"Write a haiku about spring\"), AIMessage(\"Cherry blossoms bloom...\") ] response = model.invoke(messages) ``` \\*\\*Use message prompts when:\\*\\* \\* Managing multi-turn conversations \\* Working with multimodal content (images, audio, files) \\* Including system instructions ### Dictionary format You can also specify messages directly in OpenAI chat completions format. ```python theme={null} messages = [ {\"role\": \"system\", \"content\": \"You are a poetry expert\"}, {\"role\": \"user\", \"content\": \"Write a haiku about spring\"}, {\"role\": \"assistant\", \"content\": \"Cherry blossoms bloom...\"} ] response = model.invoke(messages) ``` ## Message types \\* [System message](#system-message) - Tells the model how to behave and provide context for interactions \\* [Human message](#human-message) - Represents user input and interactions with the model \\* [AI message](#ai-message) - Responses generated by the model, including text content, tool calls, and metadata \\* [Tool message](#tool-message) - Represents the outputs of [tool calls](/oss/python/langchain/models#tool-calling) ### System Message A [`SystemMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.SystemMessage) represent an initial set of instructions that primes the model's behavior. You can use a system message to set the tone, define the model's role, and establish guidelines for responses. ```python Basic instructions theme={null} system\\_msg = SystemMessage(\"You are a helpful coding assistant.\") messages = [ system\\_msg, HumanMessage(\"How do I create a REST API?\") ] response = model.invoke(messages) ``` ```python Detailed persona theme={null} from langchain.messages import SystemMessage, HumanMessage system\\_msg = SystemMessage(\"\"\" You are a senior Python developer with expertise in web frameworks. Always provide code examples and explain your reasoning. Be concise but thorough in your explanations. \"\"\") messages = [ system\\_msg, HumanMessage(\"How do I create a REST API?\") ] response = model.invoke(messages) ``` \\*\\*\\* ### Human Message A [`HumanMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.HumanMessage) represents user input and interactions. They can contain text, images, audio, files, and any other amount of multimodal [content](#message-content). #### Text content  ```python Message object theme={null} response = model.invoke([ HumanMessage(\"What is machine learning?\") ]) ``` ```python String shortcut theme={null} # Using a string is a shortcut for a single HumanMessage response = model.invoke(\"What is machine learning?\") ```  #### Message metadata ```python Add metadata theme={null} human\\_msg = HumanMessage( content=\"Hello!\", name=\"alice\", # Optional: identify different users id=\"msg\\_123\", # Optional: unique identifier for tracing ) ```  The `name` field behavior varies by provider \u2013 some use it for user identification, others ignore it. To check, refer to the model provider's [reference](https://reference.langchain.com/python/integrations/).  \\*\\*\\* ### AI Message An [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) represents the output of a model invocation. They can include multimodal data, tool calls, and provider-specific metadata that you can later access. ```python theme={null} response = model.invoke(\"Explain AI\") print(type(response)) #  ``` [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) objects are returned by the model when calling it, which contains all of the associated metadata in the response. Providers weigh/contextualize types of messages differently, which means it is sometimes helpful to manually create a new [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) object and insert it into the message history as if it came from the model. ```python theme={null} from langchain.messages import AIMessage, SystemMessage, HumanMessage # Create an AI message manually (e.g., for conversation history) ai\\_msg = AIMessage(\"I'd be happy to help you with that question!\") # Add to conversation history messages = [ SystemMessage(\"You are a helpful assistant\"), HumanMessage(\"Can you help me?\"), ai\\_msg, # Insert as if it came from the model HumanMessage(\"Great! What's 2+2?\") ] response = model.invoke(messages) ```   The text content of the message.   The raw content of the message.   The standardized [content blocks](#message-content) of the message.   The tool calls made by the model. Empty if no tools are called.   A unique identifier for the message (either automatically generated by LangChain or returned in the provider response)   The usage metadata of the message, which can contain token counts when available.   The response metadata of the message.   #### Tool calls When models make [tool calls](/oss/python/langchain/models#tool-calling), they're included in the [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage): ```python theme={null} from langchain.chat\\_models import init\\_chat\\_model model = init\\_chat\\_model(\"gpt-5-nano\") def get\\_weather(location: str) -> str: \"\"\"Get the weather at a location.\"\"\" ... model\\_with\\_tools = model.bind\\_tools([get\\_weather]) response = model\\_with\\_tools.invoke(\"What's the weather in Paris?\") for tool\\_call in response.tool\\_calls: print(f\"Tool: {tool\\_call['name']}\") print(f\"Args: {tool\\_call['args']}\") print(f\"ID: {tool\\_call['id']}\") ``` Other structured data, such as reasoning or citations, can also appear in message [content](/oss/python/langchain/messages#message-content). #### Token usage An [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) can hold token counts and other usage metadata in its [`usage\\_metadata`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.UsageMetadata) field: ```python theme={null} from langchain.chat\\_models import init\\_chat\\_model model = init\\_chat\\_model(\"gpt-5-nano\") response = model.invoke(\"Hello!\") response.usage\\_metadata ``` ``` {'input\\_tokens': 8, 'output\\_tokens': 304, 'total\\_tokens': 312, 'input\\_token\\_details': {'audio': 0, 'cache\\_read': 0}, 'output\\_token\\_details': {'audio': 0, 'reasoning': 256}} ``` See [`UsageMetadata`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.UsageMetadata) for details. #### Streaming and chunks During streaming, you'll receive [`AIMessageChunk`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessageChunk) objects that can be combined into a full message object: ```python theme={null} chunks = [] full\\_message = None for chunk in model.stream(\"Hi\"): chunks.append(chunk) print(chunk.text) full\\_message = chunk if full\\_message is None else full\\_message + chunk ```  Learn more: \\* [Streaming tokens from chat models](/oss/python/langchain/models#stream) \\* [Streaming tokens and/or steps from agents](/oss/python/langchain/streaming)  \\*\\*\\* ### Tool Message For models that support [tool calling](/oss/python/langchain/models#tool-calling), AI messages can contain tool calls. Tool messages are used to pass the results of a single tool execution back to the model. [Tools](/oss/python/langchain/tools) can generate [`ToolMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage) objects directly. Below, we show a simple example. Read more in the [tools guide](/oss/python/langchain/tools). ```python theme={null} from langchain.messages import AIMessage from langchain.messages import ToolMessage # After a model makes a tool call # (Here, we demonstrate manually creating the messages for brevity) ai\\_message = AIMessage( content=[], tool\\_calls=[{ \"name\": \"get\\_weather\", \"args\": {\"location\": \"San Francisco\"}, \"id\": \"call\\_123\" }] ) # Execute tool and create result message weather\\_result = \"Sunny, 72\u00b0F\" tool\\_message = ToolMessage( content=weather\\_result, tool\\_call\\_id=\"call\\_123\" # Must match the call ID ) # Continue conversation messages = [ HumanMessage(\"What's the weather in San Francisco?\"), ai\\_message, # Model's tool call tool\\_message, # Tool execution result ] response = model.invoke(messages) # Model processes the result ```   The stringified output of the tool call.   The ID of the tool call that this message is responding to. Must match the ID of the tool call in the [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage).   The name of the tool that was called.   Additional data not sent to the model but can be accessed programmatically.    The `artifact` field stores supplementary data that won't be sent to the model but can be accessed programmatically. This is useful for storing raw results, debugging information, or data for downstream processing without cluttering the model's context.  For example, a [retrieval](/oss/python/langchain/retrieval) tool could retrieve a passage from a document for reference by a model. Where message `content` contains text that the model will reference, an `artifact` can contain document identifiers or other metadata that an application can use (e.g., to render a page). See example below: ```python theme={null} from langchain.messages import ToolMessage # Sent to model message\\_content = \"It was the best of times, it was the worst of times.\" # Artifact available downstream artifact = {\"document\\_id\": \"doc\\_123\", \"page\": 0} tool\\_message = ToolMessage( content=message\\_content, tool\\_call\\_id=\"call\\_123\", name=\"search\\_books\", artifact=artifact, ) ``` See the [RAG tutorial](/oss/python/langchain/rag) for an end-to-end example of building retrieval [agents](/oss/python/langchain/agents) with LangChain.   \\*\\*\\* ## Message content You can think of a message's content as the payload of data that gets sent to the model. Messages have a `content` attribute that is loosely-typed, supporting strings and lists of untyped objects (e.g., dictionaries). This allows support for provider-native structures directly in LangChain chat models, such as [multimodal](#multimodal) content and other data. Separately, LangChain provides dedicated content types for text, reasoning, citations, multi-modal data, server-side tool calls, and other message content. See [content blocks](#standard-content-blocks) below. LangChain chat models accept message content in the `content` attribute. This may contain either: 1. A string 2. A list of content blocks in a provider-native format 3. A list of [LangChain's standard content blocks](#standard-content-blocks) See below for an example using [multimodal](#multimodal) inputs: ```python theme={null} from langchain.messages import HumanMessage # String content human\\_message = HumanMessage(\"Hello, how are you?\") # Provider-native format (e.g., OpenAI) human\\_message = HumanMessage(content=[ {\"type\": \"text\", \"text\": \"Hello, how are you?\"}, {\"type\": \"image\\_url\", \"image\\_url\": {\"url\": \"https://example.com/image.jpg\"}} ]) # List of standard content blocks human\\_message = HumanMessage(content\\_blocks=[ {\"type\": \"text\", \"text\": \"Hello, how are you?\"}, {\"type\": \"image\", \"url\": \"https://example.com/image.jpg\"}, ]) ```  Specifying `content\\_blocks` when initializing a message will still populate message `content`, but provides a type-safe interface for doing so.  ### Standard content blocks LangChain provides a standard representation for message content that works across providers. Message objects implement a `content\\_blocks` property that will lazily parse the `content` attribute into a standard, type-safe representation. For example, messages generated from [`ChatAnthropic`](/oss/python/integrations/chat/anthropic) or [`ChatOpenAI`](/oss/python/integrations/chat/openai) will include `thinking` or `reasoning` blocks in the format of the respective provider, but can be lazily parsed into a consistent [`ReasoningContentBlock`](#content-block-reference) representation:   ```python theme={null} from langchain.messages import AIMessage message = AIMessage( content=[ {\"type\": \"thinking\", \"thinking\": \"...\", \"signature\": \"WaUjzkyp...\"}, {\"type\": \"text\", \"text\": \"...\"}, ], response\\_metadata={\"model\\_provider\": \"anthropic\"} ) message.content\\_blocks ``` ``` [{'type': 'reasoning', 'reasoning': '...', 'extras': {'signature': 'WaUjzkyp...'}}, {'type': 'text', 'text': '...'}] ```   ```python theme={null} from langchain.messages import AIMessage message = AIMessage( content=[ { \"type\": \"reasoning\", \"id\": \"rs\\_abc123\", \"summary\": [ {\"type\": \"summary\\_text\", \"text\": \"summary 1\"}, {\"type\": \"summary\\_text\", \"text\": \"summary 2\"}, ], }, {\"type\": \"text\", \"text\": \"...\", \"id\": \"msg\\_abc123\"}, ], response\\_metadata={\"model\\_provider\": \"openai\"} ) message.content\\_blocks ``` ``` [{'type': 'reasoning', 'id': 'rs\\_abc123', 'reasoning': 'summary 1'}, {'type': 'reasoning', 'id': 'rs\\_abc123', 'reasoning': 'summary 2'}, {'type': 'text', 'text': '...', 'id': 'msg\\_abc123'}] ```   See the [integrations guides](/oss/python/integrations/providers/overview) to get started with the inference provider of your choice.  \\*\\*Serializing standard content\\*\\* If an application outside of LangChain needs access to the standard content block representation, you can opt-in to storing content blocks in message content. To do this, you can set the `LC\\_OUTPUT\\_VERSION` environment variable to `v1`. Or, initialize any chat model with `output\\_version=\"v1\"`: ```python theme={null} from langchain.chat\\_models import init\\_chat\\_model model = init\\_chat\\_model(\"gpt-5-nano\", output\\_version=\"v1\") ```  ### Multimodal \\*\\*Multimodality\\*\\* refers to the ability to work with data that comes in different forms, such as text, audio, images, and video. LangChain includes standard types for these data that can be used across providers. [Chat models](/oss/python/langchain/models) can accept multimodal data as input and generate it as output. Below we show short examples of input messages featuring multimodal data.  Extra keys can be included top-level in the content block or nested in `\"extras\": {\"key\": value}`. [OpenAI](/oss/python/integrations/chat/openai#pdfs) and [AWS Bedrock Converse](/oss/python/integrations/chat/bedrock), for example, require a filename for PDFs. See the [provider page](/oss/python/integrations/providers/overview) for your chosen model for specifics.   ```python Image input theme={null} # From URL message = { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"Describe the content of this image.\"}, {\"type\": \"image\", \"url\": \"https://example.com/path/to/image.jpg\"}, ] } # From base64 data message = { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"Describe the content of this image.\"}, { \"type\": \"image\", \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\", \"mime\\_type\": \"image/jpeg\", }, ] } # From provider-managed File ID message = { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"Describe the content of this image.\"}, {\"type\": \"image\", \"file\\_id\": \"file-abc123\"}, ] } ``` ```python PDF document input theme={null} # From URL message = { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"Describe the content of this document.\"}, {\"type\": \"file\", \"url\": \"https://example.com/path/to/document.pdf\"}, ] } # From base64 data message = { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"Describe the content of this document.\"}, { \"type\": \"file\", \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\", \"mime\\_type\": \"application/pdf\", }, ] } # From provider-managed File ID message = { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"Describe the content of this document.\"}, {\"type\": \"file\", \"file\\_id\": \"file-abc123\"}, ] } ``` ```python Audio input theme={null} # From base64 data message = { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"Describe the content of this audio.\"}, { \"type\": \"audio\", \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\", \"mime\\_type\": \"audio/wav\", }, ] } # From provider-managed File ID message = { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"Describe the content of this audio.\"}, {\"type\": \"audio\", \"file\\_id\": \"file-abc123\"}, ] } ``` ```python Video input theme={null} # From base64 data message = { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"Describe the content of this video.\"}, { \"type\": \"video\", \"base64\": \"AAAAIGZ0eXBtcDQyAAAAAGlzb21tcDQyAAACAGlzb2...\", \"mime\\_type\": \"video/mp4\", }, ] } # From provider-managed File ID message = { \"role\": \"user\", \"content\": [ {\"type\": \"text\", \"text\": \"Describe the content of this video.\"}, {\"type\": \"video\", \"file\\_id\": \"file-abc123\"}, ] } ```   Not all models support all file types. Check the model provider's [reference](https://reference.langchain.com/python/integrations/) for supported formats and size limits.  ### Content block reference Content blocks are represented (either when creating a message or accessing the `content\\_blocks` property) as a list of typed dictionaries. Each item in the list must adhere to one of the following block types:     \\*\\*Purpose:\\*\\* Standard text output  Always `\"text\"`   The text content   List of annotations for the text   Additional provider-specific data  \\*\\*Example:\\*\\* ```python theme={null} { \"type\": \"text\", \"text\": \"Hello world\", \"annotations\": [] } ```   \\*\\*Purpose:\\*\\* Model reasoning steps  Always `\"reasoning\"`   The reasoning content   Additional provider-specific data  \\*\\*Example:\\*\\* ```python theme={null} { \"type\": \"reasoning\", \"reasoning\": \"The user is asking about...\", \"extras\": {\"signature\": \"abc123\"}, } ```       \\*\\*Purpose:\\*\\* Image data  Always `\"image\"`   URL pointing to the image location.   Base64-encoded image data.   Reference ID to an externally stored image (e.g., in a provider's file system or in a bucket).   Image [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#image) (e.g., `image/jpeg`, `image/png`)    \\*\\*Purpose:\\*\\* Audio data  Always `\"audio\"`   URL pointing to the audio location.   Base64-encoded audio data.   Reference ID to an externally stored audio file (e.g., in a provider's file system or in a bucket).   Audio [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#audio) (e.g., `audio/mpeg`, `audio/wav`)    \\*\\*Purpose:\\*\\* Video data  Always `\"video\"`   URL pointing to the video location.   Base64-encoded video data.   Reference ID to an externally stored video file (e.g., in a provider's file system or in a bucket).   Video [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml#video) (e.g., `video/mp4`, `video/webm`)    \\*\\*Purpose:\\*\\* Generic files (PDF, etc)  Always `\"file\"`   URL pointing to the file location.   Base64-encoded file data.   Reference ID to an externally stored file (e.g., in a provider's file system or in a bucket).   File [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml) (e.g., `application/pdf`)    \\*\\*Purpose:\\*\\* Document text (`.txt`, `.md`)  Always `\"text-plain\"`   The text content   [MIME type](https://www.iana.org/assignments/media-types/media-types.xhtml) of the text (e.g., `text/plain`, `text/markdown`)        \\*\\*Purpose:\\*\\* Function calls  Always `\"tool\\_call\"`   Name of the tool to call   Arguments to pass to the tool   Unique identifier for this tool call  \\*\\*Example:\\*\\* ```python theme={null} { \"type\": \"tool\\_call\", \"name\": \"search\", \"args\": {\"query\": \"weather\"}, \"id\": \"call\\_123\" } ```   \\*\\*Purpose:\\*\\* Streaming tool call fragments  Always `\"tool\\_call\\_chunk\"`   Name of the tool being called   Partial tool arguments (may be incomplete JSON)   Tool call identifier   Position of this chunk in the stream    \\*\\*Purpose:\\*\\* Malformed calls, intended to catch JSON parsing errors.  Always `\"invalid\\_tool\\_call\"`   Name of the tool that failed to be called   Arguments to pass to the tool   Description of what went wrong        \\*\\*Purpose:\\*\\* Tool call that is executed server-side.  Always `\"server\\_tool\\_call\"`   An identifier associated with the tool call.   The name of the tool to be called.   Partial tool arguments (may be incomplete JSON)    \\*\\*Purpose:\\*\\* Streaming server-side tool call fragments  Always `\"server\\_tool\\_call\\_chunk\"`   An identifier associated with the tool call.   Name of the tool being called   Partial tool arguments (may be incomplete JSON)   Position of this chunk in the stream    \\*\\*Purpose:\\*\\* Search results  Always `\"server\\_tool\\_result\"`   Identifier of the corresponding server tool call.   Identifier associated with the server tool result.   Execution status of the server-side tool. `\"success\"` or `\"error\"`.   Output of the executed tool.       \\*\\*Purpose:\\*\\* Provider-specific escape hatch  Always `\"non\\_standard\"`   Provider-specific data structure  \\*\\*Usage:\\*\\* For experimental or provider-unique features  Additional provider-specific content types may be found within the [reference documentation](/oss/python/integrations/providers/overview) of each model provider.    View the canonical type definitions in the [API reference](https://reference.langchain.com/python/langchain/messages).   Content blocks were introduced as a new property on messages in LangChain v1 to standardize content formats across providers while maintaining backward compatibility with existing code. Content blocks are not a replacement for the [`content`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.messages.BaseMessage.content) property, but rather a new property that can be used to access the content of a message in a standardized format.  ## Use with chat models [Chat models](/oss/python/langchain/models) accept a sequence of message objects as input and return an [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) as output. Interactions are often stateless, so that a simple conversational loop involves invoking a model with a growing list of messages. Refer to the below guides to learn more: \\* Built-in features for [persisting and managing conversation histories](/oss/python/langchain/short-term-memory) \\* Strategies for managing context windows, including [trimming and summarizing messages](/oss/python/langchain/short-term-memory#common-patterns) \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/messages.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/middleware/built-in.md",
      "title": null,
      "raw_content": "# Built-in middleware > Prebuilt middleware for common agent use cases LangChain provides prebuilt middleware for common use cases. Each middleware is production-ready and configurable for your specific needs. ## Provider-agnostic middleware The following middleware work with any LLM provider: | Middleware | Description | | --------------------------------------- | --------------------------------------------------------------------------- | | [Summarization](#summarization) | Automatically summarize conversation history when approaching token limits. | | [Human-in-the-loop](#human-in-the-loop) | Pause execution for human approval of tool calls. | | [Model call limit](#model-call-limit) | Limit the number of model calls to prevent excessive costs. | | [Tool call limit](#tool-call-limit) | Control tool execution by limiting call counts. | | [Model fallback](#model-fallback) | Automatically fallback to alternative models when primary fails. | | [PII detection](#pii-detection) | Detect and handle Personally Identifiable Information (PII). | | [To-do list](#to-do-list) | Equip agents with task planning and tracking capabilities. | | [LLM tool selector](#llm-tool-selector) | Use an LLM to select relevant tools before calling main model. | | [Tool retry](#tool-retry) | Automatically retry failed tool calls with exponential backoff. | | [Model retry](#model-retry) | Automatically retry failed model calls with exponential backoff. | | [LLM tool emulator](#llm-tool-emulator) | Emulate tool execution using an LLM for testing purposes. | | [Context editing](#context-editing) | Manage conversation context by trimming or clearing tool uses. | | [Shell tool](#shell-tool) | Expose a persistent shell session to agents for command execution. | | [File search](#file-search) | Provide Glob and Grep search tools over filesystem files. | ### Summarization Automatically summarize conversation history when approaching token limits, preserving recent messages while compressing older context. Summarization is useful for the following: \\* Long-running conversations that exceed context windows. \\* Multi-turn dialogues with extensive history. \\* Applications where preserving full conversation context matters. \\*\\*API reference:\\*\\* [`SummarizationMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.SummarizationMiddleware) ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import SummarizationMiddleware agent = create\\_agent( model=\"gpt-4o\", tools=[your\\_weather\\_tool, your\\_calculator\\_tool], middleware=[ SummarizationMiddleware( model=\"gpt-4o-mini\", trigger=(\"tokens\", 4000), keep=(\"messages\", 20), ), ], ) ```   The `fraction` conditions for `trigger` and `keep` (shown below) rely on a chat model's [profile data](/oss/python/langchain/models#model-profiles) if using `langchain>=1.1`. If data are not available, use another condition or specify manually: ```python theme={null} from langchain.chat\\_models import init\\_chat\\_model custom\\_profile = { \"max\\_input\\_tokens\": 100\\_000, # ... } model = init\\_chat\\_model(\"gpt-4o\", profile=custom\\_profile) ```   Model for generating summaries. Can be a model identifier string (e.g., `'openai:gpt-4o-mini'`) or a `BaseChatModel` instance. See [`init\\_chat\\_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat\\_models.init\\_chat\\_model\\(model\\)) for more information.   Conditions for triggering summarization. Can be: \\* A single [`ContextSize`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.summarization.ContextSize) dict (all properties must be met - AND logic) \\* A list of [`ContextSize`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.summarization.ContextSize) dicts (any condition must be met - OR logic) Each condition can include: \\* `fraction` (float): Fraction of model's context size (0-1) \\* `tokens` (int): Absolute token count \\* `messages` (int): Message count At least one property must be specified per condition. If not provided, summarization will not trigger automatically. See the API reference for [`ContextSize`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.summarization.ContextSize) for more information.   How much context to preserve after summarization. Specify exactly one of: \\* `fraction` (float): Fraction of model's context size to keep (0-1) \\* `tokens` (int): Absolute token count to keep \\* `messages` (int): Number of recent messages to keep See the API reference for [`ContextSize`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.summarization.ContextSize) for more information.   Custom token counting function. Defaults to character-based counting.   Custom prompt template for summarization. Uses built-in template if not specified. The template should include `{messages}` placeholder where conversation history will be inserted.   Maximum number of tokens to include when generating the summary. Messages will be trimmed to fit this limit before summarization.   Prefix to add to the summary message. If not provided, a default prefix is used.   \\*\\*Deprecated:\\*\\* Use `trigger: {\"tokens\": value}` instead. Token threshold for triggering summarization.   \\*\\*Deprecated:\\*\\* Use `keep: {\"messages\": value}` instead. Recent messages to preserve.    The summarization middleware monitors message token counts and automatically summarizes older messages when thresholds are reached. \\*\\*Trigger conditions\\*\\* control when summarization runs: \\* Single condition object (all properties must be met - AND logic) \\* Array of conditions (any condition must be met - OR logic) \\* Each condition can use `fraction` (of model's context size), `tokens` (absolute count), or `messages` (message count) \\*\\*Keep conditions\\*\\* control how much context to preserve (specify exactly one): \\* `fraction` - Fraction of model's context size to keep \\* `tokens` - Absolute token count to keep \\* `messages` - Number of recent messages to keep ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import SummarizationMiddleware # Single condition: trigger if tokens >= 4000 AND messages >= 10 agent = create\\_agent( model=\"gpt-4o\", tools=[your\\_weather\\_tool, your\\_calculator\\_tool], middleware=[ SummarizationMiddleware( model=\"gpt-4o-mini\", trigger=[(\"tokens\", 4000), (\"messages\", 10)], keep=(\"messages\", 20), ), ], ) # Multiple conditions agent2 = create\\_agent( model=\"gpt-4o\", tools=[your\\_weather\\_tool, your\\_calculator\\_tool], middleware=[ SummarizationMiddleware( model=\"gpt-4o-mini\", trigger=[ (\"tokens\", 3000), (\"messages\", 6), ], keep=(\"messages\", 20), ), ], ) # Using fractional limits agent3 = create\\_agent( model=\"gpt-4o\", tools=[your\\_weather\\_tool, your\\_calculator\\_tool], middleware=[ SummarizationMiddleware( model=\"gpt-4o-mini\", trigger=(\"fraction\", 0.8), keep=(\"fraction\", 0.3), ), ], ) ```  ### Human-in-the-loop Pause agent execution for human approval, editing, or rejection of tool calls before they execute. [Human-in-the-loop](/oss/python/langchain/human-in-the-loop) is useful for the following: \\* High-stakes operations requiring human approval (e.g. database writes, financial transactions). \\* Compliance workflows where human oversight is mandatory. \\* Long-running conversations where human feedback guides the agent. \\*\\*API reference:\\*\\* [`HumanInTheLoopMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.HumanInTheLoopMiddleware)  Human-in-the-loop middleware requires a [checkpointer](/oss/python/langgraph/persistence#checkpoints) to maintain state across interruptions.  ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import HumanInTheLoopMiddleware from langgraph.checkpoint.memory import InMemorySaver def read\\_email\\_tool(email\\_id: str) -> str: \"\"\"Mock function to read an email by its ID.\"\"\" return f\"Email content for ID: {email\\_id}\" def send\\_email\\_tool(recipient: str, subject: str, body: str) -> str: \"\"\"Mock function to send an email.\"\"\" return f\"Email sent to {recipient} with subject '{subject}'\" agent = create\\_agent( model=\"gpt-4o\", tools=[your\\_read\\_email\\_tool, your\\_send\\_email\\_tool], checkpointer=InMemorySaver(), middleware=[ HumanInTheLoopMiddleware( interrupt\\_on={ \"your\\_send\\_email\\_tool\": { \"allowed\\_decisions\": [\"approve\", \"edit\", \"reject\"], }, \"your\\_read\\_email\\_tool\": False, } ), ], ) ```  For complete examples, configuration options, and integration patterns, see the [Human-in-the-loop documentation](/oss/python/langchain/human-in-the-loop).  ### Model call limit Limit the number of model calls to prevent infinite loops or excessive costs. Model call limit is useful for the following: \\* Preventing runaway agents from making too many API calls. \\* Enforcing cost controls on production deployments. \\* Testing agent behavior within specific call budgets. \\*\\*API reference:\\*\\* [`ModelCallLimitMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ModelCallLimitMiddleware) ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import ModelCallLimitMiddleware agent = create\\_agent( model=\"gpt-4o\", tools=[], middleware=[ ModelCallLimitMiddleware( thread\\_limit=10, run\\_limit=5, exit\\_behavior=\"end\", ), ], ) ```   Maximum model calls across all runs in a thread. Defaults to no limit.   Maximum model calls per single invocation. Defaults to no limit.   Behavior when limit is reached. Options: `'end'` (graceful termination) or `'error'` (raise exception)   ### Tool call limit Control agent execution by limiting the number of tool calls, either globally across all tools or for specific tools. Tool call limits are useful for the following: \\* Preventing excessive calls to expensive external APIs. \\* Limiting web searches or database queries. \\* Enforcing rate limits on specific tool usage. \\* Protecting against runaway agent loops. \\*\\*API reference:\\*\\* [`ToolCallLimitMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ToolCallLimitMiddleware) ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import ToolCallLimitMiddleware agent = create\\_agent( model=\"gpt-4o\", tools=[search\\_tool, database\\_tool], middleware=[ # Global limit ToolCallLimitMiddleware(thread\\_limit=20, run\\_limit=10), # Tool-specific limit ToolCallLimitMiddleware( tool\\_name=\"search\", thread\\_limit=5, run\\_limit=3, ), ], ) ```   Name of specific tool to limit. If not provided, limits apply to \\*\\*all tools globally\\*\\*.   Maximum tool calls across all runs in a thread (conversation). Persists across multiple invocations with the same thread ID. Requires a checkpointer to maintain state. `None` means no thread limit.   Maximum tool calls per single invocation (one user message \u2192 response cycle). Resets with each new user message. `None` means no run limit. \\*\\*Note:\\*\\* At least one of `thread\\_limit` or `run\\_limit` must be specified.   Behavior when limit is reached: \\* `'continue'` (default) - Block exceeded tool calls with error messages, let other tools and the model continue. The model decides when to end based on the error messages. \\* `'error'` - Raise a `ToolCallLimitExceededError` exception, stopping execution immediately \\* `'end'` - Stop execution immediately with a `ToolMessage` and AI message for the exceeded tool call. Only works when limiting a single tool; raises `NotImplementedError` if other tools have pending calls.    Specify limits with: \\* \\*\\*Thread limit\\*\\* - Max calls across all runs in a conversation (requires checkpointer) \\* \\*\\*Run limit\\*\\* - Max calls per single invocation (resets each turn) Exit behaviors: \\* `'continue'` (default) - Block exceeded calls with error messages, agent continues \\* `'error'` - Raise exception immediately \\* `'end'` - Stop with ToolMessage + AI message (single-tool scenarios only) ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import ToolCallLimitMiddleware global\\_limiter = ToolCallLimitMiddleware(thread\\_limit=20, run\\_limit=10) search\\_limiter = ToolCallLimitMiddleware(tool\\_name=\"search\", thread\\_limit=5, run\\_limit=3) database\\_limiter = ToolCallLimitMiddleware(tool\\_name=\"query\\_database\", thread\\_limit=10) strict\\_limiter = ToolCallLimitMiddleware(tool\\_name=\"scrape\\_webpage\", run\\_limit=2, exit\\_behavior=\"error\") agent = create\\_agent( model=\"gpt-4o\", tools=[search\\_tool, database\\_tool, scraper\\_tool], middleware=[global\\_limiter, search\\_limiter, database\\_limiter, strict\\_limiter], ) ```  ### Model fallback Automatically fallback to alternative models when the primary model fails. Model fallback is useful for the following: \\* Building resilient agents that handle model outages. \\* Cost optimization by falling back to cheaper models. \\* Provider redundancy across OpenAI, Anthropic, etc. \\*\\*API reference:\\*\\* [`ModelFallbackMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ModelFallbackMiddleware) ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import ModelFallbackMiddleware agent = create\\_agent( model=\"gpt-4o\", tools=[], middleware=[ ModelFallbackMiddleware( \"gpt-4o-mini\", \"claude-3-5-sonnet-20241022\", ), ], ) ```   First fallback model to try when the primary model fails. Can be a model identifier string (e.g., `'openai:gpt-4o-mini'`) or a `BaseChatModel` instance.   Additional fallback models to try in order if previous models fail   ### PII detection Detect and handle Personally Identifiable Information (PII) in conversations using configurable strategies. PII detection is useful for the following: \\* Healthcare and financial applications with compliance requirements. \\* Customer service agents that need to sanitize logs. \\* Any application handling sensitive user data. \\*\\*API reference:\\*\\* [`PIIMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.PIIMiddleware) ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import PIIMiddleware agent = create\\_agent( model=\"gpt-4o\", tools=[], middleware=[ PIIMiddleware(\"email\", strategy=\"redact\", apply\\_to\\_input=True), PIIMiddleware(\"credit\\_card\", strategy=\"mask\", apply\\_to\\_input=True), ], ) ``` #### Custom PII types You can create custom PII types by providing a `detector` parameter. This allows you to detect patterns specific to your use case beyond the built-in types. \\*\\*Three ways to create custom detectors:\\*\\* 1. \\*\\*Regex pattern string\\*\\* - Simple pattern matching 2. \\*\\*Custom function\\*\\* - Complex detection logic with validation ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import PIIMiddleware import re # Method 1: Regex pattern string agent1 = create\\_agent( model=\"gpt-4o\", tools=[], middleware=[ PIIMiddleware( \"api\\_key\", detector=r\"sk-[a-zA-Z0-9]{32}\", strategy=\"block\", ), ], ) # Method 2: Compiled regex pattern agent2 = create\\_agent( model=\"gpt-4o\", tools=[], middleware=[ PIIMiddleware( \"phone\\_number\", detector=re.compile(r\"\\+?\\d{1,3}[\\s.-]?\\d{3,4}[\\s.-]?\\d{4}\"), strategy=\"mask\", ), ], ) # Method 3: Custom detector function def detect\\_ssn(content: str) -> list[dict[str, str | int]]: \"\"\"Detect SSN with validation. Returns a list of dictionaries with 'text', 'start', and 'end' keys. \"\"\" import re matches = [] pattern = r\"\\d{3}-\\d{2}-\\d{4}\" for match in re.finditer(pattern, content): ssn = match.group(0) # Validate: first 3 digits shouldn't be 000, 666, or 900-999 first\\_three = int(ssn[:3]) if first\\_three not in [0, 666] and not (900 <= first\\_three <= 999): matches.append({ \"text\": ssn, \"start\": match.start(), \"end\": match.end(), }) return matches agent3 = create\\_agent( model=\"gpt-4o\", tools=[], middleware=[ PIIMiddleware( \"ssn\", detector=detect\\_ssn, strategy=\"hash\", ), ], ) ``` \\*\\*Custom detector function signature:\\*\\* The detector function must accept a string (content) and return matches: Returns a list of dictionaries with `text`, `start`, and `end` keys: ```python theme={null} def detector(content: str) -> list[dict[str, str | int]]: return [ {\"text\": \"matched\\_text\", \"start\": 0, \"end\": 12}, # ... more matches ] ```  For custom detectors: \\* Use regex strings for simple patterns \\* Use RegExp objects when you need flags (e.g., case-insensitive matching) \\* Use custom functions when you need validation logic beyond pattern matching \\* Custom functions give you full control over detection logic and can implement complex validation rules    Type of PII to detect. Can be a built-in type (`email`, `credit\\_card`, `ip`, `mac\\_address`, `url`) or a custom type name.   How to handle detected PII. Options: \\* `'block'` - Raise exception when detected \\* `'redact'` - Replace with `[REDACTED\\_TYPE]` \\* `'mask'` - Partially mask (e.g., `\\*\\*\\*\\*-\\*\\*\\*\\*-\\*\\*\\*\\*-1234`) \\* `'hash'` - Replace with deterministic hash   Custom detector function or regex pattern. If not provided, uses built-in detector for the PII type.   Check user messages before model call   Check AI messages after model call   Check tool result messages after execution   ### To-do list Equip agents with task planning and tracking capabilities for complex multi-step tasks. To-do lists are useful for the following: \\* Complex multi-step tasks requiring coordination across multiple tools. \\* Long-running operations where progress visibility is important.  This middleware automatically provides agents with a `write\\_todos` tool and system prompts to guide effective task planning.  \\*\\*API reference:\\*\\* [`TodoListMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.TodoListMiddleware) ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import TodoListMiddleware agent = create\\_agent( model=\"gpt-4o\", tools=[read\\_file, write\\_file, run\\_tests], middleware=[TodoListMiddleware()], ) ```   Custom system prompt for guiding todo usage. Uses built-in prompt if not specified.   Custom description for the `write\\_todos` tool. Uses built-in description if not specified.   ### LLM tool selector Use an LLM to intelligently select relevant tools before calling the main model. LLM tool selectors are useful for the following: \\* Agents with many tools (10+) where most aren't relevant per query. \\* Reducing token usage by filtering irrelevant tools. \\* Improving model focus and accuracy. This middleware uses structured output to ask an LLM which tools are most relevant for the current query. The structured output schema defines the available tool names and descriptions. Model providers often add this structured output information to the system prompt behind the scenes. \\*\\*API reference:\\*\\* [`LLMToolSelectorMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.LLMToolSelectorMiddleware) ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import LLMToolSelectorMiddleware agent = create\\_agent( model=\"gpt-4o\", tools=[tool1, tool2, tool3, tool4, tool5, ...], middleware=[ LLMToolSelectorMiddleware( model=\"gpt-4o-mini\", max\\_tools=3, always\\_include=[\"search\"], ), ], ) ```   Model for tool selection. Can be a model identifier string (e.g., `'openai:gpt-4o-mini'`) or a `BaseChatModel` instance. See [`init\\_chat\\_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat\\_models.init\\_chat\\_model\\(model\\)) for more information. Defaults to the agent's main model.   Instructions for the selection model. Uses built-in prompt if not specified.   Maximum number of tools to select. If the model selects more, only the first max\\\\_tools will be used. No limit if not specified.   Tool names to always include regardless of selection. These do not count against the max\\\\_tools limit.   ### Tool retry Automatically retry failed tool calls with configurable exponential backoff. Tool retry is useful for the following: \\* Handling transient failures in external API calls. \\* Improving reliability of network-dependent tools. \\* Building resilient agents that gracefully handle temporary errors. \\*\\*API reference:\\*\\* [`ToolRetryMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ToolRetryMiddleware) ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import ToolRetryMiddleware agent = create\\_agent( model=\"gpt-4o\", tools=[search\\_tool, database\\_tool], middleware=[ ToolRetryMiddleware( max\\_retries=3, backoff\\_factor=2.0, initial\\_delay=1.0, ), ], ) ```   Maximum number of retry attempts after the initial call (3 total attempts with default)   Optional list of tools or tool names to apply retry logic to. If `None`, applies to all tools.   Either a tuple of exception types to retry on, or a callable that takes an exception and returns `True` if it should be retried.   Behavior when all retries are exhausted. Options: \\* `'return\\_message'` - Return a `ToolMessage` with error details (allows LLM to handle failure) \\* `'raise'` - Re-raise the exception (stops agent execution) \\* Custom callable - Function that takes the exception and returns a string for the `ToolMessage` content   Multiplier for exponential backoff. Each retry waits `initial\\_delay \\* (backoff\\_factor \\*\\* retry\\_number)` seconds. Set to `0.0` for constant delay.   Initial delay in seconds before first retry   Maximum delay in seconds between retries (caps exponential backoff growth)   Whether to add random jitter (`\u00b125%`) to delay to avoid thundering herd    The middleware automatically retries failed tool calls with exponential backoff. \\*\\*Key configuration:\\*\\* \\* `max\\_retries` - Number of retry attempts (default: 2) \\* `backoff\\_factor` - Multiplier for exponential backoff (default: 2.0) \\* `initial\\_delay` - Starting delay in seconds (default: 1.0) \\* `max\\_delay` - Cap on delay growth (default: 60.0) \\* `jitter` - Add random variation (default: True) \\*\\*Failure handling:\\*\\* \\* `on\\_failure='return\\_message'` - Return error message \\* `on\\_failure='raise'` - Re-raise exception \\* Custom function - Function returning error message ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import ToolRetryMiddleware agent = create\\_agent( model=\"gpt-4o\", tools=[search\\_tool, database\\_tool, api\\_tool], middleware=[ ToolRetryMiddleware( max\\_retries=3, backoff\\_factor=2.0, initial\\_delay=1.0, max\\_delay=60.0, jitter=True, tools=[\"api\\_tool\"], retry\\_on=(ConnectionError, TimeoutError), on\\_failure=\"continue\", ), ], ) ```  ### Model retry Automatically retry failed model calls with configurable exponential backoff. Model retry is useful for the following: \\* Handling transient failures in model API calls. \\* Improving reliability of network-dependent model requests. \\* Building resilient agents that gracefully handle temporary model errors. \\*\\*API reference:\\*\\* [`ModelRetryMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ModelRetryMiddleware) ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import ModelRetryMiddleware agent = create\\_agent( model=\"gpt-4o\", tools=[search\\_tool, database\\_tool], middleware=[ ModelRetryMiddleware( max\\_retries=3, backoff\\_factor=2.0, initial\\_delay=1.0, ), ], ) ```   Maximum number of retry attempts after the initial call (3 total attempts with default)   Either a tuple of exception types to retry on, or a callable that takes an exception and returns `True` if it should be retried.   Behavior when all retries are exhausted. Options: \\* `'continue'` (default) - Return an `AIMessage` with error details, allowing the agent to potentially handle the failure gracefully \\* `'error'` - Re-raise the exception (stops agent execution) \\* Custom callable - Function that takes the exception and returns a string for the `AIMessage` content   Multiplier for exponential backoff. Each retry waits `initial\\_delay \\* (backoff\\_factor \\*\\* retry\\_number)` seconds. Set to `0.0` for constant delay.   Initial delay in seconds before first retry   Maximum delay in seconds between retries (caps exponential backoff growth)   Whether to add random jitter (`\u00b125%`) to delay to avoid thundering herd    The middleware automatically retries failed model calls with exponential backoff. ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import ModelRetryMiddleware # Basic usage with default settings (2 retries, exponential backoff) agent = create\\_agent( model=\"gpt-4o\", tools=[search\\_tool], middleware=[ModelRetryMiddleware()], ) # Custom exception filtering class TimeoutError(Exception): \"\"\"Custom exception for timeout errors.\"\"\" pass class ConnectionError(Exception): \"\"\"Custom exception for connection errors.\"\"\" pass # Retry specific exceptions only retry = ModelRetryMiddleware( max\\_retries=4, retry\\_on=(TimeoutError, ConnectionError), backoff\\_factor=1.5, ) def should\\_retry(error: Exception) -> bool: # Only retry on rate limit errors if isinstance(error, TimeoutError): return True # Or check for specific HTTP status codes if hasattr(error, \"status\\_code\"): return error.status\\_code in (429, 503) return False retry\\_with\\_filter = ModelRetryMiddleware( max\\_retries=3, retry\\_on=should\\_retry, ) # Return error message instead of raising retry\\_continue = ModelRetryMiddleware( max\\_retries=4, on\\_failure=\"continue\", # Return AIMessage with error instead of raising ) # Custom error message formatting def format\\_error(error: Exception) -> str: return f\"Model call failed: {error}. Please try again later.\" retry\\_with\\_formatter = ModelRetryMiddleware( max\\_retries=4, on\\_failure=format\\_error, ) # Constant backoff (no exponential growth) constant\\_backoff = ModelRetryMiddleware( max\\_retries=5, backoff\\_factor=0.0, # No exponential growth initial\\_delay=2.0, # Always wait 2 seconds ) # Raise exception on failure strict\\_retry = ModelRetryMiddleware( max\\_retries=2, on\\_failure=\"error\", # Re-raise exception instead of returning message ) ```  ### LLM tool emulator Emulate tool execution using an LLM for testing purposes, replacing actual tool calls with AI-generated responses. LLM tool emulators are useful for the following: \\* Testing agent behavior without executing real tools. \\* Developing agents when external tools are unavailable or expensive. \\* Prototyping agent workflows before implementing actual tools. \\*\\*API reference:\\*\\* [`LLMToolEmulator`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.LLMToolEmulator) ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import LLMToolEmulator agent = create\\_agent( model=\"gpt-4o\", tools=[get\\_weather, search\\_database, send\\_email], middleware=[ LLMToolEmulator(), # Emulate all tools ], ) ```   List of tool names (str) or BaseTool instances to emulate. If `None` (default), ALL tools will be emulated. If empty list `[]`, no tools will be emulated. If array with tool names/instances, only those tools will be emulated.   Model to use for generating emulated tool responses. Can be a model identifier string (e.g., `'anthropic:claude-sonnet-4-5-20250929'`) or a `BaseChatModel` instance. Defaults to the agent's model if not specified. See [`init\\_chat\\_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat\\_models.init\\_chat\\_model\\(model\\)) for more information.    The middleware uses an LLM to generate plausible responses for tool calls instead of executing the actual tools. ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import LLMToolEmulator from langchain.tools import tool @tool def get\\_weather(location: str) -> str: \"\"\"Get the current weather for a location.\"\"\" return f\"Weather in {location}\" @tool def send\\_email(to: str, subject: str, body: str) -> str: \"\"\"Send an email.\"\"\" return \"Email sent\" # Emulate all tools (default behavior) agent = create\\_agent( model=\"gpt-4o\", tools=[get\\_weather, send\\_email], middleware=[LLMToolEmulator()], ) # Emulate specific tools only agent2 = create\\_agent( model=\"gpt-4o\", tools=[get\\_weather, send\\_email], middleware=[LLMToolEmulator(tools=[\"get\\_weather\"])], ) # Use custom model for emulation agent4 = create\\_agent( model=\"gpt-4o\", tools=[get\\_weather, send\\_email], middleware=[LLMToolEmulator(model=\"claude-sonnet-4-5-20250929\")], ) ```  ### Context editing Manage conversation context by clearing older tool call outputs when token limits are reached, while preserving recent results. This helps keep context windows manageable in long conversations with many tool calls. Context editing is useful for the following: \\* Long conversations with many tool calls that exceed token limits \\* Reducing token costs by removing older tool outputs that are no longer relevant \\* Maintaining only the most recent N tool results in context \\*\\*API reference:\\*\\* [`ContextEditingMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ContextEditingMiddleware), [`ClearToolUsesEdit`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ClearToolUsesEdit) ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import ContextEditingMiddleware, ClearToolUsesEdit agent = create\\_agent( model=\"gpt-4o\", tools=[], middleware=[ ContextEditingMiddleware( edits=[ ClearToolUsesEdit( trigger=100000, keep=3, ), ], ), ], ) ```   List of [`ContextEdit`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ContextEdit) strategies to apply   Token counting method. Options: `'approximate'` or `'model'`  \\*\\*[`ClearToolUsesEdit`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ClearToolUsesEdit) options:\\*\\*  Token count that triggers the edit. When the conversation exceeds this token count, older tool outputs will be cleared.   Minimum number of tokens to reclaim when the edit runs. If set to 0, clears as much as needed.   Number of most recent tool results that must be preserved. These will never be cleared.   Whether to clear the originating tool call parameters on the AI message. When `True`, tool call arguments are replaced with empty objects.   List of tool names to exclude from clearing. These tools will never have their outputs cleared.   Placeholder text inserted for cleared tool outputs. This replaces the original tool message content.    The middleware applies context editing strategies when token limits are reached. The most common strategy is `ClearToolUsesEdit`, which clears older tool results while preserving recent ones. \\*\\*How it works:\\*\\* 1. Monitor token count in conversation 2. When threshold is reached, clear older tool outputs 3. Keep most recent N tool results 4. Optionally preserve tool call arguments for context ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import ContextEditingMiddleware, ClearToolUsesEdit agent = create\\_agent( model=\"gpt-4o\", tools=[search\\_tool, your\\_calculator\\_tool, database\\_tool], middleware=[ ContextEditingMiddleware( edits=[ ClearToolUsesEdit( trigger=2000, keep=3, clear\\_tool\\_inputs=False, exclude\\_tools=[], placeholder=\"[cleared]\", ), ], ), ], ) ```  ### Shell tool Expose a persistent shell session to agents for command execution. Shell tool middleware is useful for the following: \\* Agents that need to execute system commands \\* Development and deployment automation tasks \\* Testing and validation workflows \\* File system operations and script execution  \\*\\*Security consideration\\*\\*: Use appropriate execution policies (`HostExecutionPolicy`, `DockerExecutionPolicy`, or `CodexSandboxExecutionPolicy`) to match your deployment's security requirements.   \\*\\*Limitation\\*\\*: Persistent shell sessions do not currently work with interrupts (human-in-the-loop). We anticipate adding support for this in the future.  \\*\\*API reference:\\*\\* [`ShellToolMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ShellToolMiddleware) ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import ( ShellToolMiddleware, HostExecutionPolicy, ) agent = create\\_agent( model=\"gpt-4o\", tools=[search\\_tool], middleware=[ ShellToolMiddleware( workspace\\_root=\"/workspace\", execution\\_policy=HostExecutionPolicy(), ), ], ) ```   Base directory for the shell session. If omitted, a temporary directory is created when the agent starts and removed when it ends.   Optional commands executed sequentially after the session starts   Optional commands executed before the session shuts down   Execution policy controlling timeouts, output limits, and resource configuration. Options: \\* `HostExecutionPolicy` - Full host access (default); best for trusted environments where the agent already runs inside a container or VM \\* `DockerExecutionPolicy` - Launches a separate Docker container for each agent run, providing harder isolation \\* `CodexSandboxExecutionPolicy` - Reuses the Codex CLI sandbox for additional syscall/filesystem restrictions   Optional redaction rules to sanitize command output before returning it to the model   Optional override for the registered shell tool description   Optional shell executable (string) or argument sequence used to launch the persistent session. Defaults to `/bin/bash`.   Optional environment variables to supply to the shell session. Values are coerced to strings before command execution.    The middleware provides a single persistent shell session that agents can use to execute commands sequentially. \\*\\*Execution policies:\\*\\* \\* `HostExecutionPolicy` (default) - Native execution with full host access \\* `DockerExecutionPolicy` - Isolated Docker container execution \\* `CodexSandboxExecutionPolicy` - Sandboxed execution via Codex CLI ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import ( ShellToolMiddleware, HostExecutionPolicy, DockerExecutionPolicy, RedactionRule, ) # Basic shell tool with host execution agent = create\\_agent( model=\"gpt-4o\", tools=[search\\_tool], middleware=[ ShellToolMiddleware( workspace\\_root=\"/workspace\", execution\\_policy=HostExecutionPolicy(), ), ], ) # Docker isolation with startup commands agent\\_docker = create\\_agent( model=\"gpt-4o\", tools=[], middleware=[ ShellToolMiddleware( workspace\\_root=\"/workspace\", startup\\_commands=[\"pip install requests\", \"export PYTHONPATH=/workspace\"], execution\\_policy=DockerExecutionPolicy( image=\"python:3.11-slim\", command\\_timeout=60.0, ), ), ], ) # With output redaction agent\\_redacted = create\\_agent( model=\"gpt-4o\", tools=[], middleware=[ ShellToolMiddleware( workspace\\_root=\"/workspace\", redaction\\_rules=[ RedactionRule(pii\\_type=\"api\\_key\", detector=r\"sk-[a-zA-Z0-9]{32}\"), ], ), ], ) ```  ### File search Provide Glob and Grep search tools over filesystem files. File search middleware is useful for the following: \\* Code exploration and analysis \\* Finding files by name patterns \\* Searching code content with regex \\* Large codebases where file discovery is needed \\*\\*API reference:\\*\\* [`FilesystemFileSearchMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.FilesystemFileSearchMiddleware) ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import FilesystemFileSearchMiddleware agent = create\\_agent( model=\"gpt-4o\", tools=[], middleware=[ FilesystemFileSearchMiddleware( root\\_path=\"/workspace\", use\\_ripgrep=True, ), ], ) ```   Root directory to search. All file operations are relative to this path.   Whether to use ripgrep for search. Falls back to Python regex if ripgrep is unavailable.   Maximum file size to search in MB. Files larger than this are skipped.    The middleware adds two search tools to agents: \\*\\*Glob tool\\*\\* - Fast file pattern matching: \\* Supports patterns like `\\*\\*/\\*.py`, `src/\\*\\*/\\*.ts` \\* Returns matching file paths sorted by modification time \\*\\*Grep tool\\*\\* - Content search with regex: \\* Full regex syntax support \\* Filter by file patterns with `include` parameter \\* Three output modes: `files\\_with\\_matches`, `content`, `count` ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import FilesystemFileSearchMiddleware from langchain.messages import HumanMessage agent = create\\_agent( model=\"gpt-4o\", tools=[], middleware=[ FilesystemFileSearchMiddleware( root\\_path=\"/workspace\", use\\_ripgrep=True, max\\_file\\_size\\_mb=10, ), ], ) # Agent can now use glob\\_search and grep\\_search tools result = agent.invoke({ \"messages\": [HumanMessage(\"Find all Python files containing 'async def'\")] }) # The agent will use: # 1. glob\\_search(pattern=\"\\*\\*/\\*.py\") to find Python files # 2. grep\\_search(pattern=\"async def\", include=\"\\*.py\") to find async functions ```  ## Provider-specific middleware These middleware are optimized for specific LLM providers. See each provider's documentation for full details and examples.   Prompt caching, bash tool, text editor, memory, and file search middleware for Claude models.   Content moderation middleware for OpenAI models.   \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/middleware/built-in.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/middleware/custom.md",
      "title": null,
      "raw_content": "# Custom middleware Build custom middleware by implementing hooks that run at specific points in the agent execution flow. ## Hooks Middleware provides two styles of hooks to intercept agent execution:   Run sequentially at specific execution points.   Run around each model or tool call.   ### Node-style hooks Run sequentially at specific execution points. Use for logging, validation, and state updates. \\*\\*Available hooks:\\*\\* \\* `before\\_agent` - Before agent starts (once per invocation) \\* `before\\_model` - Before each model call \\* `after\\_model` - After each model response \\* `after\\_agent` - After agent completes (once per invocation) \\*\\*Example:\\*\\*   ```python theme={null} from langchain.agents.middleware import before\\_model, after\\_model, AgentState from langchain.messages import AIMessage from langgraph.runtime import Runtime from typing import Any @before\\_model(can\\_jump\\_to=[\"end\"]) def check\\_message\\_limit(state: AgentState, runtime: Runtime) -> dict[str, Any] | None: if len(state[\"messages\"]) >= 50: return { \"messages\": [AIMessage(\"Conversation limit reached.\")], \"jump\\_to\": \"end\" } return None @after\\_model def log\\_response(state: AgentState, runtime: Runtime) -> dict[str, Any] | None: print(f\"Model returned: {state['messages'][-1].content}\") return None ```   ```python theme={null} from langchain.agents.middleware import AgentMiddleware, AgentState, hook\\_config from langchain.messages import AIMessage from langgraph.runtime import Runtime from typing import Any class MessageLimitMiddleware(AgentMiddleware): def \\_\\_init\\_\\_(self, max\\_messages: int = 50): super().\\_\\_init\\_\\_() self.max\\_messages = max\\_messages @hook\\_config(can\\_jump\\_to=[\"end\"]) def before\\_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None: if len(state[\"messages\"]) == self.max\\_messages: return { \"messages\": [AIMessage(\"Conversation limit reached.\")], \"jump\\_to\": \"end\" } return None def after\\_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None: print(f\"Model returned: {state['messages'][-1].content}\") return None ```   ### Wrap-style hooks Intercept execution and control when the handler is called. Use for retries, caching, and transformation. You decide if the handler is called zero times (short-circuit), once (normal flow), or multiple times (retry logic). \\*\\*Available hooks:\\*\\* \\* `wrap\\_model\\_call` - Around each model call \\* `wrap\\_tool\\_call` - Around each tool call \\*\\*Example:\\*\\*   ```python theme={null} from langchain.agents.middleware import wrap\\_model\\_call, ModelRequest, ModelResponse from typing import Callable @wrap\\_model\\_call def retry\\_model( request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse], ) -> ModelResponse: for attempt in range(3): try: return handler(request) except Exception as e: if attempt == 2: raise print(f\"Retry {attempt + 1}/3 after error: {e}\") ```   ```python theme={null} from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse from typing import Callable class RetryMiddleware(AgentMiddleware): def \\_\\_init\\_\\_(self, max\\_retries: int = 3): super().\\_\\_init\\_\\_() self.max\\_retries = max\\_retries def wrap\\_model\\_call( self, request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse], ) -> ModelResponse: for attempt in range(self.max\\_retries): try: return handler(request) except Exception as e: if attempt == self.max\\_retries - 1: raise print(f\"Retry {attempt + 1}/{self.max\\_retries} after error: {e}\") ```   ## Create middleware You can create middleware in two ways:   Quick and simple for single-hook middleware. Use decorators to wrap individual functions.   More powerful for complex middleware with multiple hooks or configuration.   ### Decorator-based middleware Quick and simple for single-hook middleware. Use decorators to wrap individual functions. \\*\\*Available decorators:\\*\\* \\*\\*Node-style:\\*\\* \\* [`@before\\_agent`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.before\\_agent) - Runs before agent starts (once per invocation) \\* [`@before\\_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.before\\_model) - Runs before each model call \\* [`@after\\_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.after\\_model) - Runs after each model response \\* [`@after\\_agent`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.after\\_agent) - Runs after agent completes (once per invocation) \\*\\*Wrap-style:\\*\\* \\* [`@wrap\\_model\\_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap\\_model\\_call) - Wraps each model call with custom logic \\* [`@wrap\\_tool\\_call`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.wrap\\_tool\\_call) - Wraps each tool call with custom logic \\*\\*Convenience:\\*\\* \\* [`@dynamic\\_prompt`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.dynamic\\_prompt) - Generates dynamic system prompts \\*\\*Example:\\*\\* ```python theme={null} from langchain.agents.middleware import ( before\\_model, wrap\\_model\\_call, AgentState, ModelRequest, ModelResponse, ) from langchain.agents import create\\_agent from langgraph.runtime import Runtime from typing import Any, Callable @before\\_model def log\\_before\\_model(state: AgentState, runtime: Runtime) -> dict[str, Any] | None: print(f\"About to call model with {len(state['messages'])} messages\") return None @wrap\\_model\\_call def retry\\_model( request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse], ) -> ModelResponse: for attempt in range(3): try: return handler(request) except Exception as e: if attempt == 2: raise print(f\"Retry {attempt + 1}/3 after error: {e}\") agent = create\\_agent( model=\"gpt-4o\", middleware=[log\\_before\\_model, retry\\_model], tools=[...], ) ``` \\*\\*When to use decorators:\\*\\* \\* Single hook needed \\* No complex configuration \\* Quick prototyping ### Class-based middleware More powerful for complex middleware with multiple hooks or configuration. Use classes when you need to define both sync and async implementations for the same hook, or when you want to combine multiple hooks in a single middleware. \\*\\*Example:\\*\\* ```python theme={null} from langchain.agents.middleware import ( AgentMiddleware, AgentState, ModelRequest, ModelResponse, ) from langgraph.runtime import Runtime from typing import Any, Callable class LoggingMiddleware(AgentMiddleware): def before\\_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None: print(f\"About to call model with {len(state['messages'])} messages\") return None def after\\_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None: print(f\"Model returned: {state['messages'][-1].content}\") return None agent = create\\_agent( model=\"gpt-4o\", middleware=[LoggingMiddleware()], tools=[...], ) ``` \\*\\*When to use classes:\\*\\* \\* Defining both sync and async implementations for the same hook \\* Multiple hooks needed in a single middleware \\* Complex configuration required (e.g., configurable thresholds, custom models) \\* Reuse across projects with init-time configuration ## Custom state schema Middleware can extend the agent's state with custom properties.   ```python theme={null} from langchain.agents import create\\_agent from langchain.messages import HumanMessage from langchain.agents.middleware import AgentState, before\\_model, after\\_model from typing\\_extensions import NotRequired from typing import Any from langgraph.runtime import Runtime class CustomState(AgentState): model\\_call\\_count: NotRequired[int] user\\_id: NotRequired[str] @before\\_model(state\\_schema=CustomState, can\\_jump\\_to=[\"end\"]) def check\\_call\\_limit(state: CustomState, runtime: Runtime) -> dict[str, Any] | None: count = state.get(\"model\\_call\\_count\", 0) if count > 10: return {\"jump\\_to\": \"end\"} return None @after\\_model(state\\_schema=CustomState) def increment\\_counter(state: CustomState, runtime: Runtime) -> dict[str, Any] | None: return {\"model\\_call\\_count\": state.get(\"model\\_call\\_count\", 0) + 1} agent = create\\_agent( model=\"gpt-4o\", middleware=[check\\_call\\_limit, increment\\_counter], tools=[], ) # Invoke with custom state result = agent.invoke({ \"messages\": [HumanMessage(\"Hello\")], \"model\\_call\\_count\": 0, \"user\\_id\": \"user-123\", }) ```   ```python theme={null} from langchain.agents import create\\_agent from langchain.messages import HumanMessage from langchain.agents.middleware import AgentState, AgentMiddleware from typing\\_extensions import NotRequired from typing import Any class CustomState(AgentState): model\\_call\\_count: NotRequired[int] user\\_id: NotRequired[str] class CallCounterMiddleware(AgentMiddleware[CustomState]): state\\_schema = CustomState def before\\_model(self, state: CustomState, runtime) -> dict[str, Any] | None: count = state.get(\"model\\_call\\_count\", 0) if count > 10: return {\"jump\\_to\": \"end\"} return None def after\\_model(self, state: CustomState, runtime) -> dict[str, Any] | None: return {\"model\\_call\\_count\": state.get(\"model\\_call\\_count\", 0) + 1} agent = create\\_agent( model=\"gpt-4o\", middleware=[CallCounterMiddleware()], tools=[], ) # Invoke with custom state result = agent.invoke({ \"messages\": [HumanMessage(\"Hello\")], \"model\\_call\\_count\": 0, \"user\\_id\": \"user-123\", }) ```   ## Execution order When using multiple middleware, understand how they execute: ```python theme={null} agent = create\\_agent( model=\"gpt-4o\", middleware=[middleware1, middleware2, middleware3], tools=[...], ) ```  \\*\\*Before hooks run in order:\\*\\* 1. `middleware1.before\\_agent()` 2. `middleware2.before\\_agent()` 3. `middleware3.before\\_agent()` \\*\\*Agent loop starts\\*\\* 4. `middleware1.before\\_model()` 5. `middleware2.before\\_model()` 6. `middleware3.before\\_model()` \\*\\*Wrap hooks nest like function calls:\\*\\* 7. `middleware1.wrap\\_model\\_call()` \u2192 `middleware2.wrap\\_model\\_call()` \u2192 `middleware3.wrap\\_model\\_call()` \u2192 model \\*\\*After hooks run in reverse order:\\*\\* 8. `middleware3.after\\_model()` 9. `middleware2.after\\_model()` 10. `middleware1.after\\_model()` \\*\\*Agent loop ends\\*\\* 11. `middleware3.after\\_agent()` 12. `middleware2.after\\_agent()` 13. `middleware1.after\\_agent()`  \\*\\*Key rules:\\*\\* \\* `before\\_\\*` hooks: First to last \\* `after\\_\\*` hooks: Last to first (reverse) \\* `wrap\\_\\*` hooks: Nested (first middleware wraps all others) ## Agent jumps To exit early from middleware, return a dictionary with `jump\\_to`: \\*\\*Available jump targets:\\*\\* \\* `'end'`: Jump to the end of the agent execution (or the first `after\\_agent` hook) \\* `'tools'`: Jump to the tools node \\* `'model'`: Jump to the model node (or the first `before\\_model` hook)   ```python theme={null} from langchain.agents.middleware import after\\_model, hook\\_config, AgentState from langchain.messages import AIMessage from langgraph.runtime import Runtime from typing import Any @after\\_model @hook\\_config(can\\_jump\\_to=[\"end\"]) def check\\_for\\_blocked(state: AgentState, runtime: Runtime) -> dict[str, Any] | None: last\\_message = state[\"messages\"][-1] if \"BLOCKED\" in last\\_message.content: return { \"messages\": [AIMessage(\"I cannot respond to that request.\")], \"jump\\_to\": \"end\" } return None ```   ```python theme={null} from langchain.agents.middleware import AgentMiddleware, hook\\_config, AgentState from langchain.messages import AIMessage from langgraph.runtime import Runtime from typing import Any class BlockedContentMiddleware(AgentMiddleware): @hook\\_config(can\\_jump\\_to=[\"end\"]) def after\\_model(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None: last\\_message = state[\"messages\"][-1] if \"BLOCKED\" in last\\_message.content: return { \"messages\": [AIMessage(\"I cannot respond to that request.\")], \"jump\\_to\": \"end\" } return None ```   ## Best practices 1. Keep middleware focused - each should do one thing well 2. Handle errors gracefully - don't let middleware errors crash the agent 3. \\*\\*Use appropriate hook types\\*\\*: \\* Node-style for sequential logic (logging, validation) \\* Wrap-style for control flow (retry, fallback, caching) 4. Clearly document any custom state properties 5. Unit test middleware independently before integrating 6. Consider execution order - place critical middleware first in the list 7. Use built-in middleware when possible ## Examples ### Dynamic model selection   ```python theme={null} from langchain.agents.middleware import wrap\\_model\\_call, ModelRequest, ModelResponse from langchain.chat\\_models import init\\_chat\\_model from typing import Callable complex\\_model = init\\_chat\\_model(\"gpt-4o\") simple\\_model = init\\_chat\\_model(\"gpt-4o-mini\") @wrap\\_model\\_call def dynamic\\_model( request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse], ) -> ModelResponse: # Use different model based on conversation length if len(request.messages) > 10: model = complex\\_model else: model = simple\\_model return handler(request.override(model=model)) ```   ```python theme={null} from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse from langchain.chat\\_models import init\\_chat\\_model from typing import Callable complex\\_model = init\\_chat\\_model(\"gpt-4o\") simple\\_model = init\\_chat\\_model(\"gpt-4o-mini\") class DynamicModelMiddleware(AgentMiddleware): def wrap\\_model\\_call( self, request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse], ) -> ModelResponse: # Use different model based on conversation length if len(request.messages) > 10: model = complex\\_model else: model = simple\\_model return handler(request.override(model=model)) ```   ### Tool call monitoring   ```python theme={null} from langchain.agents.middleware import wrap\\_tool\\_call from langchain.tools.tool\\_node import ToolCallRequest from langchain.messages import ToolMessage from langgraph.types import Command from typing import Callable @wrap\\_tool\\_call def monitor\\_tool( request: ToolCallRequest, handler: Callable[[ToolCallRequest], ToolMessage | Command], ) -> ToolMessage | Command: print(f\"Executing tool: {request.tool\\_call['name']}\") print(f\"Arguments: {request.tool\\_call['args']}\") try: result = handler(request) print(f\"Tool completed successfully\") return result except Exception as e: print(f\"Tool failed: {e}\") raise ```   ```python theme={null} from langchain.tools.tool\\_node import ToolCallRequest from langchain.agents.middleware import AgentMiddleware from langchain.messages import ToolMessage from langgraph.types import Command from typing import Callable class ToolMonitoringMiddleware(AgentMiddleware): def wrap\\_tool\\_call( self, request: ToolCallRequest, handler: Callable[[ToolCallRequest], ToolMessage | Command], ) -> ToolMessage | Command: print(f\"Executing tool: {request.tool\\_call['name']}\") print(f\"Arguments: {request.tool\\_call['args']}\") try: result = handler(request) print(f\"Tool completed successfully\") return result except Exception as e: print(f\"Tool failed: {e}\") raise ```   ### Dynamically selecting tools Select relevant tools at runtime to improve performance and accuracy. \\*\\*Benefits:\\*\\* \\* \\*\\*Shorter prompts\\*\\* - Reduce complexity by exposing only relevant tools \\* \\*\\*Better accuracy\\*\\* - Models choose correctly from fewer options \\* \\*\\*Permission control\\*\\* - Dynamically filter tools based on user access   ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import wrap\\_model\\_call, ModelRequest, ModelResponse from typing import Callable @wrap\\_model\\_call def select\\_tools( request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse], ) -> ModelResponse: \"\"\"Middleware to select relevant tools based on state/context.\"\"\" # Select a small, relevant subset of tools based on state/context relevant\\_tools = select\\_relevant\\_tools(request.state, request.runtime) return handler(request.override(tools=relevant\\_tools)) agent = create\\_agent( model=\"gpt-4o\", tools=all\\_tools, # All available tools need to be registered upfront middleware=[select\\_tools], ) ```   ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse from typing import Callable class ToolSelectorMiddleware(AgentMiddleware): def wrap\\_model\\_call( self, request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse], ) -> ModelResponse: \"\"\"Middleware to select relevant tools based on state/context.\"\"\" # Select a small, relevant subset of tools based on state/context relevant\\_tools = select\\_relevant\\_tools(request.state, request.runtime) return handler(request.override(tools=relevant\\_tools)) agent = create\\_agent( model=\"gpt-4o\", tools=all\\_tools, # All available tools need to be registered upfront middleware=[ToolSelectorMiddleware()], ) ```   ### Working with system messages Modify system messages in middleware using the `system\\_message` field on `ModelRequest`. The `system\\_message` field contains a [`SystemMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.SystemMessage) object (even if the agent was created with a string `system\\_prompt`). \\*\\*Example: Adding context to system message\\*\\*   ```python theme={null} from langchain.agents.middleware import wrap\\_model\\_call, ModelRequest, ModelResponse from langchain.messages import SystemMessage from typing import Callable @wrap\\_model\\_call def add\\_context( request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse], ) -> ModelResponse: # Always work with content blocks new\\_content = list(request.system\\_message.content\\_blocks) + [ {\"type\": \"text\", \"text\": \"Additional context.\"} ] new\\_system\\_message = SystemMessage(content=new\\_content) return handler(request.override(system\\_message=new\\_system\\_message)) ```   ```python theme={null} from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse from langchain.messages import SystemMessage from typing import Callable class ContextMiddleware(AgentMiddleware): def wrap\\_model\\_call( self, request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse], ) -> ModelResponse: # Always work with content blocks new\\_content = list(request.system\\_message.content\\_blocks) + [ {\"type\": \"text\", \"text\": \"Additional context.\"} ] new\\_system\\_message = SystemMessage(content=new\\_content) return handler(request.override(system\\_message=new\\_system\\_message)) ```   \\*\\*Example: Working with cache control (Anthropic)\\*\\* When working with Anthropic models, you can use structured content blocks with cache control directives to cache large system prompts:   ```python theme={null} from langchain.agents.middleware import wrap\\_model\\_call, ModelRequest, ModelResponse from langchain.messages import SystemMessage from typing import Callable @wrap\\_model\\_call def add\\_cached\\_context( request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse], ) -> ModelResponse: # Always work with content blocks new\\_content = list(request.system\\_message.content\\_blocks) + [ { \"type\": \"text\", \"text\": \"Here is a large document to analyze:\\n\\n...\", # content up until this point is cached \"cache\\_control\": {\"type\": \"ephemeral\"} } ] new\\_system\\_message = SystemMessage(content=new\\_content) return handler(request.override(system\\_message=new\\_system\\_message)) ```   ```python theme={null} from langchain.agents.middleware import AgentMiddleware, ModelRequest, ModelResponse from langchain.messages import SystemMessage from typing import Callable class CachedContextMiddleware(AgentMiddleware): def wrap\\_model\\_call( self, request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse], ) -> ModelResponse: # Always work with content blocks new\\_content = list(request.system\\_message.content\\_blocks) + [ { \"type\": \"text\", \"text\": \"Here is a large document to analyze:\\n\\n...\", \"cache\\_control\": {\"type\": \"ephemeral\"} # This content will be cached } ] new\\_system\\_message = SystemMessage(content=new\\_content) return handler(request.override(system\\_message=new\\_system\\_message)) ```   \\*\\*Notes:\\*\\* \\* `ModelRequest.system\\_message` is always a [`SystemMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.SystemMessage) object, even if the agent was created with `system\\_prompt=\"string\"` \\* Use `SystemMessage.content\\_blocks` to access content as a list of blocks, regardless of whether the original content was a string or list \\* When modifying system messages, use `content\\_blocks` and append new blocks to preserve existing structure \\* You can pass [`SystemMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.SystemMessage) objects directly to `create\\_agent`'s `system\\_prompt` parameter for advanced use cases like cache control ## Additional resources \\* [Middleware API reference](https://reference.langchain.com/python/langchain/middleware/) \\* [Built-in middleware](/oss/python/langchain/middleware/built-in) \\* [Testing agents](/oss/python/langchain/test) \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/middleware/custom.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/middleware/overview.md",
      "title": null,
      "raw_content": "# Overview > Control and customize agent execution at every step Middleware provides a way to more tightly control what happens inside the agent. Middleware is useful for the following: \\* Tracking agent behavior with logging, analytics, and debugging. \\* Transforming prompts, [tool selection](/oss/python/langchain/middleware/built-in#llm-tool-selector), and output formatting. \\* Adding [retries](/oss/python/langchain/middleware/built-in#tool-retry), [fallbacks](/oss/python/langchain/middleware/built-in#model-fallback), and early termination logic. \\* Applying [rate limits](/oss/python/langchain/middleware/built-in#model-call-limit), guardrails, and [PII detection](/oss/python/langchain/middleware/built-in#pii-detection). Add middleware by passing them to [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent): ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import SummarizationMiddleware, HumanInTheLoopMiddleware agent = create\\_agent( model=\"gpt-4o\", tools=[...], middleware=[ SummarizationMiddleware(...), HumanInTheLoopMiddleware(...) ], ) ``` ## The agent loop The core agent loop involves calling a model, letting it choose tools to execute, and then finishing when it calls no more tools: Middleware exposes hooks before and after each of those steps: ## Additional resources   Explore built-in middleware for common use cases.   Build your own middleware with hooks and decorators.   Complete API reference for middleware.   Test your agents with LangSmith.   \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/middleware/overview.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/models.md",
      "title": null,
      "raw_content": "# Models [LLMs](https://en.wikipedia.org/wiki/Large\\_language\\_model) are powerful AI tools that can interpret and generate text like humans. They're versatile enough to write content, translate languages, summarize, and answer questions without needing specialized training for each task. In addition to text generation, many models support: \\* [Tool calling](#tool-calling) - calling external tools (like databases queries or API calls) and use results in their responses. \\* [Structured output](#structured-output) - where the model's response is constrained to follow a defined format. \\* [Multimodality](#multimodal) - process and return data other than text, such as images, audio, and video. \\* [Reasoning](#reasoning) - models perform multi-step reasoning to arrive at a conclusion. Models are the reasoning engine of [agents](/oss/python/langchain/agents). They drive the agent's decision-making process, determining which tools to call, how to interpret results, and when to provide a final answer. The quality and capabilities of the model you choose directly impact your agent's baseline reliability and performance. Different models excel at different tasks - some are better at following complex instructions, others at structured reasoning, and some support larger context windows for handling more information. LangChain's standard model interfaces give you access to many different provider integrations, which makes it easy to experiment with and switch between models to find the best fit for your use case.  For provider-specific integration information and capabilities, see the provider's [chat model page](/oss/python/integrations/chat).  ## Basic usage Models can be utilized in two ways: 1. \\*\\*With agents\\*\\* - Models can be dynamically specified when creating an [agent](/oss/python/langchain/agents#model). 2. \\*\\*Standalone\\*\\* - Models can be called directly (outside of the agent loop) for tasks like text generation, classification, or extraction without the need for an agent framework. The same model interface works in both contexts, which gives you the flexibility to start simple and scale up to more complex agent-based workflows as needed. ### Initialize a model The easiest way to get started with a standalone model in LangChain is to use [`init\\_chat\\_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat\\_models.init\\_chat\\_model) to initialize one from a [chat model provider](/oss/python/integrations/chat) of your choice (examples below):   \ud83d\udc49 Read the [OpenAI chat model integration docs](/oss/python/integrations/chat/openai/) ```shell theme={null} pip install -U \"langchain[openai]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"OPENAI\\_API\\_KEY\"] = \"sk-...\" model = init\\_chat\\_model(\"gpt-4.1\") ``` ```python Model Class theme={null} import os from langchain\\_openai import ChatOpenAI os.environ[\"OPENAI\\_API\\_KEY\"] = \"sk-...\" model = ChatOpenAI(model=\"gpt-4.1\") ```    \ud83d\udc49 Read the [Anthropic chat model integration docs](/oss/python/integrations/chat/anthropic/) ```shell theme={null} pip install -U \"langchain[anthropic]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"ANTHROPIC\\_API\\_KEY\"] = \"sk-...\" model = init\\_chat\\_model(\"claude-sonnet-4-5-20250929\") ``` ```python Model Class theme={null} import os from langchain\\_anthropic import ChatAnthropic os.environ[\"ANTHROPIC\\_API\\_KEY\"] = \"sk-...\" model = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\") ```    \ud83d\udc49 Read the [Azure chat model integration docs](/oss/python/integrations/chat/azure\\_chat\\_openai/) ```shell theme={null} pip install -U \"langchain[openai]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"AZURE\\_OPENAI\\_API\\_KEY\"] = \"...\" os.environ[\"AZURE\\_OPENAI\\_ENDPOINT\"] = \"...\" os.environ[\"OPENAI\\_API\\_VERSION\"] = \"2025-03-01-preview\" model = init\\_chat\\_model( \"azure\\_openai:gpt-4.1\", azure\\_deployment=os.environ[\"AZURE\\_OPENAI\\_DEPLOYMENT\\_NAME\"], ) ``` ```python Model Class theme={null} import os from langchain\\_openai import AzureChatOpenAI os.environ[\"AZURE\\_OPENAI\\_API\\_KEY\"] = \"...\" os.environ[\"AZURE\\_OPENAI\\_ENDPOINT\"] = \"...\" os.environ[\"OPENAI\\_API\\_VERSION\"] = \"2025-03-01-preview\" model = AzureChatOpenAI( model=\"gpt-4.1\", azure\\_deployment=os.environ[\"AZURE\\_OPENAI\\_DEPLOYMENT\\_NAME\"] ) ```    \ud83d\udc49 Read the [Google GenAI chat model integration docs](/oss/python/integrations/chat/google\\_generative\\_ai/) ```shell theme={null} pip install -U \"langchain[google-genai]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"GOOGLE\\_API\\_KEY\"] = \"...\" model = init\\_chat\\_model(\"google\\_genai:gemini-2.5-flash-lite\") ``` ```python Model Class theme={null} import os from langchain\\_google\\_genai import ChatGoogleGenerativeAI os.environ[\"GOOGLE\\_API\\_KEY\"] = \"...\" model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\") ```    \ud83d\udc49 Read the [AWS Bedrock chat model integration docs](/oss/python/integrations/chat/bedrock/) ```shell theme={null} pip install -U \"langchain[aws]\" ```  ```python init\\_chat\\_model theme={null} from langchain.chat\\_models import init\\_chat\\_model # Follow the steps here to configure your credentials: # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html model = init\\_chat\\_model( \"anthropic.claude-3-5-sonnet-20240620-v1:0\", model\\_provider=\"bedrock\\_converse\", ) ``` ```python Model Class theme={null} from langchain\\_aws import ChatBedrock model = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\") ```   \ud83d\udc49 Read the [HuggingFace chat model integration docs](/oss/python/integrations/chat/huggingface/) ```shell theme={null} pip install -U \"langchain[huggingface]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"HUGGINGFACEHUB\\_API\\_TOKEN\"] = \"hf\\_...\" model = init\\_chat\\_model( \"microsoft/Phi-3-mini-4k-instruct\", model\\_provider=\"huggingface\", temperature=0.7, max\\_tokens=1024, ) ``` ```python Model Class theme={null} import os from langchain\\_huggingface import ChatHuggingFace, HuggingFaceEndpoint os.environ[\"HUGGINGFACEHUB\\_API\\_TOKEN\"] = \"hf\\_...\" llm = HuggingFaceEndpoint( repo\\_id=\"microsoft/Phi-3-mini-4k-instruct\", temperature=0.7, max\\_length=1024, ) model = ChatHuggingFace(llm=llm) ```     ```python theme={null} response = model.invoke(\"Why do parrots talk?\") ``` See [`init\\_chat\\_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat\\_models.init\\_chat\\_model) for more detail, including information on how to pass model [parameters](#parameters). ### Key methods  The model takes messages as input and outputs messages after generating a complete response.   Invoke the model, but stream the output as it is generated in real-time.   Send multiple requests to a model in a batch for more efficient processing.   In addition to chat models, LangChain provides support for other adjacent technologies, such as embedding models and vector stores. See the [integrations page](/oss/python/integrations/providers/overview) for details.  ## Parameters A chat model takes parameters that can be used to configure its behavior. The full set of supported parameters varies by model and provider, but standard ones include:  The name or identifier of the specific model you want to use with a provider. You can also specify both the model and its provider in a single argument using the '{model\\_provider}:{model}' format, for example, 'openai:o1'.   The key required for authenticating with the model's provider. This is usually issued when you sign up for access to the model. Often accessed by setting an environment variable.   Controls the randomness of the model's output. A higher number makes responses more creative; lower ones make them more deterministic.   Limits the total number of tokens in the response, effectively controlling how long the output can be.   The maximum time (in seconds) to wait for a response from the model before canceling the request.   The maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.  Using [`init\\_chat\\_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat\\_models.init\\_chat\\_model), pass these parameters as inline `\\*\\*kwargs`: ```python Initialize using model parameters theme={null} model = init\\_chat\\_model( \"claude-sonnet-4-5-20250929\", # Kwargs passed to the model: temperature=0.7, timeout=30, max\\_tokens=1000, ) ```  Each chat model integration may have additional params used to control provider-specific functionality. For example, [`ChatOpenAI`](https://reference.langchain.com/python/integrations/langchain\\_openai/ChatOpenAI) has `use\\_responses\\_api` to dictate whether to use the OpenAI Responses or Completions API. To find all the parameters supported by a given chat model, head to the [chat model integrations](/oss/python/integrations/chat) page.  \\*\\*\\* ## Invocation A chat model must be invoked to generate an output. There are three primary invocation methods, each suited to different use cases. ### Invoke The most straightforward way to call a model is to use [`invoke()`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.language\\_models.chat\\_models.BaseChatModel.invoke) with a single message or a list of messages. ```python Single message theme={null} response = model.invoke(\"Why do parrots have colorful feathers?\") print(response) ``` A list of messages can be provided to a chat model to represent conversation history. Each message has a role that models use to indicate who sent the message in the conversation. See the [messages](/oss/python/langchain/messages) guide for more detail on roles, types, and content. ```python Dictionary format theme={null} conversation = [ {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates English to French.\"}, {\"role\": \"user\", \"content\": \"Translate: I love programming.\"}, {\"role\": \"assistant\", \"content\": \"J'adore la programmation.\"}, {\"role\": \"user\", \"content\": \"Translate: I love building applications.\"} ] response = model.invoke(conversation) print(response) # AIMessage(\"J'adore cr\u00e9er des applications.\") ``` ```python Message objects theme={null} from langchain.messages import HumanMessage, AIMessage, SystemMessage conversation = [ SystemMessage(\"You are a helpful assistant that translates English to French.\"), HumanMessage(\"Translate: I love programming.\"), AIMessage(\"J'adore la programmation.\"), HumanMessage(\"Translate: I love building applications.\") ] response = model.invoke(conversation) print(response) # AIMessage(\"J'adore cr\u00e9er des applications.\") ```  If the return type of your invocation is a string, ensure that you are using a chat model as opposed to a LLM. Legacy, text-completion LLMs return strings directly. LangChain chat models are prefixed with \"Chat\", e.g., [`ChatOpenAI`](https://reference.langchain.com/python/integrations/langchain\\_openai/ChatOpenAI)(/oss/integrations/chat/openai).  ### Stream Most models can stream their output content while it is being generated. By displaying output progressively, streaming significantly improves user experience, particularly for longer responses. Calling [`stream()`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.language\\_models.chat\\_models.BaseChatModel.stream) returns an iterator that yields output chunks as they are produced. You can use a loop to process each chunk in real-time:  ```python Basic text streaming theme={null} for chunk in model.stream(\"Why do parrots have colorful feathers?\"): print(chunk.text, end=\"|\", flush=True) ``` ```python Stream tool calls, reasoning, and other content theme={null} for chunk in model.stream(\"What color is the sky?\"): for block in chunk.content\\_blocks: if block[\"type\"] == \"reasoning\" and (reasoning := block.get(\"reasoning\")): print(f\"Reasoning: {reasoning}\") elif block[\"type\"] == \"tool\\_call\\_chunk\": print(f\"Tool call chunk: {block}\") elif block[\"type\"] == \"text\": print(block[\"text\"]) else: ... ```  As opposed to [`invoke()`](#invoke), which returns a single [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) after the model has finished generating its full response, `stream()` returns multiple [`AIMessageChunk`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessageChunk) objects, each containing a portion of the output text. Importantly, each chunk in a stream is designed to be gathered into a full message via summation: ```python Construct an AIMessage theme={null} full = None # None | AIMessageChunk for chunk in model.stream(\"What color is the sky?\"): full = chunk if full is None else full + chunk print(full.text) # The # The sky # The sky is # The sky is typically # The sky is typically blue # ... print(full.content\\_blocks) # [{\"type\": \"text\", \"text\": \"The sky is typically blue...\"}] ``` The resulting message can be treated the same as a message that was generated with [`invoke()`](#invoke) \u2013 for example, it can be aggregated into a message history and passed back to the model as conversational context.  Streaming only works if all steps in the program know how to process a stream of chunks. For instance, an application that isn't streaming-capable would be one that needs to store the entire output in memory before it can be processed.    LangChain chat models can also stream semantic events using `astream\\_events()`. This simplifies filtering based on event types and other metadata, and will aggregate the full message in the background. See below for an example. ```python theme={null} async for event in model.astream\\_events(\"Hello\"): if event[\"event\"] == \"on\\_chat\\_model\\_start\": print(f\"Input: {event['data']['input']}\") elif event[\"event\"] == \"on\\_chat\\_model\\_stream\": print(f\"Token: {event['data']['chunk'].text}\") elif event[\"event\"] == \"on\\_chat\\_model\\_end\": print(f\"Full message: {event['data']['output'].text}\") else: pass ``` ```txt theme={null} Input: Hello Token: Hi Token: there Token: ! Token: How Token: can Token: I ... Full message: Hi there! How can I help today? ```  See the [`astream\\_events()`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.language\\_models.chat\\_models.BaseChatModel.astream\\_events) reference for event types and other details.    LangChain simplifies streaming from chat models by automatically enabling streaming mode in certain cases, even when you're not explicitly calling the streaming methods. This is particularly useful when you use the non-streaming invoke method but still want to stream the entire application, including intermediate results from the chat model. In [LangGraph agents](/oss/python/langchain/agents), for example, you can call `model.invoke()` within nodes, but LangChain will automatically delegate to streaming if running in a streaming mode. #### How it works When you `invoke()` a chat model, LangChain will automatically switch to an internal streaming mode if it detects that you are trying to stream the overall application. The result of the invocation will be the same as far as the code that was using invoke is concerned; however, while the chat model is being streamed, LangChain will take care of invoking [`on\\_llm\\_new\\_token`](https://reference.langchain.com/python/langchain\\_core/callbacks/#langchain\\_core.callbacks.base.AsyncCallbackHandler.on\\_llm\\_new\\_token) events in LangChain's callback system. Callback events allow LangGraph `stream()` and `astream\\_events()` to surface the chat model's output in real-time.   ### Batch Batching a collection of independent requests to a model can significantly improve performance and reduce costs, as the processing can be done in parallel: ```python Batch theme={null} responses = model.batch([ \"Why do parrots have colorful feathers?\", \"How do airplanes fly?\", \"What is quantum computing?\" ]) for response in responses: print(response) ```  This section describes a chat model method [`batch()`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.language\\_models.chat\\_models.BaseChatModel.batch), which parallelizes model calls client-side. It is \\*\\*distinct\\*\\* from batch APIs supported by inference providers, such as [OpenAI](https://platform.openai.com/docs/guides/batch) or [Anthropic](https://platform.claude.com/docs/en/build-with-claude/batch-processing#message-batches-api).  By default, [`batch()`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.language\\_models.chat\\_models.BaseChatModel.batch) will only return the final output for the entire batch. If you want to receive the output for each individual input as it finishes generating, you can stream results with [`batch\\_as\\_completed()`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.language\\_models.chat\\_models.BaseChatModel.batch\\_as\\_completed): ```python Yield batch responses upon completion theme={null} for response in model.batch\\_as\\_completed([ \"Why do parrots have colorful feathers?\", \"How do airplanes fly?\", \"What is quantum computing?\" ]): print(response) ```  When using [`batch\\_as\\_completed()`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.language\\_models.chat\\_models.BaseChatModel.batch\\_as\\_completed), results may arrive out of order. Each includes the input index for matching to reconstruct the original order as needed.   When processing a large number of inputs using [`batch()`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.language\\_models.chat\\_models.BaseChatModel.batch) or [`batch\\_as\\_completed()`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.language\\_models.chat\\_models.BaseChatModel.batch\\_as\\_completed), you may want to control the maximum number of parallel calls. This can be done by setting the [`max\\_concurrency`](https://reference.langchain.com/python/langchain\\_core/runnables/#langchain\\_core.runnables.RunnableConfig.max\\_concurrency) attribute in the [`RunnableConfig`](https://reference.langchain.com/python/langchain\\_core/runnables/#langchain\\_core.runnables.RunnableConfig) dictionary. ```python Batch with max concurrency theme={null} model.batch( list\\_of\\_inputs, config={ 'max\\_concurrency': 5, # Limit to 5 parallel calls } ) ``` See the [`RunnableConfig`](https://reference.langchain.com/python/langchain\\_core/runnables/#langchain\\_core.runnables.RunnableConfig) reference for a full list of supported attributes.  For more details on batching, see the [reference](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.language\\_models.chat\\_models.BaseChatModel.batch). \\*\\*\\* ## Tool calling Models can request to call tools that perform tasks such as fetching data from a database, searching the web, or running code. Tools are pairings of: 1. A schema, including the name of the tool, a description, and/or argument definitions (often a JSON schema) 2. A function or coroutine to execute.  You may hear the term \"function calling\". We use this interchangeably with \"tool calling\".  Here's the basic tool calling flow between a user and a model: ```mermaid theme={null} sequenceDiagram participant U as User participant M as Model participant T as Tools U->>M: \"What's the weather in SF and NYC?\" M->>M: Analyze request & decide tools needed par Parallel Tool Calls M->>T: get\\_weather(\"San Francisco\") M->>T: get\\_weather(\"New York\") end par Tool Execution T-->>M: SF weather data T-->>M: NYC weather data end M->>M: Process results & generate response M->>U: \"SF: 72\u00b0F sunny, NYC: 68\u00b0F cloudy\" ``` To make tools that you have defined available for use by a model, you must bind them using [`bind\\_tools`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.language\\_models.chat\\_models.BaseChatModel.bind\\_tools). In subsequent invocations, the model can choose to call any of the bound tools as needed. Some model providers offer built-in tools that can be enabled via model or invocation parameters (e.g. [`ChatOpenAI`](/oss/python/integrations/chat/openai), [`ChatAnthropic`](/oss/python/integrations/chat/anthropic)). Check the respective [provider reference](/oss/python/integrations/providers/overview) for details.  See the [tools guide](/oss/python/langchain/tools) for details and other options for creating tools.  ```python Binding user tools theme={null} from langchain.tools import tool @tool def get\\_weather(location: str) -> str: \"\"\"Get the weather at a location.\"\"\" return f\"It's sunny in {location}.\" model\\_with\\_tools = model.bind\\_tools([get\\_weather]) # [!code highlight] response = model\\_with\\_tools.invoke(\"What's the weather like in Boston?\") for tool\\_call in response.tool\\_calls: # View tool calls made by the model print(f\"Tool: {tool\\_call['name']}\") print(f\"Args: {tool\\_call['args']}\") ``` When binding user-defined tools, the model's response includes a \\*\\*request\\*\\* to execute a tool. When using a model separately from an [agent](/oss/python/langchain/agents), it is up to you to execute the requested tool and return the result back to the model for use in subsequent reasoning. When using an [agent](/oss/python/langchain/agents), the agent loop will handle the tool execution loop for you. Below, we show some common ways you can use tool calling.   When a model returns tool calls, you need to execute the tools and pass the results back to the model. This creates a conversation loop where the model can use tool results to generate its final response. LangChain includes [agent](/oss/python/langchain/agents) abstractions that handle this orchestration for you. Here's a simple example of how to do this: ```python Tool execution loop theme={null} # Bind (potentially multiple) tools to the model model\\_with\\_tools = model.bind\\_tools([get\\_weather]) # Step 1: Model generates tool calls messages = [{\"role\": \"user\", \"content\": \"What's the weather in Boston?\"}] ai\\_msg = model\\_with\\_tools.invoke(messages) messages.append(ai\\_msg) # Step 2: Execute tools and collect results for tool\\_call in ai\\_msg.tool\\_calls: # Execute the tool with the generated arguments tool\\_result = get\\_weather.invoke(tool\\_call) messages.append(tool\\_result) # Step 3: Pass results back to model for final response final\\_response = model\\_with\\_tools.invoke(messages) print(final\\_response.text) # \"The current weather in Boston is 72\u00b0F and sunny.\" ``` Each [`ToolMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage) returned by the tool includes a `tool\\_call\\_id` that matches the original tool call, helping the model correlate results with requests.   By default, the model has the freedom to choose which bound tool to use based on the user's input. However, you might want to force choosing a tool, ensuring the model uses either a particular tool or \\*\\*any\\*\\* tool from a given list:  ```python Force use of any tool theme={null} model\\_with\\_tools = model.bind\\_tools([tool\\_1], tool\\_choice=\"any\") ``` ```python Force use of specific tools theme={null} model\\_with\\_tools = model.bind\\_tools([tool\\_1], tool\\_choice=\"tool\\_1\") ```    Many models support calling multiple tools in parallel when appropriate. This allows the model to gather information from different sources simultaneously. ```python Parallel tool calls theme={null} model\\_with\\_tools = model.bind\\_tools([get\\_weather]) response = model\\_with\\_tools.invoke( \"What's the weather in Boston and Tokyo?\" ) # The model may generate multiple tool calls print(response.tool\\_calls) # [ # {'name': 'get\\_weather', 'args': {'location': 'Boston'}, 'id': 'call\\_1'}, # {'name': 'get\\_weather', 'args': {'location': 'Tokyo'}, 'id': 'call\\_2'}, # ] # Execute all tools (can be done in parallel with async) results = [] for tool\\_call in response.tool\\_calls: if tool\\_call['name'] == 'get\\_weather': result = get\\_weather.invoke(tool\\_call) ... results.append(result) ``` The model intelligently determines when parallel execution is appropriate based on the independence of the requested operations.  Most models supporting tool calling enable parallel tool calls by default. Some (including [OpenAI](/oss/python/integrations/chat/openai) and [Anthropic](/oss/python/integrations/chat/anthropic)) allow you to disable this feature. To do this, set `parallel\\_tool\\_calls=False`: ```python theme={null} model.bind\\_tools([get\\_weather], parallel\\_tool\\_calls=False) ```    When streaming responses, tool calls are progressively built through [`ToolCallChunk`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolCallChunk). This allows you to see tool calls as they're being generated rather than waiting for the complete response. ```python Streaming tool calls theme={null} for chunk in model\\_with\\_tools.stream( \"What's the weather in Boston and Tokyo?\" ): # Tool call chunks arrive progressively for tool\\_chunk in chunk.tool\\_call\\_chunks: if name := tool\\_chunk.get(\"name\"): print(f\"Tool: {name}\") if id\\_ := tool\\_chunk.get(\"id\"): print(f\"ID: {id\\_}\") if args := tool\\_chunk.get(\"args\"): print(f\"Args: {args}\") # Output: # Tool: get\\_weather # ID: call\\_SvMlU1TVIZugrFLckFE2ceRE # Args: {\"lo # Args: catio # Args: n\": \"B # Args: osto # Args: n\"} # Tool: get\\_weather # ID: call\\_QMZdy6qInx13oWKE7KhuhOLR # Args: {\"lo # Args: catio # Args: n\": \"T # Args: okyo # Args: \"} ``` You can accumulate chunks to build complete tool calls: ```python Accumulate tool calls theme={null} gathered = None for chunk in model\\_with\\_tools.stream(\"What's the weather in Boston?\"): gathered = chunk if gathered is None else gathered + chunk print(gathered.tool\\_calls) ```   \\*\\*\\* ## Structured output Models can be requested to provide their response in a format matching a given schema. This is useful for ensuring the output can be easily parsed and used in subsequent processing. LangChain supports multiple schema types and methods for enforcing structured output.   [Pydantic models](https://docs.pydantic.dev/latest/concepts/models/#basic-model-usage) provide the richest feature set with field validation, descriptions, and nested structures. ```python theme={null} from pydantic import BaseModel, Field class Movie(BaseModel): \"\"\"A movie with details.\"\"\" title: str = Field(..., description=\"The title of the movie\") year: int = Field(..., description=\"The year the movie was released\") director: str = Field(..., description=\"The director of the movie\") rating: float = Field(..., description=\"The movie's rating out of 10\") model\\_with\\_structure = model.with\\_structured\\_output(Movie) response = model\\_with\\_structure.invoke(\"Provide details about the movie Inception\") print(response) # Movie(title=\"Inception\", year=2010, director=\"Christopher Nolan\", rating=8.8) ```   `TypedDict` provides a simpler alternative using Python's built-in typing, ideal when you don't need runtime validation. ```python theme={null} from typing\\_extensions import TypedDict, Annotated class MovieDict(TypedDict): \"\"\"A movie with details.\"\"\" title: Annotated[str, ..., \"The title of the movie\"] year: Annotated[int, ..., \"The year the movie was released\"] director: Annotated[str, ..., \"The director of the movie\"] rating: Annotated[float, ..., \"The movie's rating out of 10\"] model\\_with\\_structure = model.with\\_structured\\_output(MovieDict) response = model\\_with\\_structure.invoke(\"Provide details about the movie Inception\") print(response) # {'title': 'Inception', 'year': 2010, 'director': 'Christopher Nolan', 'rating': 8.8} ```   For maximum control or interoperability, you can provide a raw JSON Schema. ```python theme={null} import json json\\_schema = { \"title\": \"Movie\", \"description\": \"A movie with details\", \"type\": \"object\", \"properties\": { \"title\": { \"type\": \"string\", \"description\": \"The title of the movie\" }, \"year\": { \"type\": \"integer\", \"description\": \"The year the movie was released\" }, \"director\": { \"type\": \"string\", \"description\": \"The director of the movie\" }, \"rating\": { \"type\": \"number\", \"description\": \"The movie's rating out of 10\" } }, \"required\": [\"title\", \"year\", \"director\", \"rating\"] } model\\_with\\_structure = model.with\\_structured\\_output( json\\_schema, method=\"json\\_schema\", ) response = model\\_with\\_structure.invoke(\"Provide details about the movie Inception\") print(response) # {'title': 'Inception', 'year': 2010, ...} ```    \\*\\*Key considerations for structured output:\\*\\* \\* \\*\\*Method parameter\\*\\*: Some providers support different methods (`'json\\_schema'`, `'function\\_calling'`, `'json\\_mode'`) \\* `'json\\_schema'` typically refers to dedicated structured output features offered by a provider \\* `'function\\_calling'` derives structured output by forcing a [tool call](#tool-calling) following the given schema \\* `'json\\_mode'` is a precursor to `'json\\_schema'` offered by some providers - it generates valid json, but the schema must be described in the prompt \\* \\*\\*Include raw\\*\\*: Use `include\\_raw=True` to get both the parsed output and the raw AI message \\* \\*\\*Validation\\*\\*: Pydantic models provide automatic validation, while `TypedDict` and JSON Schema require manual validation   It can be useful to return the raw [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) object alongside the parsed representation to access response metadata such as [token counts](#token-usage). To do this, set [`include\\_raw=True`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.language\\_models.chat\\_models.BaseChatModel.with\\_structured\\_output\\(include\\_raw\\)) when calling [`with\\_structured\\_output`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.language\\_models.chat\\_models.BaseChatModel.with\\_structured\\_output): ```python theme={null} from pydantic import BaseModel, Field class Movie(BaseModel): \"\"\"A movie with details.\"\"\" title: str = Field(..., description=\"The title of the movie\") year: int = Field(..., description=\"The year the movie was released\") director: str = Field(..., description=\"The director of the movie\") rating: float = Field(..., description=\"The movie's rating out of 10\") model\\_with\\_structure = model.with\\_structured\\_output(Movie, include\\_raw=True) # [!code highlight] response = model\\_with\\_structure.invoke(\"Provide details about the movie Inception\") response # { # \"raw\": AIMessage(...), # \"parsed\": Movie(title=..., year=..., ...), # \"parsing\\_error\": None, # } ```   Schemas can be nested:  ```python Pydantic BaseModel theme={null} from pydantic import BaseModel, Field class Actor(BaseModel): name: str role: str class MovieDetails(BaseModel): title: str year: int cast: list[Actor] genres: list[str] budget: float | None = Field(None, description=\"Budget in millions USD\") model\\_with\\_structure = model.with\\_structured\\_output(MovieDetails) ``` ```python TypedDict theme={null} from typing\\_extensions import Annotated, TypedDict class Actor(TypedDict): name: str role: str class MovieDetails(TypedDict): title: str year: int cast: list[Actor] genres: list[str] budget: Annotated[float | None, ..., \"Budget in millions USD\"] model\\_with\\_structure = model.with\\_structured\\_output(MovieDetails) ```   \\*\\*\\* ## Supported models LangChain supports all major model providers, including OpenAI, Anthropic, Google, Azure, AWS Bedrock, and more. Each provider offers a variety of models with different capabilities. For a full list of supported models in LangChain, see the [integrations page](/oss/python/integrations/providers/overview). \\*\\*\\* ## Advanced topics ### Model profiles  This is a beta feature. The format of model profiles is subject to change.   Model profiles require `langchain>=1.1`.  LangChain chat models can expose a dictionary of supported features and capabilities through a `.profile` attribute: ```python theme={null} model.profile # { # \"max\\_input\\_tokens\": 400000, # \"image\\_inputs\": True, # \"reasoning\\_output\": True, # \"tool\\_calling\": True, # ... # } ``` Refer to the full set of fields in the [API reference](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.language\\_models.BaseChatModel.profile). Much of the model profile data is powered by the [models.dev](https://github.com/sst/models.dev) project, an open source initiative that provides model capability data. These data are augmented with additional fields for purposes of use with LangChain. These augmentations are kept aligned with the upstream project as it evolves. Model profile data allow applications to work around model capabilities dynamically. For example: 1. [Summarization middleware](/oss/python/langchain/middleware/built-in#summarization) can trigger summarization based on a model's context window size. 2. [Structured output](/oss/python/langchain/structured-output) strategies in `create\\_agent` can be inferred automatically (e.g., by checking support for native structured output features). 3. Model inputs can be gated based on supported [modalities](#multimodal) and maximum input tokens.  Model profile data can be changed if it is missing, stale, or incorrect. \\*\\*Option 1 (quick fix)\\*\\* You can instantiate a chat model with any valid profile: ```python theme={null} custom\\_profile = { \"max\\_input\\_tokens\": 100\\_000, \"tool\\_calling\": True, \"structured\\_output\": True, # ... } model = init\\_chat\\_model(\"...\", profile=custom\\_profile) ``` The `profile` is also a regular `dict` and can be updated in place. If the model instance is shared, consider using `model\\_copy` to avoid mutating shared state. ```python theme={null} new\\_profile = model.profile | {\"key\": \"value\"} model.model\\_copy(update={\"profile\": new\\_profile}) ``` \\*\\*Option 2 (fix data upstream)\\*\\* The primary source for the data is the [models.dev](https://models.dev/) project. This data is merged with additional fields and overrides in LangChain [integration packages](/oss/python/integrations/providers/overview) and are shipped with those packages. Model profile data can be updated through the following process: 1. (If needed) update the source data at [models.dev](https://models.dev/) through a pull request to its [repository on Github](https://github.com/sst/models.dev). 2. (If needed) update additional fields and overrides in `langchain\\_/data/profile\\_augmentations.toml` through a pull request to the LangChain [integration package](/oss/python/integrations/providers/overview)\\`. 3. Use the [`langchain-model-profiles`](https://pypi.org/project/langchain-model-profiles/) CLI tool to pull the latest data from [models.dev](https://models.dev/), merge in the augmentations and update the profile data: ```bash theme={null} pip install langchain-model-profiles ``` ```bash theme={null} langchain-profiles refresh --provider  --data-dir  ``` This command: \\* Downloads the latest data for `` from models.dev \\* Merges augmentations from `profile\\_augmentations.toml` in `` \\* Writes merged profiles to `profiles.py` in `` For example: from [`libs/partners/anthropic`](https://github.com/langchain-ai/langchain/tree/master/libs/partners/anthropic) in the [LangChain monorepo](https://github.com/langchain-ai/langchain): ```bash theme={null} uv run --with langchain-model-profiles --provider anthropic --data-dir langchain\\_anthropic/data ```  ### Multimodal Certain models can process and return non-textual data such as images, audio, and video. You can pass non-textual data to a model by providing [content blocks](/oss/python/langchain/messages#message-content).  All LangChain chat models with underlying multimodal capabilities support: 1. Data in the cross-provider standard format (see [our messages guide](/oss/python/langchain/messages)) 2. OpenAI [chat completions](https://platform.openai.com/docs/api-reference/chat) format 3. Any format that is native to that specific provider (e.g., Anthropic models accept Anthropic native format)  See the [multimodal section](/oss/python/langchain/messages#multimodal) of the messages guide for details. Some models can return multimodal data as part of their response. If invoked to do so, the resulting [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) will have content blocks with multimodal types. ```python Multimodal output theme={null} response = model.invoke(\"Create a picture of a cat\") print(response.content\\_blocks) # [ # {\"type\": \"text\", \"text\": \"Here's a picture of a cat\"}, # {\"type\": \"image\", \"base64\": \"...\", \"mime\\_type\": \"image/jpeg\"}, # ] ``` See the [integrations page](/oss/python/integrations/providers/overview) for details on specific providers. ### Reasoning Many models are capable of performing multi-step reasoning to arrive at a conclusion. This involves breaking down complex problems into smaller, more manageable steps. \\*\\*If supported by the underlying model,\\*\\* you can surface this reasoning process to better understand how the model arrived at its final answer.  ```python Stream reasoning output theme={null} for chunk in model.stream(\"Why do parrots have colorful feathers?\"): reasoning\\_steps = [r for r in chunk.content\\_blocks if r[\"type\"] == \"reasoning\"] print(reasoning\\_steps if reasoning\\_steps else chunk.text) ``` ```python Complete reasoning output theme={null} response = model.invoke(\"Why do parrots have colorful feathers?\") reasoning\\_steps = [b for b in response.content\\_blocks if b[\"type\"] == \"reasoning\"] print(\" \".join(step[\"reasoning\"] for step in reasoning\\_steps)) ```  Depending on the model, you can sometimes specify the level of effort it should put into reasoning. Similarly, you can request that the model turn off reasoning entirely. This may take the form of categorical \"tiers\" of reasoning (e.g., `'low'` or `'high'`) or integer token budgets. For details, see the [integrations page](/oss/python/integrations/providers/overview) or [reference](https://reference.langchain.com/python/integrations/) for your respective chat model. ### Local models LangChain supports running models locally on your own hardware. This is useful for scenarios where either data privacy is critical, you want to invoke a custom model, or when you want to avoid the costs incurred when using a cloud-based model. [Ollama](/oss/python/integrations/chat/ollama) is one of the easiest ways to run models locally. See the full list of local integrations on the [integrations page](/oss/python/integrations/providers/overview). ### Prompt caching Many providers offer prompt caching features to reduce latency and cost on repeat processing of the same tokens. These features can be \\*\\*implicit\\*\\* or \\*\\*explicit\\*\\*: \\* \\*\\*Implicit prompt caching:\\*\\* providers will automatically pass on cost savings if a request hits a cache. Examples: [OpenAI](/oss/python/integrations/chat/openai) and [Gemini](/oss/python/integrations/chat/google\\_generative\\_ai). \\* \\*\\*Explicit caching:\\*\\* providers allow you to manually indicate cache points for greater control or to guarantee cost savings. Examples: \\* [`ChatOpenAI`](https://reference.langchain.com/python/integrations/langchain\\_openai/ChatOpenAI) (via `prompt\\_cache\\_key`) \\* Anthropic's [`AnthropicPromptCachingMiddleware`](/oss/python/integrations/chat/anthropic#prompt-caching) \\* [Gemini](https://python.langchain.com/api\\_reference/google\\_genai/chat\\_models/langchain\\_google\\_genai.chat\\_models.ChatGoogleGenerativeAI.html). \\* [AWS Bedrock](/oss/python/integrations/chat/bedrock#prompt-caching)  Prompt caching is often only engaged above a minimum input token threshold. See [provider pages](/oss/python/integrations/chat) for details.  Cache usage will be reflected in the [usage metadata](/oss/python/langchain/messages#token-usage) of the model response. ### Server-side tool use Some providers support server-side [tool-calling](#tool-calling) loops: models can interact with web search, code interpreters, and other tools and analyze the results in a single conversational turn. If a model invokes a tool server-side, the content of the response message will include content representing the invocation and result of the tool. Accessing the [content blocks](/oss/python/langchain/messages#standard-content-blocks) of the response will return the server-side tool calls and results in a provider-agnostic format: ```python Invoke with server-side tool use theme={null} from langchain.chat\\_models import init\\_chat\\_model model = init\\_chat\\_model(\"gpt-4.1-mini\") tool = {\"type\": \"web\\_search\"} model\\_with\\_tools = model.bind\\_tools([tool]) response = model\\_with\\_tools.invoke(\"What was a positive news story from today?\") response.content\\_blocks ``` ```python Result expandable theme={null} [ { \"type\": \"server\\_tool\\_call\", \"name\": \"web\\_search\", \"args\": { \"query\": \"positive news stories today\", \"type\": \"search\" }, \"id\": \"ws\\_abc123\" }, { \"type\": \"server\\_tool\\_result\", \"tool\\_call\\_id\": \"ws\\_abc123\", \"status\": \"success\" }, { \"type\": \"text\", \"text\": \"Here are some positive news stories from today...\", \"annotations\": [ { \"end\\_index\": 410, \"start\\_index\": 337, \"title\": \"article title\", \"type\": \"citation\", \"url\": \"...\" } ] } ] ``` This represents a single conversational turn; there are no associated [ToolMessage](/oss/python/langchain/messages#tool-message) objects that need to be passed in as in client-side [tool-calling](#tool-calling). See the [integration page](/oss/python/integrations/chat) for your given provider for available tools and usage details. ### Rate limiting Many chat model providers impose a limit on the number of invocations that can be made in a given time period. If you hit a rate limit, you will typically receive a rate limit error response from the provider, and will need to wait before making more requests. To help manage rate limits, chat model integrations accept a `rate\\_limiter` parameter that can be provided during initialization to control the rate at which requests are made.  LangChain in comes with (an optional) built-in [`InMemoryRateLimiter`](https://reference.langchain.com/python/langchain\\_core/rate\\_limiters/#langchain\\_core.rate\\_limiters.InMemoryRateLimiter). This limiter is thread safe and can be shared by multiple threads in the same process. ```python Define a rate limiter theme={null} from langchain\\_core.rate\\_limiters import InMemoryRateLimiter rate\\_limiter = InMemoryRateLimiter( requests\\_per\\_second=0.1, # 1 request every 10s check\\_every\\_n\\_seconds=0.1, # Check every 100ms whether allowed to make a request max\\_bucket\\_size=10, # Controls the maximum burst size. ) model = init\\_chat\\_model( model=\"gpt-5\", model\\_provider=\"openai\", rate\\_limiter=rate\\_limiter # [!code highlight] ) ```  The provided rate limiter can only limit the number of requests per unit time. It will not help if you need to also limit based on the size of the requests.   ### Base URL or proxy For many chat model integrations, you can configure the base URL for API requests, which allows you to use model providers that have OpenAI-compatible APIs or to use a proxy server.  Many model providers offer OpenAI-compatible APIs (e.g., [Together AI](https://www.together.ai/), [vLLM](https://github.com/vllm-project/vllm)). You can use [`init\\_chat\\_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat\\_models.init\\_chat\\_model) with these providers by specifying the appropriate `base\\_url` parameter: ```python theme={null} model = init\\_chat\\_model( model=\"MODEL\\_NAME\", model\\_provider=\"openai\", base\\_url=\"BASE\\_URL\", api\\_key=\"YOUR\\_API\\_KEY\", ) ```  When using direct chat model class instantiation, the parameter name may vary by provider. Check the respective [reference](/oss/python/integrations/providers/overview) for details.    For deployments requiring HTTP proxies, some model integrations support proxy configuration: ```python theme={null} from langchain\\_openai import ChatOpenAI model = ChatOpenAI( model=\"gpt-4o\", openai\\_proxy=\"http://proxy.example.com:8080\" ) ```  Proxy support varies by integration. Check the specific model provider's [reference](/oss/python/integrations/providers/overview) for proxy configuration options.   ### Log probabilities Certain models can be configured to return token-level log probabilities representing the likelihood of a given token by setting the `logprobs` parameter when initializing the model: ```python theme={null} model = init\\_chat\\_model( model=\"gpt-4o\", model\\_provider=\"openai\" ).bind(logprobs=True) response = model.invoke(\"Why do parrots talk?\") print(response.response\\_metadata[\"logprobs\"]) ``` ### Token usage A number of model providers return token usage information as part of the invocation response. When available, this information will be included on the [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) objects produced by the corresponding model. For more details, see the [messages](/oss/python/langchain/messages) guide.  Some provider APIs, notably OpenAI and Azure OpenAI chat completions, require users opt-in to receiving token usage data in streaming contexts. See the [streaming usage metadata](/oss/python/integrations/chat/openai#streaming-usage-metadata) section of the integration guide for details.  You can track aggregate token counts across models in an application using either a callback or context manager, as shown below:   ```python theme={null} from langchain.chat\\_models import init\\_chat\\_model from langchain\\_core.callbacks import UsageMetadataCallbackHandler model\\_1 = init\\_chat\\_model(model=\"gpt-4o-mini\") model\\_2 = init\\_chat\\_model(model=\"claude-haiku-4-5-20251001\") callback = UsageMetadataCallbackHandler() result\\_1 = model\\_1.invoke(\"Hello\", config={\"callbacks\": [callback]}) result\\_2 = model\\_2.invoke(\"Hello\", config={\"callbacks\": [callback]}) callback.usage\\_metadata ``` ```python theme={null} { 'gpt-4o-mini-2024-07-18': { 'input\\_tokens': 8, 'output\\_tokens': 10, 'total\\_tokens': 18, 'input\\_token\\_details': {'audio': 0, 'cache\\_read': 0}, 'output\\_token\\_details': {'audio': 0, 'reasoning': 0} }, 'claude-haiku-4-5-20251001': { 'input\\_tokens': 8, 'output\\_tokens': 21, 'total\\_tokens': 29, 'input\\_token\\_details': {'cache\\_read': 0, 'cache\\_creation': 0} } } ```   ```python theme={null} from langchain.chat\\_models import init\\_chat\\_model from langchain\\_core.callbacks import get\\_usage\\_metadata\\_callback model\\_1 = init\\_chat\\_model(model=\"gpt-4o-mini\") model\\_2 = init\\_chat\\_model(model=\"claude-haiku-4-5-20251001\") with get\\_usage\\_metadata\\_callback() as cb: model\\_1.invoke(\"Hello\") model\\_2.invoke(\"Hello\") print(cb.usage\\_metadata) ``` ```python theme={null} { 'gpt-4o-mini-2024-07-18': { 'input\\_tokens': 8, 'output\\_tokens': 10, 'total\\_tokens': 18, 'input\\_token\\_details': {'audio': 0, 'cache\\_read': 0}, 'output\\_token\\_details': {'audio': 0, 'reasoning': 0} }, 'claude-haiku-4-5-20251001': { 'input\\_tokens': 8, 'output\\_tokens': 21, 'total\\_tokens': 29, 'input\\_token\\_details': {'cache\\_read': 0, 'cache\\_creation': 0} } } ```   ### Invocation config When invoking a model, you can pass additional configuration through the `config` parameter using a [`RunnableConfig`](https://reference.langchain.com/python/langchain\\_core/runnables/#langchain\\_core.runnables.RunnableConfig) dictionary. This provides run-time control over execution behavior, callbacks, and metadata tracking. Common configuration options include: ```python Invocation with config theme={null} response = model.invoke( \"Tell me a joke\", config={ \"run\\_name\": \"joke\\_generation\", # Custom name for this run \"tags\": [\"humor\", \"demo\"], # Tags for categorization \"metadata\": {\"user\\_id\": \"123\"}, # Custom metadata \"callbacks\": [my\\_callback\\_handler], # Callback handlers } ) ``` These configuration values are particularly useful when: \\* Debugging with [LangSmith](https://docs.smith.langchain.com/) tracing \\* Implementing custom logging or monitoring \\* Controlling resource usage in production \\* Tracking invocations across complex pipelines   Identifies this specific invocation in logs and traces. Not inherited by sub-calls.   Labels inherited by all sub-calls for filtering and organization in debugging tools.   Custom key-value pairs for tracking additional context, inherited by all sub-calls.   Controls the maximum number of parallel calls when using [`batch()`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.language\\_models.chat\\_models.BaseChatModel.batch) or [`batch\\_as\\_completed()`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.language\\_models.chat\\_models.BaseChatModel.batch\\_as\\_completed).   Handlers for monitoring and responding to events during execution.   Maximum recursion depth for chains to prevent infinite loops in complex pipelines.    See full [`RunnableConfig`](https://reference.langchain.com/python/langchain\\_core/runnables/#langchain\\_core.runnables.RunnableConfig) reference for all supported attributes.  ### Configurable models You can also create a runtime-configurable model by specifying [`configurable\\_fields`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.language\\_models.chat\\_models.BaseChatModel.configurable\\_fields). If you don't specify a model value, then `'model'` and `'model\\_provider'` will be configurable by default. ```python theme={null} from langchain.chat\\_models import init\\_chat\\_model configurable\\_model = init\\_chat\\_model(temperature=0) configurable\\_model.invoke( \"what's your name\", config={\"configurable\": {\"model\": \"gpt-5-nano\"}}, # Run with GPT-5-Nano ) configurable\\_model.invoke( \"what's your name\", config={\"configurable\": {\"model\": \"claude-sonnet-4-5-20250929\"}}, # Run with Claude ) ```  We can create a configurable model with default model values, specify which parameters are configurable, and add prefixes to configurable params: ```python theme={null} first\\_model = init\\_chat\\_model( model=\"gpt-4.1-mini\", temperature=0, configurable\\_fields=(\"model\", \"model\\_provider\", \"temperature\", \"max\\_tokens\"), config\\_prefix=\"first\", # Useful when you have a chain with multiple models ) first\\_model.invoke(\"what's your name\") ``` ```python theme={null} first\\_model.invoke( \"what's your name\", config={ \"configurable\": { \"first\\_model\": \"claude-sonnet-4-5-20250929\", \"first\\_temperature\": 0.5, \"first\\_max\\_tokens\": 100, } }, ) ``` See the [`init\\_chat\\_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat\\_models.init\\_chat\\_model) reference for more details on `configurable\\_fields` and `config\\_prefix`.   We can call declarative operations like `bind\\_tools`, `with\\_structured\\_output`, `with\\_configurable`, etc. on a configurable model and chain a configurable model in the same way that we would a regularly instantiated chat model object. ```python theme={null} from pydantic import BaseModel, Field class GetWeather(BaseModel): \"\"\"Get the current weather in a given location\"\"\" location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\") class GetPopulation(BaseModel): \"\"\"Get the current population in a given location\"\"\" location: str = Field(..., description=\"The city and state, e.g. San Francisco, CA\") model = init\\_chat\\_model(temperature=0) model\\_with\\_tools = model.bind\\_tools([GetWeather, GetPopulation]) model\\_with\\_tools.invoke( \"what's bigger in 2024 LA or NYC\", config={\"configurable\": {\"model\": \"gpt-4.1-mini\"}} ).tool\\_calls ``` ``` [ { 'name': 'GetPopulation', 'args': {'location': 'Los Angeles, CA'}, 'id': 'call\\_Ga9m8FAArIyEjItHmztPYA22', 'type': 'tool\\_call' }, { 'name': 'GetPopulation', 'args': {'location': 'New York, NY'}, 'id': 'call\\_jh2dEvBaAHRaw5JUDthOs7rt', 'type': 'tool\\_call' } ] ``` ```python theme={null} model\\_with\\_tools.invoke( \"what's bigger in 2024 LA or NYC\", config={\"configurable\": {\"model\": \"claude-sonnet-4-5-20250929\"}}, ).tool\\_calls ``` ``` [ { 'name': 'GetPopulation', 'args': {'location': 'Los Angeles, CA'}, 'id': 'toolu\\_01JMufPf4F4t2zLj7miFeqXp', 'type': 'tool\\_call' }, { 'name': 'GetPopulation', 'args': {'location': 'New York City, NY'}, 'id': 'toolu\\_01RQBHcE8kEEbYTuuS8WqY1u', 'type': 'tool\\_call' } ] ```  \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/models.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/multi-agent.md",
      "title": null,
      "raw_content": "# Multi-agent \\*\\*Multi-agent systems\\*\\* break a complex application into multiple specialized agents that work together to solve problems. Instead of relying on a single agent to handle every step, \\*\\*multi-agent architectures\\*\\* allow you to compose smaller, focused agents into a coordinated workflow. Multi-agent systems are useful when: \\* A single agent has too many tools and makes poor decisions about which to use. \\* Context or memory grows too large for one agent to track effectively. \\* Tasks require \\*\\*specialization\\*\\* (e.g., a planner, researcher, math expert). ## Multi-agent patterns | Pattern | How it works | Control flow | Example use case | | --------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------- | ------------------------------------------------ | | [\\*\\*Tool Calling\\*\\*](#tool-calling) | A \\*\\*supervisor\\*\\* agent calls other agents as \\*tools\\*. The \u201ctool\u201d agents don\u2019t talk to the user directly \u2014 they just run their task and return results. | Centralized: all routing passes through the calling agent. | Task orchestration, structured workflows. | | [\\*\\*Handoffs\\*\\*](#handoffs) | The current agent decides to \\*\\*transfer control\\*\\* to another agent. The active agent changes, and the user may continue interacting directly with the new agent. | Decentralized: agents can change who is active. | Multi-domain conversations, specialist takeover. |  Learn how to build a personal assistant using the supervisor pattern, where a central supervisor agent coordinates specialized worker agents. This tutorial demonstrates: \\* Creating specialized sub-agents for different domains (calendar and email) \\* Wrapping sub-agents as tools for centralized orchestration \\* Adding human-in-the-loop review for sensitive actions  ## Choosing a pattern | Question | Tool Calling | Handoffs | | ----------------------------------------------------- | ------------ | -------- | | Need centralized control over workflow? | \u2705 Yes | \u274c No | | Want agents to interact directly with the user? | \u274c No | \u2705 Yes | | Complex, human-like conversation between specialists? | \u274c Limited | \u2705 Strong |  You can mix both patterns \u2014 use \\*\\*handoffs\\*\\* for agent switching, and have each agent \\*\\*call subagents as tools\\*\\* for specialized tasks.  ## Customizing agent context At the heart of multi-agent design is \\*\\*context engineering\\*\\* - deciding what information each agent sees. LangChain gives you fine-grained control over: \\* Which parts of the conversation or state are passed to each agent. \\* Specialized prompts tailored to subagents. \\* Inclusion/exclusion of intermediate reasoning. \\* Customizing input/output formats per agent. The quality of your system \\*\\*heavily depends\\*\\* on context engineering. The goal is to ensure that each agent has access to the correct data it needs to perform its task, whether it\u2019s acting as a tool or as an active agent. ## Tool calling In \\*\\*tool calling\\*\\*, one agent (the \u201c\\*\\*controller\\*\\*\u201d) treats other agents as \\*tools\\* to be invoked when needed. The controller manages orchestration, while tool agents perform specific tasks and return results. Flow: 1. The \\*\\*controller\\*\\* receives input and decides which tool (subagent) to call. 2. The \\*\\*tool agent\\*\\* runs its task based on the controller\u2019s instructions. 3. The \\*\\*tool agent\\*\\* returns results to the controller. 4. The \\*\\*controller\\*\\* decides the next step or finishes. ```mermaid theme={null} graph LR A[User] --> B[Controller Agent] B --> C[Tool Agent 1] B --> D[Tool Agent 2] C --> B D --> B B --> E[User Response] ```  Agents used as tools are generally \\*\\*not expected\\*\\* to continue conversation with the user. Their role is to perform a task and return results to the controller agent. If you need subagents to be able to converse with the user, use \\*\\*handoffs\\*\\* instead.  ### Implementation Below is a minimal example where the main agent is given access to a single subagent via a tool definition: ```python theme={null} from langchain.tools import tool from langchain.agents import create\\_agent subagent1 = create\\_agent(model=\"...\", tools=[...]) @tool( \"subagent1\\_name\", description=\"subagent1\\_description\" ) def call\\_subagent1(query: str): result = subagent1.invoke({ \"messages\": [{\"role\": \"user\", \"content\": query}] }) return result[\"messages\"][-1].content agent = create\\_agent(model=\"...\", tools=[call\\_subagent1]) ``` In this pattern: 1. The main agent invokes `call\\_subagent1` when it decides the task matches the subagent\u2019s description. 2. The subagent runs independently and returns its result. 3. The main agent receives the result and continues orchestration. ### Where to customize There are several points where you can control how context is passed between the main agent and its subagents: 1. \\*\\*Subagent name\\*\\* (`\"subagent1\\_name\"`): This is how the main agent refers to the subagent. Since it influences prompting, choose it carefully. 2. \\*\\*Subagent description\\*\\* (`\"subagent1\\_description\"`): This is what the main agent \u201cknows\u201d about the subagent. It directly shapes how the main agent decides when to call it. 3. \\*\\*Input to the subagent\\*\\*: You can customize this input to better shape how the subagent interprets tasks. In the example above, we pass the agent-generated `query` directly. 4. \\*\\*Output from the subagent\\*\\*: This is the \\*\\*response\\*\\* passed back to the main agent. You can adjust what is returned to control how the main agent interprets results. In the example above, we return the final message text, but you could return additional state or metadata. ### Control the input to the subagent There are two main levers to control the input that the main agent passes to a subagent: \\* \\*\\*Modify the prompt\\*\\* \u2013 Adjust the main agent's prompt or the tool metadata (i.e., sub-agent's name and description) to better guide when and how it calls the subagent. \\* \\*\\*Context injection\\*\\* \u2013 Add input that isn\u2019t practical to capture in a static prompt (e.g., full message history, prior results, task metadata) by adjusting the tool call to pull from the agent\u2019s state. ```python theme={null} from langchain.agents import AgentState from langchain.tools import tool, ToolRuntime class CustomState(AgentState): example\\_state\\_key: str @tool( \"subagent1\\_name\", description=\"subagent1\\_description\" ) def call\\_subagent1(query: str, runtime: ToolRuntime[None, CustomState]): # Apply any logic needed to transform the messages into a suitable input subagent\\_input = some\\_logic(query, runtime.state[\"messages\"]) result = subagent1.invoke({ \"messages\": subagent\\_input, # You could also pass other state keys here as needed. # Make sure to define these in both the main and subagent's # state schemas. \"example\\_state\\_key\": runtime.state[\"example\\_state\\_key\"] }) return result[\"messages\"][-1].content ``` ### Control the output from the subagent Two common strategies for shaping what the main agent receives back from a subagent: \\* \\*\\*Modify the prompt\\*\\* \u2013 Refine the subagent\u2019s prompt to specify exactly what should be returned. \\* Useful when outputs are incomplete, too verbose, or missing key details. \\* A common failure mode is that the subagent performs tool calls or reasoning but does \\*\\*not include the results\\*\\* in its final message. Remind it that the controller (and user) only see the final output, so all relevant info must be included there. \\* \\*\\*Custom output formatting\\*\\* \u2013 adjust or enrich the subagent's response in code before handing it back to the main agent. \\* Example: pass specific state keys back to the main agent in addition to the final text. \\* This requires wrapping the result in a [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) (or equivalent structure) so you can merge custom state with the subagent\u2019s response. ```python theme={null} from typing import Annotated from langchain.agents import AgentState from langchain.tools import InjectedToolCallId from langgraph.types import Command @tool( \"subagent1\\_name\", description=\"subagent1\\_description\" ) # We need to pass the `tool\\_call\\_id` to the sub agent so it can use it to respond with the tool call result def call\\_subagent1( query: str, tool\\_call\\_id: Annotated[str, InjectedToolCallId], # You need to return a `Command` object to include more than just a final tool call ) -> Command: result = subagent1.invoke({ \"messages\": [{\"role\": \"user\", \"content\": query}] }) return Command(update={ # This is the example state key we are passing back \"example\\_state\\_key\": result[\"example\\_state\\_key\"], \"messages\": [ ToolMessage( content=result[\"messages\"][-1].content, # We need to include the tool call id so it matches up with the right tool call tool\\_call\\_id=tool\\_call\\_id ) ] }) ``` ## Handoffs In \\*\\*handoffs\\*\\*, agents can directly pass control to each other. The \u201cactive\u201d agent changes, and the user interacts with whichever agent currently has control. Flow: 1. The \\*\\*current agent\\*\\* decides it needs help from another agent. 2. It passes control (and state) to the \\*\\*next agent\\*\\*. 3. The \\*\\*new agent\\*\\* interacts directly with the user until it decides to hand off again or finish. ```mermaid theme={null} graph LR A[User] --> B[Agent A] B --> C[Agent B] C --> A ``` ### Implementation (Coming soon) \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/multi-agent.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/observability.md",
      "title": null,
      "raw_content": "# LangSmith Observability As you build and run agents with LangChain, you need visibility into how they behave: which [tools](/oss/python/langchain/tools) they call, what prompts they generate, and how they make decisions. LangChain agents built with [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent) automatically support tracing through [LangSmith](/langsmith/home), a platform for capturing, debugging, evaluating, and monitoring LLM application behavior. [\\*Traces\\*](/langsmith/observability-concepts#traces) record every step of your agent's execution, from the initial user input to the final response, including all tool calls, model interactions, and decision points. This execution data helps you debug issues, evaluate performance across different inputs, and monitor usage patterns in production. This guide shows you how to enable tracing for your LangChain agents and use LangSmith to analyze their execution. ## Prerequisites Before you begin, ensure you have the following: \\* \\*\\*A LangSmith account\\*\\*: Sign up (for free) or log in at [smith.langchain.com](https://smith.langchain.com). \\* \\*\\*A LangSmith API key\\*\\*: Follow the [Create an API key](/langsmith/create-account-api-key#create-an-api-key) guide. ## Enable tracing All LangChain agents automatically support LangSmith tracing. To enable it, set the following environment variables: ```bash theme={null} export LANGSMITH\\_TRACING=true export LANGSMITH\\_API\\_KEY= ``` ## Quickstart No extra code is needed to log a trace to LangSmith. Just run your agent code as you normally would: ```python theme={null} from langchain.agents import create\\_agent def send\\_email(to: str, subject: str, body: str): \"\"\"Send an email to a recipient.\"\"\" # ... email sending logic return f\"Email sent to {to}\" def search\\_web(query: str): \"\"\"Search the web for information.\"\"\" # ... web search logic return f\"Search results for: {query}\" agent = create\\_agent( model=\"gpt-4o\", tools=[send\\_email, search\\_web], system\\_prompt=\"You are a helpful assistant that can send emails and search the web.\" ) # Run the agent - all steps will be traced automatically response = agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"Search for the latest AI news and email a summary to john@example.com\"}] }) ``` By default, the trace will be logged to the project with the name `default`. To configure a custom project name, see [Log to a project](#log-to-a-project). ## Trace selectively You may opt to trace specific invocations or parts of your application using LangSmith's `tracing\\_context` context manager: ```python theme={null} import langsmith as ls # This WILL be traced with ls.tracing\\_context(enabled=True): agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Send a test email to alice@example.com\"}]}) # This will NOT be traced (if LANGSMITH\\_TRACING is not set) agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Send another email\"}]}) ``` ## Log to a project  You can set a custom project name for your entire application by setting the `LANGSMITH\\_PROJECT` environment variable: ```bash theme={null} export LANGSMITH\\_PROJECT=my-agent-project ```   You can set the project name programmatically for specific operations: ```python theme={null} import langsmith as ls with ls.tracing\\_context(project\\_name=\"email-agent-test\", enabled=True): response = agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"Send a welcome email\"}] }) ```  ## Add metadata to traces You can annotate your traces with custom metadata and tags: ```python theme={null} response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"Send a welcome email\"}]}, config={ \"tags\": [\"production\", \"email-assistant\", \"v1.0\"], \"metadata\": { \"user\\_id\": \"user\\_123\", \"session\\_id\": \"session\\_456\", \"environment\": \"production\" } } ) ``` `tracing\\_context` also accepts tags and metadata for fine-grained control: ```python theme={null} with ls.tracing\\_context( project\\_name=\"email-agent-test\", enabled=True, tags=[\"production\", \"email-assistant\", \"v1.0\"], metadata={\"user\\_id\": \"user\\_123\", \"session\\_id\": \"session\\_456\", \"environment\": \"production\"}): response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"Send a welcome email\"}]} ) ``` This custom metadata and tags will be attached to the trace in LangSmith.  To learn more about how to use traces to debug, evaluate, and monitor your agents, see the [LangSmith documentation](/langsmith/home).  \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/observability.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/overview.md",
      "title": null,
      "raw_content": "# LangChain overview  \\*\\*LangChain v1.x is now available!\\*\\* For a complete list of changes and instructions on how to upgrade your code, see the [release notes](/oss/python/releases/langchain-v1) and [migration guide](/oss/python/migrate/langchain-v1). If you encounter any issues or have feedback, please [open an issue](https://github.com/langchain-ai/docs/issues/new?template=01-langchain.yml) so we can improve. To view v0.x documentation, [go to the archived content](https://github.com/langchain-ai/langchain/tree/v0.3/docs/docs).  LangChain is the easiest way to start building agents and applications powered by LLMs. With under 10 lines of code, you can connect to OpenAI, Anthropic, Google, and [more](/oss/python/integrations/providers/overview). LangChain provides a pre-built agent architecture and model integrations to help you get started quickly and seamlessly incorporate LLMs into your agents and applications. We recommend you use LangChain if you want to quickly build agents and autonomous applications. Use [LangGraph](/oss/python/langgraph/overview), our low-level agent orchestration framework and runtime, when you have more advanced needs that require a combination of deterministic and agentic workflows, heavy customization, and carefully controlled latency. LangChain [agents](/oss/python/langchain/agents) are built on top of LangGraph in order to provide durable execution, streaming, human-in-the-loop, persistence, and more. You do not need to know LangGraph for basic LangChain agent usage. ## Create an agent ```python theme={null} # pip install -qU langchain \"langchain[anthropic]\" from langchain.agents import create\\_agent def get\\_weather(city: str) -> str: \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=[get\\_weather], system\\_prompt=\"You are a helpful assistant\", ) # Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ``` See the [Installation instructions](/oss/python/langchain/install) and [Quickstart guide](/oss/python/langchain/quickstart) to get started building your own agents and applications with LangChain. ## Core benefits   Different providers have unique APIs for interacting with models, including the format of responses. LangChain standardizes how you interact with models so that you can seamlessly swap providers and avoid lock-in.   LangChain's agent abstraction is designed to be easy to get started with, letting you build a simple agent in under 10 lines of code. But it also provides enough flexibility to allow you to do all the context engineering your heart desires.   LangChain's agents are built on top of LangGraph. This allows us to take advantage of LangGraph's durable execution, human-in-the-loop support, persistence, and more.   Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics.   \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/overview.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/philosophy.md",
      "title": null,
      "raw_content": "# Philosophy > LangChain exists to be the easiest place to start building with LLMs, while also being flexible and production-ready. LangChain is driven by a few core beliefs: \\* Large Language Models (LLMs) are great, powerful new technology. \\* LLMs are even better when you combine them with external sources of data. \\* LLMs will transform what the applications of the future look like. Specifically, the applications of the future will look more and more agentic. \\* It is still very early on in that transformation. \\* While it's easy to build a prototype of those agentic applications, it's still really hard to build agents that are reliable enough to put into production. With LangChain, we have two core focuses:   Different providers expose different APIs, with different model parameters and different message formats. Standardizing these model inputs and outputs is a core focus, making it easy for developer to easily change to the most recent state-of-the-art model, avoiding lock-in.   Models should be used for more than just \\*text generation\\* - they should also be used to orchestrate more complex flows that interact with other data. LangChain makes it easy to define [tools](/oss/python/langchain/tools) that LLMs can use dynamically, as well as help with parsing of and access to unstructured data.   ## History Given the constant rate of change in the field, LangChain has also evolved over time. Below is a brief timeline of how LangChain has changed over the years, evolving alongside what it means to build with LLMs:  A month before ChatGPT, \\*\\*LangChain was launched as a Python package\\*\\*. It consisted of two main components: \\* LLM abstractions \\* \"Chains\", or predetermined steps of computation to run, for common use cases. For example - RAG: run a retrieval step, then run a generation step. The name LangChain comes from \"Language\" (like Language models) and \"Chains\".   The first general purpose agents were added to LangChain. These general purpose agents were based on the [ReAct paper](https://arxiv.org/abs/2210.03629) (ReAct standing for Reasoning and Acting). They used LLMs to generate JSON that represented tool calls, and then parsed that JSON to determine what tools to call.   OpenAI releases a 'Chat Completion' API. Previously, models took in strings and returned a string. In the ChatCompletions API, they evolved to take in a list of messages and return a message. Other model providers followed suit, and LangChain updated to work with lists of messages.   LangChain releases a JavaScript version. LLMs and agents will change how applications are built and JavaScript is the language of application developers.   \\*\\*LangChain Inc. was formed as a company\\*\\* around the open source LangChain project. The main goal was to \"make intelligent agents ubiquitous\". The team recognized that while LangChain was a key part (LangChain made it simple to get started with LLMs), there was also a need for other components.   OpenAI releases 'function calling' in their API. This allowed the API to explicitly generate payloads that represented tool calls. Other model providers followed suit, and LangChain was updated to use this as the preferred method for tool calling (rather than parsing JSON).   \\*\\*LangSmith is released\\*\\* as closed source platform by LangChain Inc., providing observability and evals The main issue with building agents is getting them to be reliable, and LangSmith, which provides observability and evals, was built to solve that need. LangChain was updated to integrate seamlessly with LangSmith.   \\*\\*LangChain releases 0.1.0\\*\\*, its first non-0.0.x. The industry matured from prototypes to production, and as such, LangChain increased its focus on stability.   \\*\\*LangGraph is released\\*\\* as an open-source library. The original LangChain had two focuses: LLM abstractions, and high-level interfaces for getting started with common applications; however, it was missing a low-level orchestration layer that allowed developers to control the exact flow of their agent. Enter: LangGraph. When building LangGraph, we learned from lessons when building LangChain and added functionality we discovered was needed: streaming, durable execution, short-term memory, human-in-the-loop, and more.   \\*\\*LangChain has over 700 integrations.\\*\\* Integrations were split out of the core LangChain package, and either moved into their own standalone packages (for the core integrations) or `langchain-community`.   LangGraph becomes the preferred way to build any AI application that is more than a single LLM call. As developers tried to improve the reliability of their applications, they needed more control than the high-level interfaces provided. LangGraph provided that low-level flexibility. Most chains and agents were marked as deprecated in LangChain with guides on how to migrate them to LangGraph. There is still one high-level abstraction created in LangGraph: an agent abstraction. It is built on top of low-level LangGraph and has the same interface as the ReAct agents from LangChain.   Model APIs become more multimodal. Models started to accept files, images, videos, and more. We updated the `langchain-core` message format accordingly to allow developers to specify these multimodal inputs in a standard way.   \\*\\*LangChain releases 1.0\\*\\* with two major changes: 1. Complete revamp of all chains and agents in `langchain`. All chains and agents are now replaced with only one high level abstraction: an agent abstraction built on top of LangGraph. This was the high-level abstraction that was originally created in LangGraph, but just moved to LangChain. For users still using old LangChain chains/agents who do NOT want to upgrade (note: we recommend you do), you can continue using old LangChain by installing the `langchain-classic` package. 2. A standard message content format: Model APIs evolved from returning messages with a simple content string to more complex output types - reasoning blocks, citations, server-side tool calls, etc. LangChain evolved its message formats to standardize these across providers.  \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/philosophy.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/quickstart.md",
      "title": null,
      "raw_content": "# Quickstart This quickstart takes you from a simple setup to a fully functional AI agent in just a few minutes. ## Requirements For these examples, you will need to: \\* [Install](/oss/python/langchain/install) the LangChain package \\* Set up a [Claude (Anthropic)](https://www.anthropic.com/) account and get an API key \\* Set the `ANTHROPIC\\_API\\_KEY` environment variable in your terminal  \\*\\*LangChain Docs MCP server\\*\\* If you're using an AI coding assistant, you should install the [LangChain Docs MCP server](/use-these-docs) to get the most out of it. This ensures your agent has access to the latest documentation and examples.  ## Build a basic agent Start by creating a simple agent that can answer questions and call tools. The agent will use Claude Sonnet 4.5 as its language model, a basic weather function as a tool, and a simple prompt to guide its behavior. ```python theme={null} from langchain.agents import create\\_agent def get\\_weather(city: str) -> str: \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=[get\\_weather], system\\_prompt=\"You are a helpful assistant\", ) # Run the agent agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather in sf\"}]} ) ```  To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langchain).  ## Build a real-world agent Next, build a practical weather forecasting agent that demonstrates key production concepts: 1. \\*\\*Detailed system prompts\\*\\* for better agent behavior 2. \\*\\*Create tools\\*\\* that integrate with external data 3. \\*\\*Model configuration\\*\\* for consistent responses 4. \\*\\*Structured output\\*\\* for predictable results 5. \\*\\*Conversational memory\\*\\* for chat-like interactions 6. \\*\\*Create and run the agent\\*\\* create a fully functional agent Let's walk through each step:   The system prompt defines your agent\u2019s role and behavior. Keep it specific and actionable: ```python wrap theme={null} SYSTEM\\_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns. You have access to two tools: - get\\_weather\\_for\\_location: use this to get the weather for a specific location - get\\_user\\_location: use this to get the user's location If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get\\_user\\_location tool to find their location.\"\"\" ```   [Tools](/oss/python/langchain/tools) let a model interact with external systems by calling functions you define. Tools can depend on [runtime context](/oss/python/langchain/runtime) and also interact with [agent memory](/oss/python/langchain/short-term-memory). Notice below how the `get\\_user\\_location` tool uses runtime context: ```python theme={null} from dataclasses import dataclass from langchain.tools import tool, ToolRuntime @tool def get\\_weather\\_for\\_location(city: str) -> str: \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" @dataclass class Context: \"\"\"Custom runtime context schema.\"\"\" user\\_id: str @tool def get\\_user\\_location(runtime: ToolRuntime[Context]) -> str: \"\"\"Retrieve user information based on user ID.\"\"\" user\\_id = runtime.context.user\\_id return \"Florida\" if user\\_id == \"1\" else \"SF\" ```  Tools should be well-documented: their name, description, and argument names become part of the model's prompt. LangChain's [`@tool` decorator](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool) adds metadata and enables runtime injection via the `ToolRuntime` parameter.    Set up your [language model](/oss/python/langchain/models) with the right parameters for your use case: ```python theme={null} from langchain.chat\\_models import init\\_chat\\_model model = init\\_chat\\_model( \"claude-sonnet-4-5-20250929\", temperature=0.5, timeout=10, max\\_tokens=1000 ) ``` Depending on the model and provider chosen, initialization parameters may vary; refer to their reference pages for details.   Optionally, define a structured response format if you need the agent responses to match a specific schema. ```python theme={null} from dataclasses import dataclass # We use a dataclass here, but Pydantic models are also supported. @dataclass class ResponseFormat: \"\"\"Response schema for the agent.\"\"\" # A punny response (always required) punny\\_response: str # Any interesting information about the weather if available weather\\_conditions: str | None = None ```   Add [memory](/oss/python/langchain/short-term-memory) to your agent to maintain state across interactions. This allows the agent to remember previous conversations and context. ```python theme={null} from langgraph.checkpoint.memory import InMemorySaver checkpointer = InMemorySaver() ```  In production, use a persistent checkpointer that saves to a database. See [Add and manage memory](/oss/python/langgraph/add-memory#manage-short-term-memory) for more details.    Now assemble your agent with all the components and run it! ```python theme={null} from langchain.agents.structured\\_output import ToolStrategy agent = create\\_agent( model=model, system\\_prompt=SYSTEM\\_PROMPT, tools=[get\\_user\\_location, get\\_weather\\_for\\_location], context\\_schema=Context, response\\_format=ToolStrategy(ResponseFormat), checkpointer=checkpointer ) # `thread\\_id` is a unique identifier for a given conversation. config = {\"configurable\": {\"thread\\_id\": \"1\"}} response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]}, config=config, context=Context(user\\_id=\"1\") ) print(response['structured\\_response']) # ResponseFormat( # punny\\_response=\"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\", # weather\\_conditions=\"It's always sunny in Florida!\" # ) # Note that we can continue the conversation using the same `thread\\_id`. response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]}, config=config, context=Context(user\\_id=\"1\") ) print(response['structured\\_response']) # ResponseFormat( # punny\\_response=\"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\", # weather\\_conditions=None # ) ```    ```python theme={null} from dataclasses import dataclass from langchain.agents import create\\_agent from langchain.chat\\_models import init\\_chat\\_model from langchain.tools import tool, ToolRuntime from langgraph.checkpoint.memory import InMemorySaver from langchain.agents.structured\\_output import ToolStrategy # Define system prompt SYSTEM\\_PROMPT = \"\"\"You are an expert weather forecaster, who speaks in puns. You have access to two tools: - get\\_weather\\_for\\_location: use this to get the weather for a specific location - get\\_user\\_location: use this to get the user's location If a user asks you for the weather, make sure you know the location. If you can tell from the question that they mean wherever they are, use the get\\_user\\_location tool to find their location.\"\"\" # Define context schema @dataclass class Context: \"\"\"Custom runtime context schema.\"\"\" user\\_id: str # Define tools @tool def get\\_weather\\_for\\_location(city: str) -> str: \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" @tool def get\\_user\\_location(runtime: ToolRuntime[Context]) -> str: \"\"\"Retrieve user information based on user ID.\"\"\" user\\_id = runtime.context.user\\_id return \"Florida\" if user\\_id == \"1\" else \"SF\" # Configure model model = init\\_chat\\_model( \"claude-sonnet-4-5-20250929\", temperature=0 ) # Define response format @dataclass class ResponseFormat: \"\"\"Response schema for the agent.\"\"\" # A punny response (always required) punny\\_response: str # Any interesting information about the weather if available weather\\_conditions: str | None = None # Set up memory checkpointer = InMemorySaver() # Create agent agent = create\\_agent( model=model, system\\_prompt=SYSTEM\\_PROMPT, tools=[get\\_user\\_location, get\\_weather\\_for\\_location], context\\_schema=Context, response\\_format=ToolStrategy(ResponseFormat), checkpointer=checkpointer ) # Run agent # `thread\\_id` is a unique identifier for a given conversation. config = {\"configurable\": {\"thread\\_id\": \"1\"}} response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"what is the weather outside?\"}]}, config=config, context=Context(user\\_id=\"1\") ) print(response['structured\\_response']) # ResponseFormat( # punny\\_response=\"Florida is still having a 'sun-derful' day! The sunshine is playing 'ray-dio' hits all day long! I'd say it's the perfect weather for some 'solar-bration'! If you were hoping for rain, I'm afraid that idea is all 'washed up' - the forecast remains 'clear-ly' brilliant!\", # weather\\_conditions=\"It's always sunny in Florida!\" # ) # Note that we can continue the conversation using the same `thread\\_id`. response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"thank you!\"}]}, config=config, context=Context(user\\_id=\"1\") ) print(response['structured\\_response']) # ResponseFormat( # punny\\_response=\"You're 'thund-erfully' welcome! It's always a 'breeze' to help you stay 'current' with the weather. I'm just 'cloud'-ing around waiting to 'shower' you with more forecasts whenever you need them. Have a 'sun-sational' day in the Florida sunshine!\", # weather\\_conditions=None # ) ```   To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langchain).  Congratulations! You now have an AI agent that can: \\* \\*\\*Understand context\\*\\* and remember conversations \\* \\*\\*Use multiple tools\\*\\* intelligently \\* \\*\\*Provide structured responses\\*\\* in a consistent format \\* \\*\\*Handle user-specific information\\*\\* through context \\* \\*\\*Maintain conversation state\\*\\* across interactions \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/quickstart.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/rag.md",
      "title": null,
      "raw_content": "# Build a RAG agent with LangChain ## Overview One of the most powerful applications enabled by LLMs is sophisticated question-answering (Q\\&A) chatbots. These are applications that can answer questions about specific source information. These applications use a technique known as Retrieval Augmented Generation, or [RAG](/oss/python/langchain/retrieval/). This tutorial will show how to build a simple Q\\&A application over an unstructured text data source. We will demonstrate: 1. A RAG [agent](#rag-agents) that executes searches with a simple tool. This is a good general-purpose implementation. 2. A two-step RAG [chain](#rag-chains) that uses just a single LLM call per query. This is a fast and effective method for simple queries. ### Concepts We will cover the following concepts: \\* \\*\\*Indexing\\*\\*: a pipeline for ingesting data from a source and indexing it. \\*This usually happens in a separate process.\\* \\* \\*\\*Retrieval and generation\\*\\*: the actual RAG process, which takes the user query at run time and retrieves the relevant data from the index, then passes that to the model. Once we've indexed our data, we will use an [agent](/oss/python/langchain/agents) as our orchestration framework to implement the retrieval and generation steps.  The indexing portion of this tutorial will largely follow the [semantic search tutorial](/oss/python/langchain/knowledge-base). If your data is already available for search (i.e., you have a function to execute a search), or you're comfortable with the content from that tutorial, feel free to skip to the section on [retrieval and generation](#2-retrieval-and-generation)  ### Preview In this guide we'll build an app that answers questions about the website's content. The specific website we will use is the [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/) blog post by Lilian Weng, which allows us to ask questions about the contents of the post. We can create a simple indexing pipeline and RAG chain to do this in \\~40 lines of code. See below for the full code snippet:  ```python theme={null} import bs4 from langchain.agents import AgentState, create\\_agent from langchain\\_community.document\\_loaders import WebBaseLoader from langchain.messages import MessageLikeRepresentation from langchain\\_text\\_splitters import RecursiveCharacterTextSplitter # Load and chunk contents of the blog loader = WebBaseLoader( web\\_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",), bs\\_kwargs=dict( parse\\_only=bs4.SoupStrainer( class\\_=(\"post-content\", \"post-title\", \"post-header\") ) ), ) docs = loader.load() text\\_splitter = RecursiveCharacterTextSplitter(chunk\\_size=1000, chunk\\_overlap=200) all\\_splits = text\\_splitter.split\\_documents(docs) # Index chunks \\_ = vector\\_store.add\\_documents(documents=all\\_splits) # Construct a tool for retrieving context @tool(response\\_format=\"content\\_and\\_artifact\") def retrieve\\_context(query: str): \"\"\"Retrieve information to help answer a query.\"\"\" retrieved\\_docs = vector\\_store.similarity\\_search(query, k=2) serialized = \"\\n\\n\".join( (f\"Source: {doc.metadata}\\nContent: {doc.page\\_content}\") for doc in retrieved\\_docs ) return serialized, retrieved\\_docs tools = [retrieve\\_context] # If desired, specify custom instructions prompt = ( \"You have access to a tool that retrieves context from a blog post. \" \"Use the tool to help answer user queries.\" ) agent = create\\_agent(model, tools, system\\_prompt=prompt) ``` ```python theme={null} query = \"What is task decomposition?\" for step in agent.stream( {\"messages\": [{\"role\": \"user\", \"content\": query}]}, stream\\_mode=\"values\", ): step[\"messages\"][-1].pretty\\_print() ``` ``` ================================ Human Message ================================= What is task decomposition? ================================== Ai Message ================================== Tool Calls: retrieve\\_context (call\\_xTkJr8njRY0geNz43ZvGkX0R) Call ID: call\\_xTkJr8njRY0geNz43ZvGkX0R Args: query: task decomposition ================================= Tool Message ================================= Name: retrieve\\_context Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'} Content: Task decomposition can be done by... Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'} Content: Component One: Planning... ================================== Ai Message ================================== Task decomposition refers to... ``` Check out the [LangSmith trace](https://smith.langchain.com/public/a117a1f8-c96c-4c16-a285-00b85646118e/r).  ## Setup ### Installation This tutorial requires these langchain dependencies:  ```bash pip theme={null} pip install langchain langchain-text-splitters langchain-community bs4 ``` ```bash uv theme={null} uv add langchain langchain-text-splitters langchain-community bs4 ``` ```bash conda theme={null} conda install langchain langchain-text-splitters langchain-community bs4 -c conda-forge ```  For more details, see our [Installation guide](/oss/python/langchain/install). ### LangSmith Many of the applications you build with LangChain will contain multiple steps with multiple invocations of LLM calls. As these applications get more complex, it becomes crucial to be able to inspect what exactly is going on inside your chain or agent. The best way to do this is with [LangSmith](https://smith.langchain.com). After you sign up at the link above, make sure to set your environment variables to start logging traces: ```shell theme={null} export LANGSMITH\\_TRACING=\"true\" export LANGSMITH\\_API\\_KEY=\"...\" ``` Or, set them in Python: ```python theme={null} import getpass import os os.environ[\"LANGSMITH\\_TRACING\"] = \"true\" os.environ[\"LANGSMITH\\_API\\_KEY\"] = getpass.getpass() ``` ### Components We will need to select three components from LangChain's suite of integrations. Select a chat model:   \ud83d\udc49 Read the [OpenAI chat model integration docs](/oss/python/integrations/chat/openai/) ```shell theme={null} pip install -U \"langchain[openai]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"OPENAI\\_API\\_KEY\"] = \"sk-...\" model = init\\_chat\\_model(\"gpt-4.1\") ``` ```python Model Class theme={null} import os from langchain\\_openai import ChatOpenAI os.environ[\"OPENAI\\_API\\_KEY\"] = \"sk-...\" model = ChatOpenAI(model=\"gpt-4.1\") ```    \ud83d\udc49 Read the [Anthropic chat model integration docs](/oss/python/integrations/chat/anthropic/) ```shell theme={null} pip install -U \"langchain[anthropic]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"ANTHROPIC\\_API\\_KEY\"] = \"sk-...\" model = init\\_chat\\_model(\"claude-sonnet-4-5-20250929\") ``` ```python Model Class theme={null} import os from langchain\\_anthropic import ChatAnthropic os.environ[\"ANTHROPIC\\_API\\_KEY\"] = \"sk-...\" model = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\") ```    \ud83d\udc49 Read the [Azure chat model integration docs](/oss/python/integrations/chat/azure\\_chat\\_openai/) ```shell theme={null} pip install -U \"langchain[openai]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"AZURE\\_OPENAI\\_API\\_KEY\"] = \"...\" os.environ[\"AZURE\\_OPENAI\\_ENDPOINT\"] = \"...\" os.environ[\"OPENAI\\_API\\_VERSION\"] = \"2025-03-01-preview\" model = init\\_chat\\_model( \"azure\\_openai:gpt-4.1\", azure\\_deployment=os.environ[\"AZURE\\_OPENAI\\_DEPLOYMENT\\_NAME\"], ) ``` ```python Model Class theme={null} import os from langchain\\_openai import AzureChatOpenAI os.environ[\"AZURE\\_OPENAI\\_API\\_KEY\"] = \"...\" os.environ[\"AZURE\\_OPENAI\\_ENDPOINT\"] = \"...\" os.environ[\"OPENAI\\_API\\_VERSION\"] = \"2025-03-01-preview\" model = AzureChatOpenAI( model=\"gpt-4.1\", azure\\_deployment=os.environ[\"AZURE\\_OPENAI\\_DEPLOYMENT\\_NAME\"] ) ```    \ud83d\udc49 Read the [Google GenAI chat model integration docs](/oss/python/integrations/chat/google\\_generative\\_ai/) ```shell theme={null} pip install -U \"langchain[google-genai]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"GOOGLE\\_API\\_KEY\"] = \"...\" model = init\\_chat\\_model(\"google\\_genai:gemini-2.5-flash-lite\") ``` ```python Model Class theme={null} import os from langchain\\_google\\_genai import ChatGoogleGenerativeAI os.environ[\"GOOGLE\\_API\\_KEY\"] = \"...\" model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\") ```    \ud83d\udc49 Read the [AWS Bedrock chat model integration docs](/oss/python/integrations/chat/bedrock/) ```shell theme={null} pip install -U \"langchain[aws]\" ```  ```python init\\_chat\\_model theme={null} from langchain.chat\\_models import init\\_chat\\_model # Follow the steps here to configure your credentials: # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html model = init\\_chat\\_model( \"anthropic.claude-3-5-sonnet-20240620-v1:0\", model\\_provider=\"bedrock\\_converse\", ) ``` ```python Model Class theme={null} from langchain\\_aws import ChatBedrock model = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\") ```   \ud83d\udc49 Read the [HuggingFace chat model integration docs](/oss/python/integrations/chat/huggingface/) ```shell theme={null} pip install -U \"langchain[huggingface]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"HUGGINGFACEHUB\\_API\\_TOKEN\"] = \"hf\\_...\" model = init\\_chat\\_model( \"microsoft/Phi-3-mini-4k-instruct\", model\\_provider=\"huggingface\", temperature=0.7, max\\_tokens=1024, ) ``` ```python Model Class theme={null} import os from langchain\\_huggingface import ChatHuggingFace, HuggingFaceEndpoint os.environ[\"HUGGINGFACEHUB\\_API\\_TOKEN\"] = \"hf\\_...\" llm = HuggingFaceEndpoint( repo\\_id=\"microsoft/Phi-3-mini-4k-instruct\", temperature=0.7, max\\_length=1024, ) model = ChatHuggingFace(llm=llm) ```     Select an embeddings model:   ```shell theme={null} pip install -U \"langchain-openai\" ``` ```python theme={null} import getpass import os if not os.environ.get(\"OPENAI\\_API\\_KEY\"): os.environ[\"OPENAI\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \") from langchain\\_openai import OpenAIEmbeddings embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\") ```   ```shell theme={null} pip install -U \"langchain-openai\" ``` ```python theme={null} import getpass import os if not os.environ.get(\"AZURE\\_OPENAI\\_API\\_KEY\"): os.environ[\"AZURE\\_OPENAI\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for Azure: \") from langchain\\_openai import AzureOpenAIEmbeddings embeddings = AzureOpenAIEmbeddings( azure\\_endpoint=os.environ[\"AZURE\\_OPENAI\\_ENDPOINT\"], azure\\_deployment=os.environ[\"AZURE\\_OPENAI\\_DEPLOYMENT\\_NAME\"], openai\\_api\\_version=os.environ[\"AZURE\\_OPENAI\\_API\\_VERSION\"], ) ```   ```shell theme={null} pip install -qU langchain-google-genai ``` ```python theme={null} import getpass import os if not os.environ.get(\"GOOGLE\\_API\\_KEY\"): os.environ[\"GOOGLE\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for Google Gemini: \") from langchain\\_google\\_genai import GoogleGenerativeAIEmbeddings embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\") ```   ```shell theme={null} pip install -qU langchain-google-vertexai ``` ```python theme={null} from langchain\\_google\\_vertexai import VertexAIEmbeddings embeddings = VertexAIEmbeddings(model=\"text-embedding-005\") ```   ```shell theme={null} pip install -qU langchain-aws ``` ```python theme={null} from langchain\\_aws import BedrockEmbeddings embeddings = BedrockEmbeddings(model\\_id=\"amazon.titan-embed-text-v2:0\") ```   ```shell theme={null} pip install -qU langchain-huggingface ``` ```python theme={null} from langchain\\_huggingface import HuggingFaceEmbeddings embeddings = HuggingFaceEmbeddings(model\\_name=\"sentence-transformers/all-mpnet-base-v2\") ```   ```shell theme={null} pip install -qU langchain-ollama ``` ```python theme={null} from langchain\\_ollama import OllamaEmbeddings embeddings = OllamaEmbeddings(model=\"llama3\") ```   ```shell theme={null} pip install -qU langchain-cohere ``` ```python theme={null} import getpass import os if not os.environ.get(\"COHERE\\_API\\_KEY\"): os.environ[\"COHERE\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for Cohere: \") from langchain\\_cohere import CohereEmbeddings embeddings = CohereEmbeddings(model=\"embed-english-v3.0\") ```   ```shell theme={null} pip install -qU langchain-mistralai ``` ```python theme={null} import getpass import os if not os.environ.get(\"MISTRALAI\\_API\\_KEY\"): os.environ[\"MISTRALAI\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for MistralAI: \") from langchain\\_mistralai import MistralAIEmbeddings embeddings = MistralAIEmbeddings(model=\"mistral-embed\") ```   ```shell theme={null} pip install -qU langchain-nomic ``` ```python theme={null} import getpass import os if not os.environ.get(\"NOMIC\\_API\\_KEY\"): os.environ[\"NOMIC\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for Nomic: \") from langchain\\_nomic import NomicEmbeddings embeddings = NomicEmbeddings(model=\"nomic-embed-text-v1.5\") ```   ```shell theme={null} pip install -qU langchain-nvidia-ai-endpoints ``` ```python theme={null} import getpass import os if not os.environ.get(\"NVIDIA\\_API\\_KEY\"): os.environ[\"NVIDIA\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for NVIDIA: \") from langchain\\_nvidia\\_ai\\_endpoints import NVIDIAEmbeddings embeddings = NVIDIAEmbeddings(model=\"NV-Embed-QA\") ```   ```shell theme={null} pip install -qU langchain-voyageai ``` ```python theme={null} import getpass import os if not os.environ.get(\"VOYAGE\\_API\\_KEY\"): os.environ[\"VOYAGE\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for Voyage AI: \") from langchain-voyageai import VoyageAIEmbeddings embeddings = VoyageAIEmbeddings(model=\"voyage-3\") ```   ```shell theme={null} pip install -qU langchain-ibm ``` ```python theme={null} import getpass import os if not os.environ.get(\"WATSONX\\_APIKEY\"): os.environ[\"WATSONX\\_APIKEY\"] = getpass.getpass(\"Enter API key for IBM watsonx: \") from langchain\\_ibm import WatsonxEmbeddings embeddings = WatsonxEmbeddings( model\\_id=\"ibm/slate-125m-english-rtrvr\", url=\"https://us-south.ml.cloud.ibm.com\", project\\_id=\"\", ) ```   ```shell theme={null} pip install -qU langchain-core ``` ```python theme={null} from langchain\\_core.embeddings import DeterministicFakeEmbedding embeddings = DeterministicFakeEmbedding(size=4096) ```   ```shell theme={null} pip install -qU langchain-isaacus ``` ```python theme={null} import getpass import os if not os.environ.get(\"ISAACUS\\_API\\_KEY\"): os.environ[\"ISAACUS\\_API\\_KEY\"] = getpass.getpass(\"Enter API key for Isaacus: \") from langchain\\_isaacus import IsaacusEmbeddings embeddings = IsaacusEmbeddings(model=\"kanon-2-embedder\") ```   Select a vector store:   ```shell theme={null} pip install -U \"langchain-core\" ``` ```python theme={null} from langchain\\_core.vectorstores import InMemoryVectorStore vector\\_store = InMemoryVectorStore(embeddings) ```   ```shell theme={null} pip install -U \"langchain-astradb\" ``` ```python theme={null} from langchain\\_astradb import AstraDBVectorStore vector\\_store = AstraDBVectorStore( embedding=embeddings, api\\_endpoint=ASTRA\\_DB\\_API\\_ENDPOINT, collection\\_name=\"astra\\_vector\\_langchain\", token=ASTRA\\_DB\\_APPLICATION\\_TOKEN, namespace=ASTRA\\_DB\\_NAMESPACE, ) ```   ```shell theme={null} pip install -qU langchain-chroma ``` ```python theme={null} from langchain\\_chroma import Chroma vector\\_store = Chroma( collection\\_name=\"example\\_collection\", embedding\\_function=embeddings, persist\\_directory=\"./chroma\\_langchain\\_db\", # Where to save data locally, remove if not necessary ) ```   ```shell theme={null} pip install -qU langchain-community faiss-cpu ``` ```python theme={null} import faiss from langchain\\_community.docstore.in\\_memory import InMemoryDocstore from langchain\\_community.vectorstores import FAISS embedding\\_dim = len(embeddings.embed\\_query(\"hello world\")) index = faiss.IndexFlatL2(embedding\\_dim) vector\\_store = FAISS( embedding\\_function=embeddings, index=index, docstore=InMemoryDocstore(), index\\_to\\_docstore\\_id={}, ) ```   ```shell theme={null} pip install -qU langchain-milvus ``` ```python theme={null} from langchain\\_milvus import Milvus URI = \"./milvus\\_example.db\" vector\\_store = Milvus( embedding\\_function=embeddings, connection\\_args={\"uri\": URI}, index\\_params={\"index\\_type\": \"FLAT\", \"metric\\_type\": \"L2\"}, ) ```   ```shell theme={null} pip install -qU langchain-mongodb ``` ```python theme={null} from langchain\\_mongodb import MongoDBAtlasVectorSearch vector\\_store = MongoDBAtlasVectorSearch( embedding=embeddings, collection=MONGODB\\_COLLECTION, index\\_name=ATLAS\\_VECTOR\\_SEARCH\\_INDEX\\_NAME, relevance\\_score\\_fn=\"cosine\", ) ```   ```shell theme={null} pip install -qU langchain-postgres ``` ```python theme={null} from langchain\\_postgres import PGVector vector\\_store = PGVector( embeddings=embeddings, collection\\_name=\"my\\_docs\", connection=\"postgresql+psycopg://...\", ) ```   ```shell theme={null} pip install -qU langchain-postgres ``` ```python theme={null} from langchain\\_postgres import PGEngine, PGVectorStore pg\\_engine = PGEngine.from\\_connection\\_string( url=\"postgresql+psycopg://...\" ) vector\\_store = PGVectorStore.create\\_sync( engine=pg\\_engine, table\\_name='test\\_table', embedding\\_service=embedding ) ```   ```shell theme={null} pip install -qU langchain-pinecone ``` ```python theme={null} from langchain\\_pinecone import PineconeVectorStore from pinecone import Pinecone pc = Pinecone(api\\_key=...) index = pc.Index(index\\_name) vector\\_store = PineconeVectorStore(embedding=embeddings, index=index) ```   ```shell theme={null} pip install -qU langchain-qdrant ``` ```python theme={null} from qdrant\\_client.models import Distance, VectorParams from langchain\\_qdrant import QdrantVectorStore from qdrant\\_client import QdrantClient client = QdrantClient(\":memory:\") vector\\_size = len(embeddings.embed\\_query(\"sample text\")) if not client.collection\\_exists(\"test\"): client.create\\_collection( collection\\_name=\"test\", vectors\\_config=VectorParams(size=vector\\_size, distance=Distance.COSINE) ) vector\\_store = QdrantVectorStore( client=client, collection\\_name=\"test\", embedding=embeddings, ) ```   ## 1. Indexing  \\*\\*This section is an abbreviated version of the content in the [semantic search tutorial](/oss/python/langchain/knowledge-base).\\*\\* If your data is already indexed and available for search (i.e., you have a function to execute a search), or if you're comfortable with [document loaders](/oss/python/langchain/retrieval#document\\_loaders), [embeddings](/oss/python/langchain/retrieval#embedding\\_models), and [vector stores](/oss/python/langchain/retrieval#vectorstores), feel free to skip to the next section on [retrieval and generation](/oss/python/langchain/rag#2-retrieval-and-generation).  Indexing commonly works as follows: 1. \\*\\*Load\\*\\*: First we need to load our data. This is done with [Document Loaders](/oss/python/langchain/retrieval#document\\_loaders). 2. \\*\\*Split\\*\\*: [Text splitters](/oss/python/langchain/retrieval#text\\_splitters) break large `Documents` into smaller chunks. This is useful both for indexing data and passing it into a model, as large chunks are harder to search over and won't fit in a model's finite context window. 3. \\*\\*Store\\*\\*: We need somewhere to store and index our splits, so that they can be searched over later. This is often done using a [VectorStore](/oss/python/langchain/retrieval#vectorstores) and [Embeddings](/oss/python/langchain/retrieval#embedding\\_models) model. ### Loading documents We need to first load the blog post contents. We can use [DocumentLoaders](/oss/python/langchain/retrieval#document\\_loaders) for this, which are objects that load in data from a source and return a list of [Document](https://reference.langchain.com/python/langchain\\_core/documents/#langchain\\_core.documents.base.Document) objects. In this case we'll use the [`WebBaseLoader`](/oss/python/integrations/document\\_loaders/web\\_base), which uses `urllib` to load HTML from web URLs and `BeautifulSoup` to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the `BeautifulSoup` parser via `bs\\_kwargs` (see [BeautifulSoup docs](https://beautiful-soup-4.readthedocs.io/en/latest/#beautifulsoup)). In this case only HTML tags with class \u201cpost-content\u201d, \u201cpost-title\u201d, or \u201cpost-header\u201d are relevant, so we'll remove all others. ```python theme={null} import bs4 from langchain\\_community.document\\_loaders import WebBaseLoader # Only keep post title, headers, and content from the full HTML. bs4\\_strainer = bs4.SoupStrainer(class\\_=(\"post-title\", \"post-header\", \"post-content\")) loader = WebBaseLoader( web\\_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",), bs\\_kwargs={\"parse\\_only\": bs4\\_strainer}, ) docs = loader.load() assert len(docs) == 1 print(f\"Total characters: {len(docs[0].page\\_content)}\") ``` ```output theme={null} Total characters: 43131 ``` ```python theme={null} print(docs[0].page\\_content[:500]) ``` ```output theme={null} LLM Powered Autonomous Agents Date: June 23, 2023 | Estimated Reading Time: 31 min | Author: Lilian Weng Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver. Agent System Overview# In ``` \\*\\*Go deeper\\*\\* `DocumentLoader`: Object that loads data from a source as list of `Documents`. \\* [Integrations](/oss/python/integrations/document\\_loaders/): 160+ integrations to choose from. \\* [`BaseLoader`](https://reference.langchain.com/python/langchain\\_core/document\\_loaders/#langchain\\_core.document\\_loaders.BaseLoader): API reference for the base interface. ### Splitting documents Our loaded document is over 42k characters which is too long to fit into the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs. To handle this we'll split the [`Document`](https://reference.langchain.com/python/langchain\\_core/documents/#langchain\\_core.documents.base.Document) into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time. As in the [semantic search tutorial](/oss/python/langchain/knowledge-base), we use a `RecursiveCharacterTextSplitter`, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases. ```python theme={null} from langchain\\_text\\_splitters import RecursiveCharacterTextSplitter text\\_splitter = RecursiveCharacterTextSplitter( chunk\\_size=1000, # chunk size (characters) chunk\\_overlap=200, # chunk overlap (characters) add\\_start\\_index=True, # track index in original document ) all\\_splits = text\\_splitter.split\\_documents(docs) print(f\"Split blog post into {len(all\\_splits)} sub-documents.\") ``` ```output theme={null} Split blog post into 66 sub-documents. ``` \\*\\*Go deeper\\*\\* `TextSplitter`: Object that splits a list of [`Document`](https://reference.langchain.com/python/langchain\\_core/documents/#langchain\\_core.documents.base.Document) objects into smaller chunks for storage and retrieval. \\* [Integrations](/oss/python/integrations/splitters/) \\* [Interface](https://python.langchain.com/api\\_reference/text\\_splitters/base/langchain\\_text\\_splitters.base.TextSplitter.html): API reference for the base interface. ### Storing documents Now we need to index our 66 text chunks so that we can search over them at runtime. Following the [semantic search tutorial](/oss/python/langchain/knowledge-base), our approach is to [embed](/oss/python/langchain/retrieval#embedding\\_models/) the contents of each document split and insert these embeddings into a [vector store](/oss/python/langchain/retrieval#vectorstores/). Given an input query, we can then use vector search to retrieve relevant documents. We can embed and store all of our document splits in a single command using the vector store and embeddings model selected at the [start of the tutorial](/oss/python/langchain/rag#components). ```python theme={null} document\\_ids = vector\\_store.add\\_documents(documents=all\\_splits) print(document\\_ids[:3]) ``` ```output theme={null} ['07c18af6-ad58-479a-bfb1-d508033f9c64', '9000bf8e-1993-446f-8d4d-f4e507ba4b8f', 'ba3b5d14-bed9-4f5f-88be-44c88aedc2e6'] ``` \\*\\*Go deeper\\*\\* `Embeddings`: Wrapper around a text embedding model, used for converting text to embeddings. \\* [Integrations](/oss/python/integrations/text\\_embedding/): 30+ integrations to choose from. \\* [Interface](https://reference.langchain.com/python/langchain\\_core/embeddings/#langchain\\_core.embeddings.embeddings.Embeddings): API reference for the base interface. `VectorStore`: Wrapper around a vector database, used for storing and querying embeddings. \\* [Integrations](/oss/python/integrations/vectorstores/): 40+ integrations to choose from. \\* [Interface](https://python.langchain.com/api\\_reference/core/vectorstores/langchain\\_core.vectorstores.base.VectorStore.html): API reference for the base interface. This completes the \\*\\*Indexing\\*\\* portion of the pipeline. At this point we have a query-able vector store containing the chunked contents of our blog post. Given a user question, we should ideally be able to return the snippets of the blog post that answer the question. ## 2. Retrieval and Generation RAG applications commonly work as follows: 1. \\*\\*Retrieve\\*\\*: Given a user input, relevant splits are retrieved from storage using a [Retriever](/oss/python/langchain/retrieval#retrievers). 2. \\*\\*Generate\\*\\*: A [model](/oss/python/langchain/models) produces an answer using a prompt that includes both the question with the retrieved data Now let's write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer. We will demonstrate: 1. A RAG [agent](#rag-agents) that executes searches with a simple tool. This is a good general-purpose implementation. 2. A two-step RAG [chain](#rag-chains) that uses just a single LLM call per query. This is a fast and effective method for simple queries. ### RAG agents One formulation of a RAG application is as a simple [agent](/oss/python/langchain/agents) with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a [tool](/oss/python/langchain/tools) that wraps our vector store: ```python theme={null} from langchain.tools import tool @tool(response\\_format=\"content\\_and\\_artifact\") def retrieve\\_context(query: str): \"\"\"Retrieve information to help answer a query.\"\"\" retrieved\\_docs = vector\\_store.similarity\\_search(query, k=2) serialized = \"\\n\\n\".join( (f\"Source: {doc.metadata}\\nContent: {doc.page\\_content}\") for doc in retrieved\\_docs ) return serialized, retrieved\\_docs ```  Here we use the [tool decorator](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool) to configure the tool to attach raw documents as [artifacts](/oss/python/langchain/messages#param-artifact) to each [ToolMessage](/oss/python/langchain/messages#tool-message). This will let us access document metadata in our application, separate from the stringified representation that is sent to the model.   Retrieval tools are not limited to a single string `query` argument, as in the above example. You can force the LLM to specify additional search parameters by adding arguments\u2014 for example, a category: ```python theme={null} from typing import Literal def retrieve\\_context(query: str, section: Literal[\"beginning\", \"middle\", \"end\"]): ```  Given our tool, we can construct the agent: ```python theme={null} from langchain.agents import create\\_agent tools = [retrieve\\_context] # If desired, specify custom instructions prompt = ( \"You have access to a tool that retrieves context from a blog post. \" \"Use the tool to help answer user queries.\" ) agent = create\\_agent(model, tools, system\\_prompt=prompt) ``` Let's test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer: ```python theme={null} query = ( \"What is the standard method for Task Decomposition?\\n\\n\" \"Once you get the answer, look up common extensions of that method.\" ) for event in agent.stream( {\"messages\": [{\"role\": \"user\", \"content\": query}]}, stream\\_mode=\"values\", ): event[\"messages\"][-1].pretty\\_print() ``` ``` ================================ Human Message ================================= What is the standard method for Task Decomposition? Once you get the answer, look up common extensions of that method. ================================== Ai Message ================================== Tool Calls: retrieve\\_context (call\\_d6AVxICMPQYwAKj9lgH4E337) Call ID: call\\_d6AVxICMPQYwAKj9lgH4E337 Args: query: standard method for Task Decomposition ================================= Tool Message ================================= Name: retrieve\\_context Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'} Content: Task decomposition can be done... Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'} Content: Component One: Planning... ================================== Ai Message ================================== Tool Calls: retrieve\\_context (call\\_0dbMOw7266jvETbXWn4JqWpR) Call ID: call\\_0dbMOw7266jvETbXWn4JqWpR Args: query: common extensions of the standard method for Task Decomposition ================================= Tool Message ================================= Name: retrieve\\_context Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'} Content: Task decomposition can be done... Source: {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'} Content: Component One: Planning... ================================== Ai Message ================================== The standard method for Task Decomposition often used is the Chain of Thought (CoT)... ``` Note that the agent: 1. Generates a query to search for a standard method for task decomposition; 2. Receiving the answer, generates a second query to search for common extensions of it; 3. Having received all necessary context, answers the question. We can see the full sequence of steps, along with latency and other metadata, in the [LangSmith trace](https://smith.langchain.com/public/7b42d478-33d2-4631-90a4-7cb731681e88/r).  You can add a deeper level of control and customization using the [LangGraph](/oss/python/langgraph/overview) framework directly\u2014 for example, you can add steps to grade document relevance and rewrite search queries. Check out LangGraph's [Agentic RAG tutorial](/oss/python/langgraph/agentic-rag) for more advanced formulations.  ### RAG chains In the above [agentic RAG](#rag-agents) formulation we allow the LLM to use its discretion in generating a [tool call](/oss/python/langchain/models#tool-calling) to help answer user queries. This is a good general-purpose solution, but comes with some trade-offs: | \u2705 Benefits | \u26a0\ufe0f Drawbacks | | ---------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------- | | \\*\\*Search only when needed\\*\\* \u2013 The LLM can handle greetings, follow-ups, and simple queries without triggering unnecessary searches. | \\*\\*Two inference calls\\*\\* \u2013 When a search is performed, it requires one call to generate the query and another to produce the final response. | | \\*\\*Contextual search queries\\*\\* \u2013 By treating search as a tool with a `query` input, the LLM crafts its own queries that incorporate conversational context. | \\*\\*Reduced control\\*\\* \u2013 The LLM may skip searches when they are actually needed, or issue extra searches when unnecessary. | | \\*\\*Multiple searches allowed\\*\\* \u2013 The LLM can execute several searches in support of a single user query. | | Another common approach is a two-step chain, in which we always run a search (potentially using the raw user query) and incorporate the result as context for a single LLM query. This results in a single inference call per query, buying reduced latency at the expense of flexibility. In this approach we no longer call the model in a loop, but instead make a single pass. We can implement this chain by removing tools from the agent and instead incorporating the retrieval step into a custom prompt: ```python theme={null} from langchain.agents.middleware import dynamic\\_prompt, ModelRequest @dynamic\\_prompt def prompt\\_with\\_context(request: ModelRequest) -> str: \"\"\"Inject context into state messages.\"\"\" last\\_query = request.state[\"messages\"][-1].text retrieved\\_docs = vector\\_store.similarity\\_search(last\\_query) docs\\_content = \"\\n\\n\".join(doc.page\\_content for doc in retrieved\\_docs) system\\_message = ( \"You are a helpful assistant. Use the following context in your response:\" f\"\\n\\n{docs\\_content}\" ) return system\\_message agent = create\\_agent(model, tools=[], middleware=[prompt\\_with\\_context]) ``` Let's try this out: ```python theme={null} query = \"What is task decomposition?\" for step in agent.stream( {\"messages\": [{\"role\": \"user\", \"content\": query}]}, stream\\_mode=\"values\", ): step[\"messages\"][-1].pretty\\_print() ``` ``` ================================ Human Message ================================= What is task decomposition? ================================== Ai Message ================================== Task decomposition is... ``` In the [LangSmith trace](https://smith.langchain.com/public/0322904b-bc4c-4433-a568-54c6b31bbef4/r/9ef1c23e-380e-46bf-94b3-d8bb33df440c) we can see the retrieved context incorporated into the model prompt. This is a fast and effective method for simple queries in constrained settings, when we typically do want to run user queries through semantic search to pull additional context.  The above [RAG chain](#rag-chains) incorporates retrieved context into a single system message for that run. As in the [agentic RAG](#rag-agents) formulation, we sometimes want to include raw source documents in the application state to have access to document metadata. We can do this for the two-step chain case by: 1. Adding a key to the state to store the retrieved documents 2. Adding a new node via a [pre-model hook](/oss/python/langchain/agents#pre-model-hook) to populate that key (as well as inject the context). ```python theme={null} from typing import Any from langchain\\_core.documents import Document from langchain.agents.middleware import AgentMiddleware, AgentState class State(AgentState): context: list[Document] class RetrieveDocumentsMiddleware(AgentMiddleware[State]): state\\_schema = State def before\\_model(self, state: AgentState) -> dict[str, Any] | None: last\\_message = state[\"messages\"][-1] retrieved\\_docs = vector\\_store.similarity\\_search(last\\_message.text) docs\\_content = \"\\n\\n\".join(doc.page\\_content for doc in retrieved\\_docs) augmented\\_message\\_content = ( f\"{last\\_message.text}\\n\\n\" \"Use the following context to answer the query:\\n\" f\"{docs\\_content}\" ) return { \"messages\": [last\\_message.model\\_copy(update={\"content\": augmented\\_message\\_content})], \"context\": retrieved\\_docs, } agent = create\\_agent( model, tools=[], middleware=[RetrieveDocumentsMiddleware()], ) ```  ## Next steps Now that we've implemented a simple RAG application via [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent), we can easily incorporate new features and go deeper: \\* [Stream](/oss/python/langchain/streaming) tokens and other information for responsive user experiences \\* Add [conversational memory](/oss/python/langchain/short-term-memory) to support multi-turn interactions \\* Add [long-term memory](/oss/python/langchain/long-term-memory) to support memory across conversational threads \\* Add [structured responses](/oss/python/langchain/structured-output) \\* Deploy your application with [LangSmith Deployments](/langsmith/deployments) \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/rag.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/retrieval.md",
      "title": null,
      "raw_content": "# Retrieval Large Language Models (LLMs) are powerful, but they have two key limitations: \\* \\*\\*Finite context\\*\\* \u2014 they can\u2019t ingest entire corpora at once. \\* \\*\\*Static knowledge\\*\\* \u2014 their training data is frozen at a point in time. Retrieval addresses these problems by fetching relevant external knowledge at query time. This is the foundation of \\*\\*Retrieval-Augmented Generation (RAG)\\*\\*: enhancing an LLM\u2019s answers with context-specific information. ## Building a knowledge base A \\*\\*knowledge base\\*\\* is a repository of documents or structured data used during retrieval. If you need a custom knowledge base, you can use LangChain\u2019s document loaders and vector stores to build one from your own data.  If you already have a knowledge base (e.g., a SQL database, CRM, or internal documentation system), you do \\*\\*not\\*\\* need to rebuild it. You can: \\* Connect it as a \\*\\*tool\\*\\* for an agent in Agentic RAG. \\* Query it and supply the retrieved content as context to the LLM [(2-Step RAG)](#2-step-rag).  See the following tutorial to build a searchable knowledge base and minimal RAG workflow:  Learn how to create a searchable knowledge base from your own data using LangChain\u2019s document loaders, embeddings, and vector stores. In this tutorial, you\u2019ll build a search engine over a PDF, enabling retrieval of passages relevant to a query. You\u2019ll also implement a minimal RAG workflow on top of this engine to see how external knowledge can be integrated into LLM reasoning.  ### From retrieval to RAG Retrieval allows LLMs to access relevant context at runtime. But most real-world applications go one step further: they \\*\\*integrate retrieval with generation\\*\\* to produce grounded, context-aware answers. This is the core idea behind \\*\\*Retrieval-Augmented Generation (RAG)\\*\\*. The retrieval pipeline becomes a foundation for a broader system that combines search with generation. ### Retrieval Pipeline A typical retrieval workflow looks like this: ```mermaid theme={null} flowchart LR S([\"Sources  \n(Google Drive, Slack, Notion, etc.)\"]) --> L[Document Loaders] L --> A([Documents]) A --> B[Split into chunks] B --> C[Turn into embeddings] C --> D[(Vector Store)] Q([User Query]) --> E[Query embedding] E --> D D --> F[Retriever] F --> G[LLM uses retrieved info] G --> H([Answer]) ``` Each component is modular: you can swap loaders, splitters, embeddings, or vector stores without rewriting the app\u2019s logic. ### Building Blocks   Ingest data from external sources (Google Drive, Slack, Notion, etc.), returning standardized [`Document`](https://reference.langchain.com/python/langchain\\_core/documents/#langchain\\_core.documents.base.Document) objects.   Break large docs into smaller chunks that will be retrievable individually and fit within a model's context window.   An embedding model turns text into a vector of numbers so that texts with similar meaning land close together in that vector space.   Specialized databases for storing and searching embeddings.   A retriever is an interface that returns documents given an unstructured query.   ## RAG Architectures RAG can be implemented in multiple ways, depending on your system's needs. We outline each type in the sections below. | Architecture | Description | Control | Flexibility | Latency | Example Use Case | | --------------- | -------------------------------------------------------------------------- | --------- | ----------- | ---------- | ------------------------------------------------- | | \\*\\*2-Step RAG\\*\\* | Retrieval always happens before generation. Simple and predictable | \u2705 High | \u274c Low | \u26a1 Fast | FAQs, documentation bots | | \\*\\*Agentic RAG\\*\\* | An LLM-powered agent decides \\*when\\* and \\*how\\* to retrieve during reasoning | \u274c Low | \u2705 High | \u23f3 Variable | Research assistants with access to multiple tools | | \\*\\*Hybrid\\*\\* | Combines characteristics of both approaches with validation steps | \u2696\ufe0f Medium | \u2696\ufe0f Medium | \u23f3 Variable | Domain-specific Q\\&A with quality validation |  \\*\\*Latency\\*\\*: Latency is generally more \\*\\*predictable\\*\\* in \\*\\*2-Step RAG\\*\\*, as the maximum number of LLM calls is known and capped. This predictability assumes that LLM inference time is the dominant factor. However, real-world latency may also be affected by the performance of retrieval steps\u2014such as API response times, network delays, or database queries\u2014which can vary based on the tools and infrastructure in use.  ### 2-step RAG In \\*\\*2-Step RAG\\*\\*, the retrieval step is always executed before the generation step. This architecture is straightforward and predictable, making it suitable for many applications where the retrieval of relevant documents is a clear prerequisite for generating an answer. ```mermaid theme={null} graph LR A[User Question] --> B[\"Retrieve Relevant Documents\"] B --> C[\"Generate Answer\"] C --> D[Return Answer to User] %% Styling classDef startend fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff classDef process fill:#1976d2,stroke:#0d47a1,stroke-width:1.5px,color:#fff class A,D startend class B,C process ```  See how to build a Q\\&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation. This tutorial walks through two approaches: \\* A \\*\\*RAG agent\\*\\* that runs searches with a flexible tool\u2014great for general-purpose use. \\* A \\*\\*2-step RAG\\*\\* chain that requires just one LLM call per query\u2014fast and efficient for simpler tasks.  ### Agentic RAG \\*\\*Agentic Retrieval-Augmented Generation (RAG)\\*\\* combines the strengths of Retrieval-Augmented Generation with agent-based reasoning. Instead of retrieving documents before answering, an agent (powered by an LLM) reasons step-by-step and decides \\*\\*when\\*\\* and \\*\\*how\\*\\* to retrieve information during the interaction.  The only thing an agent needs to enable RAG behavior is access to one or more \\*\\*tools\\*\\* that can fetch external knowledge \u2014 such as documentation loaders, web APIs, or database queries.  ```mermaid theme={null} graph LR A[User Input / Question] --> B[\"Agent (LLM)\"] B --> C{Need external info?} C -- Yes --> D[\"Search using tool(s)\"] D --> H{Enough to answer?} H -- No --> B H -- Yes --> I[Generate final answer] C -- No --> I I --> J[Return to user] %% Dark-mode friendly styling classDef startend fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff classDef decision fill:#f9a825,stroke:#f57f17,stroke-width:2px,color:#000 classDef process fill:#1976d2,stroke:#0d47a1,stroke-width:1.5px,color:#fff class A,J startend class B,D,I process class C,H decision ``` ```python theme={null} import requests from langchain.tools import tool from langchain.chat\\_models import init\\_chat\\_model from langchain.agents import create\\_agent @tool def fetch\\_url(url: str) -> str: \"\"\"Fetch text content from a URL\"\"\" response = requests.get(url, timeout=10.0) response.raise\\_for\\_status() return response.text system\\_prompt = \"\"\"\\ Use fetch\\_url when you need to fetch information from a web-page; quote relevant snippets. \"\"\" agent = create\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=[fetch\\_url], # A tool for retrieval [!code highlight] system\\_prompt=system\\_prompt, ) ```  This example implements an \\*\\*Agentic RAG system\\*\\* to assist users in querying LangGraph documentation. The agent begins by loading [llms.txt](https://llmstxt.org/), which lists available documentation URLs, and can then dynamically use a `fetch\\_documentation` tool to retrieve and process the relevant content based on the user\u2019s question. ```python theme={null} import requests from langchain.agents import create\\_agent from langchain.messages import HumanMessage from langchain.tools import tool from markdownify import markdownify ALLOWED\\_DOMAINS = [\"https://langchain-ai.github.io/\"] LLMS\\_TXT = 'https://langchain-ai.github.io/langgraph/llms.txt' @tool def fetch\\_documentation(url: str) -> str: # [!code highlight] \"\"\"Fetch and convert documentation from a URL\"\"\" if not any(url.startswith(domain) for domain in ALLOWED\\_DOMAINS): return ( \"Error: URL not allowed. \" f\"Must start with one of: {', '.join(ALLOWED\\_DOMAINS)}\" ) response = requests.get(url, timeout=10.0) response.raise\\_for\\_status() return markdownify(response.text) # We will fetch the content of llms.txt, so this can # be done ahead of time without requiring an LLM request. llms\\_txt\\_content = requests.get(LLMS\\_TXT).text # System prompt for the agent system\\_prompt = f\"\"\" You are an expert Python developer and technical assistant. Your primary role is to help users with questions about LangGraph and related tools. Instructions: 1. If a user asks a question you're unsure about \u2014 or one that likely involves API usage, behavior, or configuration \u2014 you MUST use the `fetch\\_documentation` tool to consult the relevant docs. 2. When citing documentation, summarize clearly and include relevant context from the content. 3. Do not use any URLs outside of the allowed domain. 4. If a documentation fetch fails, tell the user and proceed with your best expert understanding. You can access official documentation from the following approved sources: {llms\\_txt\\_content} You MUST consult the documentation to get up to date documentation before answering a user's question about LangGraph. Your answers should be clear, concise, and technically accurate. \"\"\" tools = [fetch\\_documentation] model = init\\_chat\\_model(\"claude-sonnet-4-0\", max\\_tokens=32\\_000) agent = create\\_agent( model=model, tools=tools, # [!code highlight] system\\_prompt=system\\_prompt, # [!code highlight] name=\"Agentic RAG\", ) response = agent.invoke({ 'messages': [ HumanMessage(content=( \"Write a short example of a langgraph agent using the \" \"prebuilt create react agent. the agent should be able \" \"to look up stock pricing information.\" )) ] }) print(response['messages'][-1].content) ```   See how to build a Q\\&A chatbot that can answer questions grounded in your data using Retrieval-Augmented Generation. This tutorial walks through two approaches: \\* A \\*\\*RAG agent\\*\\* that runs searches with a flexible tool\u2014great for general-purpose use. \\* A \\*\\*2-step RAG\\*\\* chain that requires just one LLM call per query\u2014fast and efficient for simpler tasks.  ### Hybrid RAG Hybrid RAG combines characteristics of both 2-Step and Agentic RAG. It introduces intermediate steps such as query preprocessing, retrieval validation, and post-generation checks. These systems offer more flexibility than fixed pipelines while maintaining some control over execution. Typical components include: \\* \\*\\*Query enhancement\\*\\*: Modify the input question to improve retrieval quality. This can involve rewriting unclear queries, generating multiple variations, or expanding queries with additional context. \\* \\*\\*Retrieval validation\\*\\*: Evaluate whether retrieved documents are relevant and sufficient. If not, the system may refine the query and retrieve again. \\* \\*\\*Answer validation\\*\\*: Check the generated answer for accuracy, completeness, and alignment with source content. If needed, the system can regenerate or revise the answer. The architecture often supports multiple iterations between these steps: ```mermaid theme={null} graph LR A[User Question] --> B[Query Enhancement] B --> C[Retrieve Documents] C --> D{Sufficient Info?} D -- No --> E[Refine Query] E --> C D -- Yes --> F[Generate Answer] F --> G{Answer Quality OK?} G -- No --> H{Try Different Approach?} H -- Yes --> E H -- No --> I[Return Best Answer] G -- Yes --> I I --> J[Return to User] classDef startend fill:#2e7d32,stroke:#1b5e20,stroke-width:2px,color:#fff classDef decision fill:#f9a825,stroke:#f57f17,stroke-width:2px,color:#000 classDef process fill:#1976d2,stroke:#0d47a1,stroke-width:1.5px,color:#fff class A,J startend class B,C,E,F,I process class D,G,H decision ``` This architecture is suitable for: \\* Applications with ambiguous or underspecified queries \\* Systems that require validation or quality control steps \\* Workflows involving multiple sources or iterative refinement  An example of \\*\\*Hybrid RAG\\*\\* that combines agentic reasoning with retrieval and self-correction.  \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/retrieval.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/runtime.md",
      "title": null,
      "raw_content": "# Runtime ## Overview LangChain's [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent) runs on LangGraph's runtime under the hood. LangGraph exposes a [`Runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.Runtime) object with the following information: 1. \\*\\*Context\\*\\*: static information like user id, db connections, or other dependencies for an agent invocation 2. \\*\\*Store\\*\\*: a [BaseStore](https://reference.langchain.com/python/langgraph/store/#langgraph.store.base.BaseStore) instance used for [long-term memory](/oss/python/langchain/long-term-memory) 3. \\*\\*Stream writer\\*\\*: an object used for streaming information via the `\"custom\"` stream mode  Runtime context provides \\*\\*dependency injection\\*\\* for your tools and middleware. Instead of hardcoding values or using global state, you can inject runtime dependencies (like database connections, user IDs, or configuration) when invoking your agent. This makes your tools more testable, reusable, and flexible.  You can access the runtime information within [tools](#inside-tools) and [middleware](#inside-middleware). ## Access When creating an agent with [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent), you can specify a `context\\_schema` to define the structure of the `context` stored in the agent [`Runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.Runtime). When invoking the agent, pass the `context` argument with the relevant configuration for the run: ```python theme={null} from dataclasses import dataclass from langchain.agents import create\\_agent @dataclass class Context: user\\_name: str agent = create\\_agent( model=\"gpt-5-nano\", tools=[...], context\\_schema=Context # [!code highlight] ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]}, context=Context(user\\_name=\"John Smith\") # [!code highlight] ) ``` ### Inside tools You can access the runtime information inside tools to: \\* Access the context \\* Read or write long-term memory \\* Write to the [custom stream](/oss/python/langchain/streaming#custom-updates) (ex, tool progress / updates) Use the `ToolRuntime` parameter to access the [`Runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.Runtime) object inside a tool. ```python theme={null} from dataclasses import dataclass from langchain.tools import tool, ToolRuntime # [!code highlight] @dataclass class Context: user\\_id: str @tool def fetch\\_user\\_email\\_preferences(runtime: ToolRuntime[Context]) -> str: # [!code highlight] \"\"\"Fetch the user's email preferences from the store.\"\"\" user\\_id = runtime.context.user\\_id # [!code highlight] preferences: str = \"The user prefers you to write a brief and polite email.\" if runtime.store: # [!code highlight] if memory := runtime.store.get((\"users\",), user\\_id): # [!code highlight] preferences = memory.value[\"preferences\"] return preferences ``` ### Inside middleware You can access runtime information in middleware to create dynamic prompts, modify messages, or control agent behavior based on user context. Use `request.runtime` to access the [`Runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.Runtime) object inside middleware decorators. The runtime object is available in the [`ModelRequest`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.ModelRequest) parameter passed to middleware functions. ```python theme={null} from dataclasses import dataclass from langchain.messages import AnyMessage from langchain.agents import create\\_agent, AgentState from langchain.agents.middleware import dynamic\\_prompt, ModelRequest, before\\_model, after\\_model from langgraph.runtime import Runtime @dataclass class Context: user\\_name: str # Dynamic prompts @dynamic\\_prompt def dynamic\\_system\\_prompt(request: ModelRequest) -> str: user\\_name = request.runtime.context.user\\_name # [!code highlight] system\\_prompt = f\"You are a helpful assistant. Address the user as {user\\_name}.\" return system\\_prompt # Before model hook @before\\_model def log\\_before\\_model(state: AgentState, runtime: Runtime[Context]) -> dict | None: # [!code highlight] print(f\"Processing request for user: {runtime.context.user\\_name}\") # [!code highlight] return None # After model hook @after\\_model def log\\_after\\_model(state: AgentState, runtime: Runtime[Context]) -> dict | None: # [!code highlight] print(f\"Completed request for user: {runtime.context.user\\_name}\") # [!code highlight] return None agent = create\\_agent( model=\"gpt-5-nano\", tools=[...], middleware=[dynamic\\_system\\_prompt, log\\_before\\_model, log\\_after\\_model], # [!code highlight] context\\_schema=Context ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"What's my name?\"}]}, context=Context(user\\_name=\"John Smith\") ) ``` \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/runtime.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/short-term-memory.md",
      "title": null,
      "raw_content": "# Short-term memory ## Overview Memory is a system that remembers information about previous interactions. For AI agents, memory is crucial because it lets them remember previous interactions, learn from feedback, and adapt to user preferences. As agents tackle more complex tasks with numerous user interactions, this capability becomes essential for both efficiency and user satisfaction. Short term memory lets your application remember previous interactions within a single thread or conversation.  A thread organizes multiple interactions in a session, similar to the way email groups messages in a single conversation.  Conversation history is the most common form of short-term memory. Long conversations pose a challenge to today's LLMs; a full history may not fit inside an LLM's context window, resulting in an context loss or errors. Even if your model supports the full context length, most LLMs still perform poorly over long contexts. They get \"distracted\" by stale or off-topic content, all while suffering from slower response times and higher costs. Chat models accept context using [messages](/oss/python/langchain/messages), which include instructions (a system message) and inputs (human messages). In chat applications, messages alternate between human inputs and model responses, resulting in a list of messages that grows longer over time. Because context windows are limited, many applications can benefit from using techniques to remove or \"forget\" stale information. ## Usage To add short-term memory (thread-level persistence) to an agent, you need to specify a `checkpointer` when creating an agent.  LangChain's agent manages short-term memory as a part of your agent's state. By storing these in the graph's state, the agent can access the full context for a given conversation while maintaining separation between different threads. State is persisted to a database (or memory) using a checkpointer so the thread can be resumed at any time. Short-term memory updates when the agent is invoked or a step (like a tool call) is completed, and the state is read at the start of each step.  ```python theme={null} from langchain.agents import create\\_agent from langgraph.checkpoint.memory import InMemorySaver # [!code highlight] agent = create\\_agent( \"gpt-5\", tools=[get\\_user\\_info], checkpointer=InMemorySaver(), # [!code highlight] ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! My name is Bob.\"}]}, {\"configurable\": {\"thread\\_id\": \"1\"}}, # [!code highlight] ) ``` ### In production In production, use a checkpointer backed by a database: ```shell theme={null} pip install langgraph-checkpoint-postgres ``` ```python theme={null} from langchain.agents import create\\_agent from langgraph.checkpoint.postgres import PostgresSaver # [!code highlight] DB\\_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\" with PostgresSaver.from\\_conn\\_string(DB\\_URI) as checkpointer: checkpointer.setup() # auto create tables in PostgresSql agent = create\\_agent( \"gpt-5\", tools=[get\\_user\\_info], checkpointer=checkpointer, # [!code highlight] ) ``` ## Customizing agent memory By default, agents use [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState) to manage short term memory, specifically the conversation history via a `messages` key. You can extend [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState) to add additional fields. Custom state schemas are passed to [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent) using the [`state\\_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state\\_schema) parameter. ```python theme={null} from langchain.agents import create\\_agent, AgentState from langgraph.checkpoint.memory import InMemorySaver class CustomAgentState(AgentState): # [!code highlight] user\\_id: str # [!code highlight] preferences: dict # [!code highlight] agent = create\\_agent( \"gpt-5\", tools=[get\\_user\\_info], state\\_schema=CustomAgentState, # [!code highlight] checkpointer=InMemorySaver(), ) # Custom state can be passed in invoke result = agent.invoke( { \"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}], \"user\\_id\": \"user\\_123\", # [!code highlight] \"preferences\": {\"theme\": \"dark\"} # [!code highlight] }, {\"configurable\": {\"thread\\_id\": \"1\"}}) ``` ## Common patterns With [short-term memory](#add-short-term-memory) enabled, long conversations can exceed the LLM's context window. Common solutions are:   Remove first or last N messages (before calling LLM)   Delete messages from LangGraph state permanently   Summarize earlier messages in the history and replace them with a summary   Custom strategies (e.g., message filtering, etc.)   This allows the agent to keep track of the conversation without exceeding the LLM's context window. ### Trim messages Most LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you're using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the `strategy` (e.g., keep the last `max\\_tokens`) to use for handling the boundary. To trim message history in an agent, use the [`@before\\_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.before\\_model) middleware decorator: ```python theme={null} from langchain.messages import RemoveMessage from langgraph.graph.message import REMOVE\\_ALL\\_MESSAGES from langgraph.checkpoint.memory import InMemorySaver from langchain.agents import create\\_agent, AgentState from langchain.agents.middleware import before\\_model from langgraph.runtime import Runtime from langchain\\_core.runnables import RunnableConfig from typing import Any @before\\_model def trim\\_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None: \"\"\"Keep only the last few messages to fit context window.\"\"\" messages = state[\"messages\"] if len(messages) <= 3: return None # No changes needed first\\_msg = messages[0] recent\\_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:] new\\_messages = [first\\_msg] + recent\\_messages return { \"messages\": [ RemoveMessage(id=REMOVE\\_ALL\\_MESSAGES), \\*new\\_messages ] } agent = create\\_agent( your\\_model\\_here, tools=your\\_tools\\_here, middleware=[trim\\_messages], checkpointer=InMemorySaver(), ) config: RunnableConfig = {\"configurable\": {\"thread\\_id\": \"1\"}} agent.invoke({\"messages\": \"hi, my name is bob\"}, config) agent.invoke({\"messages\": \"write a short poem about cats\"}, config) agent.invoke({\"messages\": \"now do the same but for dogs\"}, config) final\\_response = agent.invoke({\"messages\": \"what's my name?\"}, config) final\\_response[\"messages\"][-1].pretty\\_print() \"\"\" ================================== Ai Message ================================== Your name is Bob. You told me that earlier. If you'd like me to call you a nickname or use a different name, just say the word. \"\"\" ``` ### Delete messages You can delete messages from the graph state to manage the message history. This is useful when you want to remove specific messages or clear the entire message history. To delete messages from the graph state, you can use the `RemoveMessage`. For `RemoveMessage` to work, you need to use a state key with [`add\\_messages`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.message.add\\_messages) [reducer](/oss/python/langgraph/graph-api#reducers). The default [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState) provides this. To remove specific messages: ```python theme={null} from langchain.messages import RemoveMessage # [!code highlight] def delete\\_messages(state): messages = state[\"messages\"] if len(messages) > 2: # remove the earliest two messages return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]} # [!code highlight] ``` To remove \\*\\*all\\*\\* messages: ```python theme={null} from langgraph.graph.message import REMOVE\\_ALL\\_MESSAGES # [!code highlight] def delete\\_messages(state): return {\"messages\": [RemoveMessage(id=REMOVE\\_ALL\\_MESSAGES)]} # [!code highlight] ```  When deleting messages, \\*\\*make sure\\*\\* that the resulting message history is valid. Check the limitations of the LLM provider you're using. For example: \\* Some providers expect message history to start with a `user` message \\* Most providers require `assistant` messages with tool calls to be followed by corresponding `tool` result messages.  ```python theme={null} from langchain.messages import RemoveMessage from langchain.agents import create\\_agent, AgentState from langchain.agents.middleware import after\\_model from langgraph.checkpoint.memory import InMemorySaver from langgraph.runtime import Runtime from langchain\\_core.runnables import RunnableConfig @after\\_model def delete\\_old\\_messages(state: AgentState, runtime: Runtime) -> dict | None: \"\"\"Remove old messages to keep conversation manageable.\"\"\" messages = state[\"messages\"] if len(messages) > 2: # remove the earliest two messages return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]} return None agent = create\\_agent( \"gpt-5-nano\", tools=[], system\\_prompt=\"Please be concise and to the point.\", middleware=[delete\\_old\\_messages], checkpointer=InMemorySaver(), ) config: RunnableConfig = {\"configurable\": {\"thread\\_id\": \"1\"}} for event in agent.stream( {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]}, config, stream\\_mode=\"values\", ): print([(message.type, message.content) for message in event[\"messages\"]]) for event in agent.stream( {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]}, config, stream\\_mode=\"values\", ): print([(message.type, message.content) for message in event[\"messages\"]]) ``` ``` [('human', \"hi! I'm bob\")] [('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.')] [('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.'), ('human', \"what's my name?\")] [('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! Nice to meet you. How can I help you today? I can answer questions, brainstorm ideas, draft text, explain things, or help with code.'), ('human', \"what's my name?\"), ('ai', 'Your name is Bob. How can I help you today, Bob?')] [('human', \"what's my name?\"), ('ai', 'Your name is Bob. How can I help you today, Bob?')] ``` ### Summarize messages The problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model. To summarize message history in an agent, use the built-in [`SummarizationMiddleware`](/oss/python/langchain/middleware#summarization): ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import SummarizationMiddleware from langgraph.checkpoint.memory import InMemorySaver from langchain\\_core.runnables import RunnableConfig checkpointer = InMemorySaver() agent = create\\_agent( model=\"gpt-4o\", tools=[], middleware=[ SummarizationMiddleware( model=\"gpt-4o-mini\", trigger=(\"tokens\", 4000), keep=(\"messages\", 20) ) ], checkpointer=checkpointer, ) config: RunnableConfig = {\"configurable\": {\"thread\\_id\": \"1\"}} agent.invoke({\"messages\": \"hi, my name is bob\"}, config) agent.invoke({\"messages\": \"write a short poem about cats\"}, config) agent.invoke({\"messages\": \"now do the same but for dogs\"}, config) final\\_response = agent.invoke({\"messages\": \"what's my name?\"}, config) final\\_response[\"messages\"][-1].pretty\\_print() \"\"\" ================================== Ai Message ================================== Your name is Bob! \"\"\" ``` See [`SummarizationMiddleware`](/oss/python/langchain/middleware#summarization) for more configuration options. ## Access memory You can access and modify the short-term memory (state) of an agent in several ways: ### Tools #### Read short-term memory in a tool Access short term memory (state) in a tool using the `ToolRuntime` parameter. The `tool\\_runtime` parameter is hidden from the tool signature (so the model doesn't see it), but the tool can access the state through it. ```python theme={null} from langchain.agents import create\\_agent, AgentState from langchain.tools import tool, ToolRuntime class CustomState(AgentState): user\\_id: str @tool def get\\_user\\_info( runtime: ToolRuntime ) -> str: \"\"\"Look up user info.\"\"\" user\\_id = runtime.state[\"user\\_id\"] return \"User is John Smith\" if user\\_id == \"user\\_123\" else \"Unknown user\" agent = create\\_agent( model=\"gpt-5-nano\", tools=[get\\_user\\_info], state\\_schema=CustomState, ) result = agent.invoke({ \"messages\": \"look up user information\", \"user\\_id\": \"user\\_123\" }) print(result[\"messages\"][-1].content) # > User is John Smith. ``` #### Write short-term memory from tools To modify the agent's short-term memory (state) during execution, you can return state updates directly from the tools. This is useful for persisting intermediate results or making information accessible to subsequent tools or prompts. ```python theme={null} from langchain.tools import tool, ToolRuntime from langchain\\_core.runnables import RunnableConfig from langchain.messages import ToolMessage from langchain.agents import create\\_agent, AgentState from langgraph.types import Command from pydantic import BaseModel class CustomState(AgentState): # [!code highlight] user\\_name: str class CustomContext(BaseModel): user\\_id: str @tool def update\\_user\\_info( runtime: ToolRuntime[CustomContext, CustomState], ) -> Command: \"\"\"Look up and update user info.\"\"\" user\\_id = runtime.context.user\\_id name = \"John Smith\" if user\\_id == \"user\\_123\" else \"Unknown user\" return Command(update={ # [!code highlight] \"user\\_name\": name, # update the message history \"messages\": [ ToolMessage( \"Successfully looked up user information\", tool\\_call\\_id=runtime.tool\\_call\\_id ) ] }) @tool def greet( runtime: ToolRuntime[CustomContext, CustomState] ) -> str | Command: \"\"\"Use this to greet the user once you found their info.\"\"\" user\\_name = runtime.state.get(\"user\\_name\", None) if user\\_name is None: return Command(update={ \"messages\": [ ToolMessage( \"Please call the 'update\\_user\\_info' tool it will get and update the user's name.\", tool\\_call\\_id=runtime.tool\\_call\\_id ) ] }) return f\"Hello {user\\_name}!\" agent = create\\_agent( model=\"gpt-5-nano\", tools=[update\\_user\\_info, greet], state\\_schema=CustomState, # [!code highlight] context\\_schema=CustomContext, ) agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"greet the user\"}]}, context=CustomContext(user\\_id=\"user\\_123\"), ) ``` ### Prompt Access short term memory (state) in middleware to create dynamic prompts based on conversation history or custom state fields. ```python theme={null} from langchain.agents import create\\_agent from typing import TypedDict from langchain.agents.middleware import dynamic\\_prompt, ModelRequest class CustomContext(TypedDict): user\\_name: str def get\\_weather(city: str) -> str: \"\"\"Get the weather in a city.\"\"\" return f\"The weather in {city} is always sunny!\" @dynamic\\_prompt def dynamic\\_system\\_prompt(request: ModelRequest) -> str: user\\_name = request.runtime.context[\"user\\_name\"] system\\_prompt = f\"You are a helpful assistant. Address the user as {user\\_name}.\" return system\\_prompt agent = create\\_agent( model=\"gpt-5-nano\", tools=[get\\_weather], middleware=[dynamic\\_system\\_prompt], context\\_schema=CustomContext, ) result = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]}, context=CustomContext(user\\_name=\"John Smith\"), ) for msg in result[\"messages\"]: msg.pretty\\_print() ``` ```shell title=\"Output\" theme={null} ================================ Human Message ================================= What is the weather in SF? ================================== Ai Message ================================== Tool Calls: get\\_weather (call\\_WFQlOGn4b2yoJrv7cih342FG) Call ID: call\\_WFQlOGn4b2yoJrv7cih342FG Args: city: San Francisco ================================= Tool Message ================================= Name: get\\_weather The weather in San Francisco is always sunny! ================================== Ai Message ================================== Hi John Smith, the weather in San Francisco is always sunny! ``` ### Before model Access short term memory (state) in [`@before\\_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.before\\_model) middleware to process messages before model calls. ```mermaid theme={null} %%{ init: { \"fontFamily\": \"monospace\", \"flowchart\": { \"curve\": \"basis\" }, \"themeVariables\": {\"edgeLabelBackground\": \"transparent\"} } }%% graph TD S([\"\\\\_\\\\_start\\\\_\\\\_\"]) PRE(before\\_model) MODEL(model) TOOLS(tools) END([\"\\\\_\\\\_end\\\\_\\\\_\"]) S --> PRE PRE --> MODEL MODEL -.-> TOOLS MODEL -.-> END TOOLS --> PRE classDef blueHighlight fill:#0a1c25,stroke:#0a455f,color:#bae6fd; class S blueHighlight; class END blueHighlight; ``` ```python theme={null} from langchain.messages import RemoveMessage from langgraph.graph.message import REMOVE\\_ALL\\_MESSAGES from langgraph.checkpoint.memory import InMemorySaver from langchain.agents import create\\_agent, AgentState from langchain.agents.middleware import before\\_model from langchain\\_core.runnables import RunnableConfig from langgraph.runtime import Runtime from typing import Any @before\\_model def trim\\_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None: \"\"\"Keep only the last few messages to fit context window.\"\"\" messages = state[\"messages\"] if len(messages) <= 3: return None # No changes needed first\\_msg = messages[0] recent\\_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:] new\\_messages = [first\\_msg] + recent\\_messages return { \"messages\": [ RemoveMessage(id=REMOVE\\_ALL\\_MESSAGES), \\*new\\_messages ] } agent = create\\_agent( \"gpt-5-nano\", tools=[], middleware=[trim\\_messages], checkpointer=InMemorySaver() ) config: RunnableConfig = {\"configurable\": {\"thread\\_id\": \"1\"}} agent.invoke({\"messages\": \"hi, my name is bob\"}, config) agent.invoke({\"messages\": \"write a short poem about cats\"}, config) agent.invoke({\"messages\": \"now do the same but for dogs\"}, config) final\\_response = agent.invoke({\"messages\": \"what's my name?\"}, config) final\\_response[\"messages\"][-1].pretty\\_print() \"\"\" ================================== Ai Message ================================== Your name is Bob. You told me that earlier. If you'd like me to call you a nickname or use a different name, just say the word. \"\"\" ``` ### After model Access short term memory (state) in [`@after\\_model`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.after\\_model) middleware to process messages after model calls. ```mermaid theme={null} %%{ init: { \"fontFamily\": \"monospace\", \"flowchart\": { \"curve\": \"basis\" }, \"themeVariables\": {\"edgeLabelBackground\": \"transparent\"} } }%% graph TD S([\"\\\\_\\\\_start\\\\_\\\\_\"]) MODEL(model) POST(after\\_model) TOOLS(tools) END([\"\\\\_\\\\_end\\\\_\\\\_\"]) S --> MODEL MODEL --> POST POST -.-> END POST -.-> TOOLS TOOLS --> MODEL classDef blueHighlight fill:#0a1c25,stroke:#0a455f,color:#bae6fd; class S blueHighlight; class END blueHighlight; class POST greenHighlight; ``` ```python theme={null} from langchain.messages import RemoveMessage from langgraph.checkpoint.memory import InMemorySaver from langchain.agents import create\\_agent, AgentState from langchain.agents.middleware import after\\_model from langgraph.runtime import Runtime @after\\_model def validate\\_response(state: AgentState, runtime: Runtime) -> dict | None: \"\"\"Remove messages containing sensitive words.\"\"\" STOP\\_WORDS = [\"password\", \"secret\"] last\\_message = state[\"messages\"][-1] if any(word in last\\_message.content for word in STOP\\_WORDS): return {\"messages\": [RemoveMessage(id=last\\_message.id)]} return None agent = create\\_agent( model=\"gpt-5-nano\", tools=[], middleware=[validate\\_response], checkpointer=InMemorySaver(), ) ``` \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/short-term-memory.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/sql-agent.md",
      "title": null,
      "raw_content": "# Build a SQL agent ## Overview In this tutorial, you will learn how to build an agent that can answer questions about a SQL database using LangChain [agents](/oss/python/langchain/agents). At a high level, the agent will:    Building Q\\&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your agent's needs. This will mitigate, though not eliminate, the risks of building a model-driven system.  ### Concepts We will cover the following concepts: \\* [Tools](/oss/python/langchain/tools) for reading from SQL databases \\* LangChain [agents](/oss/python/langchain/agents) \\* [Human-in-the-loop](/oss/python/langchain/human-in-the-loop) processes ## Setup ### Installation  ```bash pip theme={null} pip install langchain langgraph langchain-community ```  ### LangSmith Set up [LangSmith](https://smith.langchain.com) to inspect what is happening inside your chain or agent. Then set the following environment variables: ```shell theme={null} export LANGSMITH\\_TRACING=\"true\" export LANGSMITH\\_API\\_KEY=\"...\" ``` ## 1. Select an LLM Select a model that supports [tool-calling](/oss/python/integrations/providers/overview):   \ud83d\udc49 Read the [OpenAI chat model integration docs](/oss/python/integrations/chat/openai/) ```shell theme={null} pip install -U \"langchain[openai]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"OPENAI\\_API\\_KEY\"] = \"sk-...\" model = init\\_chat\\_model(\"gpt-4.1\") ``` ```python Model Class theme={null} import os from langchain\\_openai import ChatOpenAI os.environ[\"OPENAI\\_API\\_KEY\"] = \"sk-...\" model = ChatOpenAI(model=\"gpt-4.1\") ```    \ud83d\udc49 Read the [Anthropic chat model integration docs](/oss/python/integrations/chat/anthropic/) ```shell theme={null} pip install -U \"langchain[anthropic]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"ANTHROPIC\\_API\\_KEY\"] = \"sk-...\" model = init\\_chat\\_model(\"claude-sonnet-4-5-20250929\") ``` ```python Model Class theme={null} import os from langchain\\_anthropic import ChatAnthropic os.environ[\"ANTHROPIC\\_API\\_KEY\"] = \"sk-...\" model = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\") ```    \ud83d\udc49 Read the [Azure chat model integration docs](/oss/python/integrations/chat/azure\\_chat\\_openai/) ```shell theme={null} pip install -U \"langchain[openai]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"AZURE\\_OPENAI\\_API\\_KEY\"] = \"...\" os.environ[\"AZURE\\_OPENAI\\_ENDPOINT\"] = \"...\" os.environ[\"OPENAI\\_API\\_VERSION\"] = \"2025-03-01-preview\" model = init\\_chat\\_model( \"azure\\_openai:gpt-4.1\", azure\\_deployment=os.environ[\"AZURE\\_OPENAI\\_DEPLOYMENT\\_NAME\"], ) ``` ```python Model Class theme={null} import os from langchain\\_openai import AzureChatOpenAI os.environ[\"AZURE\\_OPENAI\\_API\\_KEY\"] = \"...\" os.environ[\"AZURE\\_OPENAI\\_ENDPOINT\"] = \"...\" os.environ[\"OPENAI\\_API\\_VERSION\"] = \"2025-03-01-preview\" model = AzureChatOpenAI( model=\"gpt-4.1\", azure\\_deployment=os.environ[\"AZURE\\_OPENAI\\_DEPLOYMENT\\_NAME\"] ) ```    \ud83d\udc49 Read the [Google GenAI chat model integration docs](/oss/python/integrations/chat/google\\_generative\\_ai/) ```shell theme={null} pip install -U \"langchain[google-genai]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"GOOGLE\\_API\\_KEY\"] = \"...\" model = init\\_chat\\_model(\"google\\_genai:gemini-2.5-flash-lite\") ``` ```python Model Class theme={null} import os from langchain\\_google\\_genai import ChatGoogleGenerativeAI os.environ[\"GOOGLE\\_API\\_KEY\"] = \"...\" model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\") ```    \ud83d\udc49 Read the [AWS Bedrock chat model integration docs](/oss/python/integrations/chat/bedrock/) ```shell theme={null} pip install -U \"langchain[aws]\" ```  ```python init\\_chat\\_model theme={null} from langchain.chat\\_models import init\\_chat\\_model # Follow the steps here to configure your credentials: # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html model = init\\_chat\\_model( \"anthropic.claude-3-5-sonnet-20240620-v1:0\", model\\_provider=\"bedrock\\_converse\", ) ``` ```python Model Class theme={null} from langchain\\_aws import ChatBedrock model = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\") ```   \ud83d\udc49 Read the [HuggingFace chat model integration docs](/oss/python/integrations/chat/huggingface/) ```shell theme={null} pip install -U \"langchain[huggingface]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"HUGGINGFACEHUB\\_API\\_TOKEN\"] = \"hf\\_...\" model = init\\_chat\\_model( \"microsoft/Phi-3-mini-4k-instruct\", model\\_provider=\"huggingface\", temperature=0.7, max\\_tokens=1024, ) ``` ```python Model Class theme={null} import os from langchain\\_huggingface import ChatHuggingFace, HuggingFaceEndpoint os.environ[\"HUGGINGFACEHUB\\_API\\_TOKEN\"] = \"hf\\_...\" llm = HuggingFaceEndpoint( repo\\_id=\"microsoft/Phi-3-mini-4k-instruct\", temperature=0.7, max\\_length=1024, ) model = ChatHuggingFace(llm=llm) ```     The output shown in the examples below used OpenAI. ## 2. Configure the database You will be creating a [SQLite database](https://www.sqlitetutorial.net/sqlite-sample-database/) for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will be loading the `chinook` database, which is a sample database that represents a digital media store. For convenience, we have hosted the database (`Chinook.db`) on a public GCS bucket. ```python theme={null} import requests, pathlib url = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\" local\\_path = pathlib.Path(\"Chinook.db\") if local\\_path.exists(): print(f\"{local\\_path} already exists, skipping download.\") else: response = requests.get(url) if response.status\\_code == 200: local\\_path.write\\_bytes(response.content) print(f\"File downloaded and saved as {local\\_path}\") else: print(f\"Failed to download the file. Status code: {response.status\\_code}\") ``` We will use a handy SQL database wrapper available in the `langchain\\_community` package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results: ```python theme={null} from langchain\\_community.utilities import SQLDatabase db = SQLDatabase.from\\_uri(\"sqlite:///Chinook.db\") print(f\"Dialect: {db.dialect}\") print(f\"Available tables: {db.get\\_usable\\_table\\_names()}\") print(f'Sample output: {db.run(\"SELECT \\* FROM Artist LIMIT 5;\")}') ``` ``` Dialect: sqlite Available tables: ['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track'] Sample output: [(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains')] ``` ## 3. Add tools for database interactions Use the `SQLDatabase` wrapper available in the `langchain\\_community` package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results: ```python theme={null} from langchain\\_community.agent\\_toolkits import SQLDatabaseToolkit toolkit = SQLDatabaseToolkit(db=db, llm=model) tools = toolkit.get\\_tools() for tool in tools: print(f\"{tool.name}: {tool.description}\\n\") ``` ``` sql\\_db\\_query: Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column 'xxxx' in 'field list', use sql\\_db\\_schema to query the correct table fields. sql\\_db\\_schema: Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql\\_db\\_list\\_tables first! Example Input: table1, table2, table3 sql\\_db\\_list\\_tables: Input is an empty string, output is a comma-separated list of tables in the database. sql\\_db\\_query\\_checker: Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql\\_db\\_query! ``` ## 4. Use `create\\_agent` Use [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent) to build a [ReAct agent](https://arxiv.org/pdf/2210.03629) with minimal code. The agent will interpret the request and generate a SQL command, which the tools will execute. If the command has an error, the error message is returned to the model. The model can then examine the original request and the new error message and generate a new command. This can continue until the LLM generates the command successfully or reaches an end count. This pattern of providing a model with feedback - error messages in this case - is very powerful. Initialize the agent with a descriptive system prompt to customize its behavior: ```python theme={null} system\\_prompt = \"\"\" You are an agent designed to interact with a SQL database. Given an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top\\_k} results. You can order the results by a relevant column to return the most interesting examples in the database. Never query for all the columns from a specific table, only ask for the relevant columns given the question. You MUST double check your query before executing it. If you get an error while executing a query, rewrite the query and try again. DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database. To start you should ALWAYS look at the tables in the database to see what you can query. Do NOT skip this step. Then you should query the schema of the most relevant tables. \"\"\".format( dialect=db.dialect, top\\_k=5, ) ``` Now, create an agent with the model, tools, and prompt: ```python theme={null} from langchain.agents import create\\_agent agent = create\\_agent( model, tools, system\\_prompt=system\\_prompt, ) ``` ## 5. Run the agent Run the agent on a sample query and observe its behavior: ```python theme={null} question = \"Which genre on average has the longest tracks?\" for step in agent.stream( {\"messages\": [{\"role\": \"user\", \"content\": question}]}, stream\\_mode=\"values\", ): step[\"messages\"][-1].pretty\\_print() ``` ``` ================================ Human Message ================================= Which genre on average has the longest tracks? ================================== Ai Message ================================== Tool Calls: sql\\_db\\_list\\_tables (call\\_BQsWg8P65apHc8BTJ1NPDvnM) Call ID: call\\_BQsWg8P65apHc8BTJ1NPDvnM Args: ================================= Tool Message ================================= Name: sql\\_db\\_list\\_tables Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track ================================== Ai Message ================================== Tool Calls: sql\\_db\\_schema (call\\_i89tjKECFSeERbuACYm4w0cU) Call ID: call\\_i89tjKECFSeERbuACYm4w0cU Args: table\\_names: Track, Genre ================================= Tool Message ================================= Name: sql\\_db\\_schema CREATE TABLE \"Genre\" ( \"GenreId\" INTEGER NOT NULL, \"Name\" NVARCHAR(120), PRIMARY KEY (\"GenreId\") ) /\\* 3 rows from Genre table: GenreId Name 1 Rock 2 Jazz 3 Metal \\*/ CREATE TABLE \"Track\" ( \"TrackId\" INTEGER NOT NULL, \"Name\" NVARCHAR(200) NOT NULL, \"AlbumId\" INTEGER, \"MediaTypeId\" INTEGER NOT NULL, \"GenreId\" INTEGER, \"Composer\" NVARCHAR(220), \"Milliseconds\" INTEGER NOT NULL, \"Bytes\" INTEGER, \"UnitPrice\" NUMERIC(10, 2) NOT NULL, PRIMARY KEY (\"TrackId\"), FOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"), FOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"), FOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\") ) /\\* 3 rows from Track table: TrackId Name AlbumId MediaTypeId GenreId Composer Milliseconds Bytes UnitPrice 1 For Those About To Rock (We Salute You) 1 1 1 Angus Young, Malcolm Young, Brian Johnson 343719 11170334 0.99 2 Balls to the Wall 2 2 1 U. Dirkschneider, W. Hoffmann, H. Frank, P. Baltes, S. Kaufmann, G. Hoffmann 342562 5510424 0.99 3 Fast As a Shark 3 2 1 F. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman 230619 3990994 0.99 \\*/ ================================== Ai Message ================================== Tool Calls: sql\\_db\\_query\\_checker (call\\_G64yYm6R6UauiVPCXJZMA49b) Call ID: call\\_G64yYm6R6UauiVPCXJZMA49b Args: query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5; ================================= Tool Message ================================= Name: sql\\_db\\_query\\_checker SELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5; ================================== Ai Message ================================== Tool Calls: sql\\_db\\_query (call\\_AnO3SrhD0ODJBxh6dHMwvHwZ) Call ID: call\\_AnO3SrhD0ODJBxh6dHMwvHwZ Args: query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AverageLength FROM Track INNER JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AverageLength DESC LIMIT 5; ================================= Tool Message ================================= Name: sql\\_db\\_query [('Sci Fi & Fantasy', 2911783.0384615385), ('Science Fiction', 2625549.076923077), ('Drama', 2575283.78125), ('TV Shows', 2145041.0215053763), ('Comedy', 1585263.705882353)] ================================== Ai Message ================================== On average, the genre with the longest tracks is \"Sci Fi & Fantasy\" with an average track length of approximately 2,911,783 milliseconds. This is followed by \"Science Fiction,\" \"Drama,\" \"TV Shows,\" and \"Comedy.\" ``` The agent correctly wrote a query, checked the query, and ran it to inform its final response.  You can inspect all aspects of the above run, including steps taken, tools invoked, what prompts were seen by the LLM, and more in the [LangSmith trace](https://smith.langchain.com/public/cd2ce887-388a-4bb1-a29d-48208ce50d15/r).  ### (Optional) Use Studio [Studio](/langsmith/studio) provides a \"client side\" loop as well as memory so you can run this as a chat interface and query the database. You can ask questions like \"Tell me the scheme of the database\" or \"Show me the invoices for the 5 top customers\". You will see the SQL command that is generated and the resulting output. The details of how to get that started are below.  In addition to the previously mentioned packages, you will need to: ```shell theme={null} pip install -U langgraph-cli[inmem]>=0.4.0 ``` In directory you will run in, you will need a `langgraph.json` file with the following contents: ```json theme={null} { \"dependencies\": [\".\"], \"graphs\": { \"agent\": \"./sql\\_agent.py:agent\", \"graph\": \"./sql\\_agent\\_langgraph.py:graph\" }, \"env\": \".env\" } ``` Create a file `sql\\_agent.py` and insert this: ```python theme={null} #sql\\_agent.py for studio import pathlib from langchain.agents import create\\_agent from langchain.chat\\_models import init\\_chat\\_model from langchain\\_community.agent\\_toolkits import SQLDatabaseToolkit from langchain\\_community.utilities import SQLDatabase import requests # Initialize an LLM model = init\\_chat\\_model(\"gpt-4.1\") # Get the database, store it locally url = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\" local\\_path = pathlib.Path(\"Chinook.db\") if local\\_path.exists(): print(f\"{local\\_path} already exists, skipping download.\") else: response = requests.get(url) if response.status\\_code == 200: local\\_path.write\\_bytes(response.content) print(f\"File downloaded and saved as {local\\_path}\") else: print(f\"Failed to download the file. Status code: {response.status\\_code}\") db = SQLDatabase.from\\_uri(\"sqlite:///Chinook.db\") # Create the tools toolkit = SQLDatabaseToolkit(db=db, llm=model) tools = toolkit.get\\_tools() for tool in tools: print(f\"{tool.name}: {tool.description}\\n\") # Use create\\_agent system\\_prompt = \"\"\" You are an agent designed to interact with a SQL database. Given an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top\\_k} results. You can order the results by a relevant column to return the most interesting examples in the database. Never query for all the columns from a specific table, only ask for the relevant columns given the question. You MUST double check your query before executing it. If you get an error while executing a query, rewrite the query and try again. DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database. To start you should ALWAYS look at the tables in the database to see what you can query. Do NOT skip this step. Then you should query the schema of the most relevant tables. \"\"\".format( dialect=db.dialect, top\\_k=5, ) agent = create\\_agent( model, tools, system\\_prompt=system\\_prompt, ) ```  ## 6. Implement human-in-the-loop review It can be prudent to check the agent's SQL queries before they are executed for any unintended actions or inefficiencies. LangChain agents feature support for built-in [human-in-the-loop middleware](/oss/python/langchain/human-in-the-loop) to add oversight to agent tool calls. Let's configure the agent to pause for human review on calling the `sql\\_db\\_query` tool: ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import HumanInTheLoopMiddleware # [!code highlight] from langgraph.checkpoint.memory import InMemorySaver # [!code highlight] agent = create\\_agent( model, tools, system\\_prompt=system\\_prompt, middleware=[ # [!code highlight] HumanInTheLoopMiddleware( # [!code highlight] interrupt\\_on={\"sql\\_db\\_query\": True}, # [!code highlight] description\\_prefix=\"Tool execution pending approval\", # [!code highlight] ), # [!code highlight] ], # [!code highlight] checkpointer=InMemorySaver(), # [!code highlight] ) ```  We've added a [checkpointer](/oss/python/langchain/short-term-memory) to our agent to allow execution to be paused and resumed. See the [human-in-the-loop guide](/oss/python/langchain/human-in-the-loop) for detalis on this as well as available middleware configurations.  On running the agent, it will now pause for review before executing the `sql\\_db\\_query` tool: ```python theme={null} question = \"Which genre on average has the longest tracks?\" config = {\"configurable\": {\"thread\\_id\": \"1\"}} # [!code highlight] for step in agent.stream( {\"messages\": [{\"role\": \"user\", \"content\": question}]}, config, # [!code highlight] stream\\_mode=\"values\", ): if \"messages\" in step: step[\"messages\"][-1].pretty\\_print() elif \"\\_\\_interrupt\\_\\_\" in step: # [!code highlight] print(\"INTERRUPTED:\") # [!code highlight] interrupt = step[\"\\_\\_interrupt\\_\\_\"][0] # [!code highlight] for request in interrupt.value[\"action\\_requests\"]: # [!code highlight] print(request[\"description\"]) # [!code highlight] else: pass ``` ``` ... INTERRUPTED: Tool execution pending approval Tool: sql\\_db\\_query Args: {'query': 'SELECT g.Name AS Genre, AVG(t.Milliseconds) AS AvgTrackLength FROM Track t JOIN Genre g ON t.GenreId = g.GenreId GROUP BY g.Name ORDER BY AvgTrackLength DESC LIMIT 1;'} ``` We can resume execution, in this case accepting the query, using [Command](/oss/python/langgraph/use-graph-api#combine-control-flow-and-state-updates-with-command): ```python theme={null} from langgraph.types import Command # [!code highlight] for step in agent.stream( Command(resume={\"decisions\": [{\"type\": \"approve\"}]}), # [!code highlight] config, stream\\_mode=\"values\", ): if \"messages\" in step: step[\"messages\"][-1].pretty\\_print() elif \"\\_\\_interrupt\\_\\_\" in step: print(\"INTERRUPTED:\") interrupt = step[\"\\_\\_interrupt\\_\\_\"][0] for request in interrupt.value[\"action\\_requests\"]: print(request[\"description\"]) else: pass ``` ``` ================================== Ai Message ================================== Tool Calls: sql\\_db\\_query (call\\_7oz86Epg7lYRqi9rQHbZPS1U) Call ID: call\\_7oz86Epg7lYRqi9rQHbZPS1U Args: query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgDuration FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AvgDuration DESC LIMIT 5; ================================= Tool Message ================================= Name: sql\\_db\\_query [('Sci Fi & Fantasy', 2911783.0384615385), ('Science Fiction', 2625549.076923077), ('Drama', 2575283.78125), ('TV Shows', 2145041.0215053763), ('Comedy', 1585263.705882353)] ================================== Ai Message ================================== The genre with the longest average track length is \"Sci Fi & Fantasy\" with an average duration of about 2,911,783 milliseconds, followed by \"Science Fiction\" and \"Drama.\" ``` Refer to the [human-in-the-loop guide](/oss/python/langchain/human-in-the-loop) for details. ## Next steps For deeper customization, check out [this tutorial](/oss/python/langgraph/sql-agent) for implementing a SQL agent directly using LangGraph primitives. \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/sql-agent.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/streaming.md",
      "title": null,
      "raw_content": "# Streaming LangChain implements a streaming system to surface real-time updates. Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs. ## Overview LangChain's streaming system lets you surface live feedback from agent runs to your application. What's possible with LangChain streaming: \\* [\\*\\*Stream agent progress\\*\\*](#agent-progress) \u2014 get state updates after each agent step. \\* [\\*\\*Stream LLM tokens\\*\\*](#llm-tokens) \u2014 stream language model tokens as they're generated. \\* [\\*\\*Stream custom updates\\*\\*](#custom-updates) \u2014 emit user-defined signals (e.g., `\"Fetched 10/100 records\"`). \\* [\\*\\*Stream multiple modes\\*\\*](#stream-multiple-modes) \u2014 choose from `updates` (agent progress), `messages` (LLM tokens + metadata), or `custom` (arbitrary user data). ## Agent progress To stream agent progress, use the [`stream`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.stream) or [`astream`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.astream) methods with `stream\\_mode=\"updates\"`. This emits an event after every agent step. For example, if you have an agent that calls a tool once, you should see the following updates: \\* \\*\\*LLM node\\*\\*: [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) with tool call requests \\* \\*\\*Tool node\\*\\*: [`ToolMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage) with execution result \\* \\*\\*LLM node\\*\\*: Final AI response ```python title=\"Streaming agent progress\" theme={null} from langchain.agents import create\\_agent def get\\_weather(city: str) -> str: \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create\\_agent( model=\"gpt-5-nano\", tools=[get\\_weather], ) for chunk in agent.stream( # [!code highlight] {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]}, stream\\_mode=\"updates\", ): for step, data in chunk.items(): print(f\"step: {step}\") print(f\"content: {data['messages'][-1].content\\_blocks}\") ``` ```shell title=\"Output\" theme={null} step: model content: [{'type': 'tool\\_call', 'name': 'get\\_weather', 'args': {'city': 'San Francisco'}, 'id': 'call\\_OW2NYNsNSKhRZpjW0wm2Aszd'}] step: tools content: [{'type': 'text', 'text': \"It's always sunny in San Francisco!\"}] step: model content: [{'type': 'text', 'text': 'It's always sunny in San Francisco!'}] ``` ## LLM tokens To stream tokens as they are produced by the LLM, use `stream\\_mode=\"messages\"`. Below you can see the output of the agent streaming tool calls and the final response. ```python title=\"Streaming LLM tokens\" theme={null} from langchain.agents import create\\_agent def get\\_weather(city: str) -> str: \"\"\"Get weather for a given city.\"\"\" return f\"It's always sunny in {city}!\" agent = create\\_agent( model=\"gpt-5-nano\", tools=[get\\_weather], ) for token, metadata in agent.stream( # [!code highlight] {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]}, stream\\_mode=\"messages\", ): print(f\"node: {metadata['langgraph\\_node']}\") print(f\"content: {token.content\\_blocks}\") print(\"\\n\") ``` ```shell title=\"Output\" expandable theme={null} node: model content: [{'type': 'tool\\_call\\_chunk', 'id': 'call\\_vbCyBcP8VuneUzyYlSBZZsVa', 'name': 'get\\_weather', 'args': '', 'index': 0}] node: model content: [{'type': 'tool\\_call\\_chunk', 'id': None, 'name': None, 'args': '{\"', 'index': 0}] node: model content: [{'type': 'tool\\_call\\_chunk', 'id': None, 'name': None, 'args': 'city', 'index': 0}] node: model content: [{'type': 'tool\\_call\\_chunk', 'id': None, 'name': None, 'args': '\":\"', 'index': 0}] node: model content: [{'type': 'tool\\_call\\_chunk', 'id': None, 'name': None, 'args': 'San', 'index': 0}] node: model content: [{'type': 'tool\\_call\\_chunk', 'id': None, 'name': None, 'args': ' Francisco', 'index': 0}] node: model content: [{'type': 'tool\\_call\\_chunk', 'id': None, 'name': None, 'args': '\"}', 'index': 0}] node: model content: [] node: tools content: [{'type': 'text', 'text': \"It's always sunny in San Francisco!\"}] node: model content: [] node: model content: [{'type': 'text', 'text': 'Here'}] node: model content: [{'type': 'text', 'text': ''s'}] node: model content: [{'type': 'text', 'text': ' what'}] node: model content: [{'type': 'text', 'text': ' I'}] node: model content: [{'type': 'text', 'text': ' got'}] node: model content: [{'type': 'text', 'text': ':'}] node: model content: [{'type': 'text', 'text': ' \"'}] node: model content: [{'type': 'text', 'text': \"It's\"}] node: model content: [{'type': 'text', 'text': ' always'}] node: model content: [{'type': 'text', 'text': ' sunny'}] node: model content: [{'type': 'text', 'text': ' in'}] node: model content: [{'type': 'text', 'text': ' San'}] node: model content: [{'type': 'text', 'text': ' Francisco'}] node: model content: [{'type': 'text', 'text': '!\"\\n\\n'}] ``` ## Custom updates To stream updates from tools as they are executed, you can use [`get\\_stream\\_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get\\_stream\\_writer). ```python title=\"Streaming custom updates\" theme={null} from langchain.agents import create\\_agent from langgraph.config import get\\_stream\\_writer # [!code highlight] def get\\_weather(city: str) -> str: \"\"\"Get weather for a given city.\"\"\" writer = get\\_stream\\_writer() # [!code highlight] # stream any arbitrary data writer(f\"Looking up data for city: {city}\") writer(f\"Acquired data for city: {city}\") return f\"It's always sunny in {city}!\" agent = create\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=[get\\_weather], ) for chunk in agent.stream( {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]}, stream\\_mode=\"custom\" # [!code highlight] ): print(chunk) ``` ```shell title=\"Output\" theme={null} Looking up data for city: San Francisco Acquired data for city: San Francisco ```  If you add [`get\\_stream\\_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get\\_stream\\_writer) inside your tool, you won't be able to invoke the tool outside of a LangGraph execution context.  ## Stream multiple modes You can specify multiple streaming modes by passing stream mode as a list: `stream\\_mode=[\"updates\", \"custom\"]`: ```python title=\"Streaming multiple modes\" theme={null} from langchain.agents import create\\_agent from langgraph.config import get\\_stream\\_writer def get\\_weather(city: str) -> str: \"\"\"Get weather for a given city.\"\"\" writer = get\\_stream\\_writer() writer(f\"Looking up data for city: {city}\") writer(f\"Acquired data for city: {city}\") return f\"It's always sunny in {city}!\" agent = create\\_agent( model=\"gpt-5-nano\", tools=[get\\_weather], ) for stream\\_mode, chunk in agent.stream( # [!code highlight] {\"messages\": [{\"role\": \"user\", \"content\": \"What is the weather in SF?\"}]}, stream\\_mode=[\"updates\", \"custom\"] ): print(f\"stream\\_mode: {stream\\_mode}\") print(f\"content: {chunk}\") print(\"\\n\") ``` ```shell title=\"Output\" theme={null} stream\\_mode: updates content: {'model': {'messages': [AIMessage(content='', response\\_metadata={'token\\_usage': {'completion\\_tokens': 280, 'prompt\\_tokens': 132, 'total\\_tokens': 412, 'completion\\_tokens\\_details': {'accepted\\_prediction\\_tokens': 0, 'audio\\_tokens': 0, 'reasoning\\_tokens': 256, 'rejected\\_prediction\\_tokens': 0}, 'prompt\\_tokens\\_details': {'audio\\_tokens': 0, 'cached\\_tokens': 0}}, 'model\\_provider': 'openai', 'model\\_name': 'gpt-5-nano-2025-08-07', 'system\\_fingerprint': None, 'id': 'chatcmpl-C9tlgBzGEbedGYxZ0rTCz5F7OXpL7', 'service\\_tier': 'default', 'finish\\_reason': 'tool\\_calls', 'logprobs': None}, id='lc\\_run--480c07cb-e405-4411-aa7f-0520fddeed66-0', tool\\_calls=[{'name': 'get\\_weather', 'args': {'city': 'San Francisco'}, 'id': 'call\\_KTNQIftMrl9vgNwEfAJMVu7r', 'type': 'tool\\_call'}], usage\\_metadata={'input\\_tokens': 132, 'output\\_tokens': 280, 'total\\_tokens': 412, 'input\\_token\\_details': {'audio': 0, 'cache\\_read': 0}, 'output\\_token\\_details': {'audio': 0, 'reasoning': 256}})]}} stream\\_mode: custom content: Looking up data for city: San Francisco stream\\_mode: custom content: Acquired data for city: San Francisco stream\\_mode: updates content: {'tools': {'messages': [ToolMessage(content=\"It's always sunny in San Francisco!\", name='get\\_weather', tool\\_call\\_id='call\\_KTNQIftMrl9vgNwEfAJMVu7r')]}} stream\\_mode: updates content: {'model': {'messages': [AIMessage(content='San Francisco weather: It's always sunny in San Francisco!\\n\\n', response\\_metadata={'token\\_usage': {'completion\\_tokens': 764, 'prompt\\_tokens': 168, 'total\\_tokens': 932, 'completion\\_tokens\\_details': {'accepted\\_prediction\\_tokens': 0, 'audio\\_tokens': 0, 'reasoning\\_tokens': 704, 'rejected\\_prediction\\_tokens': 0}, 'prompt\\_tokens\\_details': {'audio\\_tokens': 0, 'cached\\_tokens': 0}}, 'model\\_provider': 'openai', 'model\\_name': 'gpt-5-nano-2025-08-07', 'system\\_fingerprint': None, 'id': 'chatcmpl-C9tljDFVki1e1haCyikBptAuXuHYG', 'service\\_tier': 'default', 'finish\\_reason': 'stop', 'logprobs': None}, id='lc\\_run--acbc740a-18fe-4a14-8619-da92a0d0ee90-0', usage\\_metadata={'input\\_tokens': 168, 'output\\_tokens': 764, 'total\\_tokens': 932, 'input\\_token\\_details': {'audio': 0, 'cache\\_read': 0}, 'output\\_token\\_details': {'audio': 0, 'reasoning': 704}})]}} ``` ## Disable streaming In some applications you might need to disable streaming of individual tokens for a given model. This is useful in [multi-agent](/oss/python/langchain/multi-agent) systems to control which agents stream their output. See the [Models](/oss/python/langchain/models#disable-streaming) guide to learn how to disable streaming. \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/streaming.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/structured-output.md",
      "title": null,
      "raw_content": "# Structured output Structured output allows agents to return data in a specific, predictable format. Instead of parsing natural language responses, you get structured data in the form of JSON objects, Pydantic models, or dataclasses that your application can directly use. LangChain's [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent) handles structured output automatically. The user sets their desired structured output schema, and when the model generates the structured data, it's captured, validated, and returned in the `'structured\\_response'` key of the agent's state. ```python theme={null} def create\\_agent( ... response\\_format: Union[ ToolStrategy[StructuredResponseT], ProviderStrategy[StructuredResponseT], type[StructuredResponseT], ] ``` ## Response Format Controls how the agent returns structured data: \\* \\*\\*`ToolStrategy[StructuredResponseT]`\\*\\*: Uses tool calling for structured output \\* \\*\\*`ProviderStrategy[StructuredResponseT]`\\*\\*: Uses provider-native structured output \\* \\*\\*`type[StructuredResponseT]`\\*\\*: Schema type - automatically selects best strategy based on model capabilities \\* \\*\\*`None`\\*\\*: No structured output When a schema type is provided directly, LangChain automatically chooses: \\* `ProviderStrategy` for models supporting native structured output (e.g. [OpenAI](/oss/python/integrations/providers/openai), [Anthropic](/oss/python/integrations/providers/anthropic), or [Grok](/oss/python/integrations/providers/xai)). \\* `ToolStrategy` for all other models.  Support for native structured output features is read dynamically from the model's [profile data](/oss/python/langchain/models#model-profiles) if using `langchain>=1.1`. If data are not available, use another condition or specify manually: ```python theme={null} custom\\_profile = { \"structured\\_output\": True, # ... } model = init\\_chat\\_model(\"...\", profile=custom\\_profile) ``` If tools are specified, the model must support simultaneous use of tools and structured output.  The structured response is returned in the `structured\\_response` key of the agent's final state. ## Provider strategy Some model providers support structured output natively through their APIs (e.g. OpenAI, Grok, Gemini). This is the most reliable method when available. To use this strategy, configure a `ProviderStrategy`: ```python theme={null} class ProviderStrategy(Generic[SchemaT]): schema: type[SchemaT] ```  The schema defining the structured output format. Supports: \\* \\*\\*Pydantic models\\*\\*: `BaseModel` subclasses with field validation \\* \\*\\*Dataclasses\\*\\*: Python dataclasses with type annotations \\* \\*\\*TypedDict\\*\\*: Typed dictionary classes \\* \\*\\*JSON Schema\\*\\*: Dictionary with JSON schema specification  LangChain automatically uses `ProviderStrategy` when you pass a schema type directly to [`create\\_agent.response\\_format`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent\\(response\\_format\\)) and the model supports native structured output:  ```python Pydantic Model theme={null} from pydantic import BaseModel, Field from langchain.agents import create\\_agent class ContactInfo(BaseModel): \"\"\"Contact information for a person.\"\"\" name: str = Field(description=\"The name of the person\") email: str = Field(description=\"The email address of the person\") phone: str = Field(description=\"The phone number of the person\") agent = create\\_agent( model=\"gpt-5\", response\\_format=ContactInfo # Auto-selects ProviderStrategy ) result = agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}] }) print(result[\"structured\\_response\"]) # ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567') ``` ```python Dataclass theme={null} from dataclasses import dataclass from langchain.agents import create\\_agent @dataclass class ContactInfo: \"\"\"Contact information for a person.\"\"\" name: str # The name of the person email: str # The email address of the person phone: str # The phone number of the person agent = create\\_agent( model=\"gpt-5\", tools=tools, response\\_format=ContactInfo # Auto-selects ProviderStrategy ) result = agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}] }) result[\"structured\\_response\"] # ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567') ``` ```python TypedDict theme={null} from typing\\_extensions import TypedDict from langchain.agents import create\\_agent class ContactInfo(TypedDict): \"\"\"Contact information for a person.\"\"\" name: str # The name of the person email: str # The email address of the person phone: str # The phone number of the person agent = create\\_agent( model=\"gpt-5\", tools=tools, response\\_format=ContactInfo # Auto-selects ProviderStrategy ) result = agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}] }) result[\"structured\\_response\"] # {'name': 'John Doe', 'email': 'john@example.com', 'phone': '(555) 123-4567'} ``` ```python JSON Schema theme={null} from langchain.agents import create\\_agent contact\\_info\\_schema = { \"type\": \"object\", \"description\": \"Contact information for a person.\", \"properties\": { \"name\": {\"type\": \"string\", \"description\": \"The name of the person\"}, \"email\": {\"type\": \"string\", \"description\": \"The email address of the person\"}, \"phone\": {\"type\": \"string\", \"description\": \"The phone number of the person\"} }, \"required\": [\"name\", \"email\", \"phone\"] } agent = create\\_agent( model=\"gpt-5\", tools=tools, response\\_format=ProviderStrategy(contact\\_info\\_schema) ) result = agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\"}] }) result[\"structured\\_response\"] # {'name': 'John Doe', 'email': 'john@example.com', 'phone': '(555) 123-4567'} ```  Provider-native structured output provides high reliability and strict validation because the model provider enforces the schema. Use it when available.  If the provider natively supports structured output for your model choice, it is functionally equivalent to write `response\\_format=ProductReview` instead of `response\\_format=ProviderStrategy(ProductReview)`. In either case, if structured output is not supported, the agent will fall back to a tool calling strategy.  ## Tool calling strategy For models that don't support native structured output, LangChain uses tool calling to achieve the same result. This works with all models that support tool calling, which is most modern models. To use this strategy, configure a `ToolStrategy`: ```python theme={null} class ToolStrategy(Generic[SchemaT]): schema: type[SchemaT] tool\\_message\\_content: str | None handle\\_errors: Union[ bool, str, type[Exception], tuple[type[Exception], ...], Callable[[Exception], str], ] ```  The schema defining the structured output format. Supports: \\* \\*\\*Pydantic models\\*\\*: `BaseModel` subclasses with field validation \\* \\*\\*Dataclasses\\*\\*: Python dataclasses with type annotations \\* \\*\\*TypedDict\\*\\*: Typed dictionary classes \\* \\*\\*JSON Schema\\*\\*: Dictionary with JSON schema specification \\* \\*\\*Union types\\*\\*: Multiple schema options. The model will choose the most appropriate schema based on the context.   Custom content for the tool message returned when structured output is generated. If not provided, defaults to a message showing the structured response data.   Error handling strategy for structured output validation failures. Defaults to `True`. \\* \\*\\*`True`\\*\\*: Catch all errors with default error template \\* \\*\\*`str`\\*\\*: Catch all errors with this custom message \\* \\*\\*`type[Exception]`\\*\\*: Only catch this exception type with default message \\* \\*\\*`tuple[type[Exception], ...]`\\*\\*: Only catch these exception types with default message \\* \\*\\*`Callable[[Exception], str]`\\*\\*: Custom function that returns error message \\* \\*\\*`False`\\*\\*: No retry, let exceptions propagate   ```python Pydantic Model theme={null} from pydantic import BaseModel, Field from typing import Literal from langchain.agents import create\\_agent from langchain.agents.structured\\_output import ToolStrategy class ProductReview(BaseModel): \"\"\"Analysis of a product review.\"\"\" rating: int | None = Field(description=\"The rating of the product\", ge=1, le=5) sentiment: Literal[\"positive\", \"negative\"] = Field(description=\"The sentiment of the review\") key\\_points: list[str] = Field(description=\"The key points of the review. Lowercase, 1-3 words each.\") agent = create\\_agent( model=\"gpt-5\", tools=tools, response\\_format=ToolStrategy(ProductReview) ) result = agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\"}] }) result[\"structured\\_response\"] # ProductReview(rating=5, sentiment='positive', key\\_points=['fast shipping', 'expensive']) ``` ```python Dataclass theme={null} from dataclasses import dataclass from typing import Literal from langchain.agents import create\\_agent from langchain.agents.structured\\_output import ToolStrategy @dataclass class ProductReview: \"\"\"Analysis of a product review.\"\"\" rating: int | None # The rating of the product (1-5) sentiment: Literal[\"positive\", \"negative\"] # The sentiment of the review key\\_points: list[str] # The key points of the review agent = create\\_agent( model=\"gpt-5\", tools=tools, response\\_format=ToolStrategy(ProductReview) ) result = agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\"}] }) result[\"structured\\_response\"] # ProductReview(rating=5, sentiment='positive', key\\_points=['fast shipping', 'expensive']) ``` ```python TypedDict theme={null} from typing import Literal from typing\\_extensions import TypedDict from langchain.agents import create\\_agent from langchain.agents.structured\\_output import ToolStrategy class ProductReview(TypedDict): \"\"\"Analysis of a product review.\"\"\" rating: int | None # The rating of the product (1-5) sentiment: Literal[\"positive\", \"negative\"] # The sentiment of the review key\\_points: list[str] # The key points of the review agent = create\\_agent( model=\"gpt-5\", tools=tools, response\\_format=ToolStrategy(ProductReview) ) result = agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\"}] }) result[\"structured\\_response\"] # {'rating': 5, 'sentiment': 'positive', 'key\\_points': ['fast shipping', 'expensive']} ``` ```python JSON Schema theme={null} from langchain.agents import create\\_agent from langchain.agents.structured\\_output import ToolStrategy product\\_review\\_schema = { \"type\": \"object\", \"description\": \"Analysis of a product review.\", \"properties\": { \"rating\": { \"type\": [\"integer\", \"null\"], \"description\": \"The rating of the product (1-5)\", \"minimum\": 1, \"maximum\": 5 }, \"sentiment\": { \"type\": \"string\", \"enum\": [\"positive\", \"negative\"], \"description\": \"The sentiment of the review\" }, \"key\\_points\": { \"type\": \"array\", \"items\": {\"type\": \"string\"}, \"description\": \"The key points of the review\" } }, \"required\": [\"sentiment\", \"key\\_points\"] } agent = create\\_agent( model=\"gpt-5\", tools=tools, response\\_format=ToolStrategy(product\\_review\\_schema) ) result = agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\"}] }) result[\"structured\\_response\"] # {'rating': 5, 'sentiment': 'positive', 'key\\_points': ['fast shipping', 'expensive']} ``` ```python Union Types theme={null} from pydantic import BaseModel, Field from typing import Literal, Union from langchain.agents import create\\_agent from langchain.agents.structured\\_output import ToolStrategy class ProductReview(BaseModel): \"\"\"Analysis of a product review.\"\"\" rating: int | None = Field(description=\"The rating of the product\", ge=1, le=5) sentiment: Literal[\"positive\", \"negative\"] = Field(description=\"The sentiment of the review\") key\\_points: list[str] = Field(description=\"The key points of the review. Lowercase, 1-3 words each.\") class CustomerComplaint(BaseModel): \"\"\"A customer complaint about a product or service.\"\"\" issue\\_type: Literal[\"product\", \"service\", \"shipping\", \"billing\"] = Field(description=\"The type of issue\") severity: Literal[\"low\", \"medium\", \"high\"] = Field(description=\"The severity of the complaint\") description: str = Field(description=\"Brief description of the complaint\") agent = create\\_agent( model=\"gpt-5\", tools=tools, response\\_format=ToolStrategy(Union[ProductReview, CustomerComplaint]) ) result = agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"Analyze this review: 'Great product: 5 out of 5 stars. Fast shipping, but expensive'\"}] }) result[\"structured\\_response\"] # ProductReview(rating=5, sentiment='positive', key\\_points=['fast shipping', 'expensive']) ```  ### Custom tool message content The `tool\\_message\\_content` parameter allows you to customize the message that appears in the conversation history when structured output is generated: ```python theme={null} from pydantic import BaseModel, Field from typing import Literal from langchain.agents import create\\_agent from langchain.agents.structured\\_output import ToolStrategy class MeetingAction(BaseModel): \"\"\"Action items extracted from a meeting transcript.\"\"\" task: str = Field(description=\"The specific task to be completed\") assignee: str = Field(description=\"Person responsible for the task\") priority: Literal[\"low\", \"medium\", \"high\"] = Field(description=\"Priority level\") agent = create\\_agent( model=\"gpt-5\", tools=[], response\\_format=ToolStrategy( schema=MeetingAction, tool\\_message\\_content=\"Action item captured and added to meeting notes!\" ) ) agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"From our meeting: Sarah needs to update the project timeline as soon as possible\"}] }) ``` ``` ================================ Human Message ================================= From our meeting: Sarah needs to update the project timeline as soon as possible ================================== Ai Message ================================== Tool Calls: MeetingAction (call\\_1) Call ID: call\\_1 Args: task: Update the project timeline assignee: Sarah priority: high ================================= Tool Message ================================= Name: MeetingAction Action item captured and added to meeting notes! ``` Without `tool\\_message\\_content`, our final [`ToolMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage) would be: ``` ================================= Tool Message ================================= Name: MeetingAction Returning structured response: {'task': 'update the project timeline', 'assignee': 'Sarah', 'priority': 'high'} ``` ### Error handling Models can make mistakes when generating structured output via tool calling. LangChain provides intelligent retry mechanisms to handle these errors automatically. #### Multiple structured outputs error When a model incorrectly calls multiple structured output tools, the agent provides error feedback in a [`ToolMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage) and prompts the model to retry: ```python theme={null} from pydantic import BaseModel, Field from typing import Union from langchain.agents import create\\_agent from langchain.agents.structured\\_output import ToolStrategy class ContactInfo(BaseModel): name: str = Field(description=\"Person's name\") email: str = Field(description=\"Email address\") class EventDetails(BaseModel): event\\_name: str = Field(description=\"Name of the event\") date: str = Field(description=\"Event date\") agent = create\\_agent( model=\"gpt-5\", tools=[], response\\_format=ToolStrategy(Union[ContactInfo, EventDetails]) # Default: handle\\_errors=True ) agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th\"}] }) ``` ``` ================================ Human Message ================================= Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th None ================================== Ai Message ================================== Tool Calls: ContactInfo (call\\_1) Call ID: call\\_1 Args: name: John Doe email: john@email.com EventDetails (call\\_2) Call ID: call\\_2 Args: event\\_name: Tech Conference date: March 15th ================================= Tool Message ================================= Name: ContactInfo Error: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected. Please fix your mistakes. ================================= Tool Message ================================= Name: EventDetails Error: Model incorrectly returned multiple structured responses (ContactInfo, EventDetails) when only one is expected. Please fix your mistakes. ================================== Ai Message ================================== Tool Calls: ContactInfo (call\\_3) Call ID: call\\_3 Args: name: John Doe email: john@email.com ================================= Tool Message ================================= Name: ContactInfo Returning structured response: {'name': 'John Doe', 'email': 'john@email.com'} ``` #### Schema validation error When structured output doesn't match the expected schema, the agent provides specific error feedback: ```python theme={null} from pydantic import BaseModel, Field from langchain.agents import create\\_agent from langchain.agents.structured\\_output import ToolStrategy class ProductRating(BaseModel): rating: int | None = Field(description=\"Rating from 1-5\", ge=1, le=5) comment: str = Field(description=\"Review comment\") agent = create\\_agent( model=\"gpt-5\", tools=[], response\\_format=ToolStrategy(ProductRating), # Default: handle\\_errors=True system\\_prompt=\"You are a helpful assistant that parses product reviews. Do not make any field or value up.\" ) agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"Parse this: Amazing product, 10/10!\"}] }) ``` ``` ================================ Human Message ================================= Parse this: Amazing product, 10/10! ================================== Ai Message ================================== Tool Calls: ProductRating (call\\_1) Call ID: call\\_1 Args: rating: 10 comment: Amazing product ================================= Tool Message ================================= Name: ProductRating Error: Failed to parse structured output for tool 'ProductRating': 1 validation error for ProductRating.rating Input should be less than or equal to 5 [type=less\\_than\\_equal, input\\_value=10, input\\_type=int]. Please fix your mistakes. ================================== Ai Message ================================== Tool Calls: ProductRating (call\\_2) Call ID: call\\_2 Args: rating: 5 comment: Amazing product ================================= Tool Message ================================= Name: ProductRating Returning structured response: {'rating': 5, 'comment': 'Amazing product'} ``` #### Error handling strategies You can customize how errors are handled using the `handle\\_errors` parameter: \\*\\*Custom error message:\\*\\* ```python theme={null} ToolStrategy( schema=ProductRating, handle\\_errors=\"Please provide a valid rating between 1-5 and include a comment.\" ) ``` If `handle\\_errors` is a string, the agent will \\*always\\* prompt the model to re-try with a fixed tool message: ``` ================================= Tool Message ================================= Name: ProductRating Please provide a valid rating between 1-5 and include a comment. ``` \\*\\*Handle specific exceptions only:\\*\\* ```python theme={null} ToolStrategy( schema=ProductRating, handle\\_errors=ValueError # Only retry on ValueError, raise others ) ``` If `handle\\_errors` is an exception type, the agent will only retry (using the default error message) if the exception raised is the specified type. In all other cases, the exception will be raised. \\*\\*Handle multiple exception types:\\*\\* ```python theme={null} ToolStrategy( schema=ProductRating, handle\\_errors=(ValueError, TypeError) # Retry on ValueError and TypeError ) ``` If `handle\\_errors` is a tuple of exceptions, the agent will only retry (using the default error message) if the exception raised is one of the specified types. In all other cases, the exception will be raised. \\*\\*Custom error handler function:\\*\\* ```python theme={null} from langchain.agents.structured\\_output import StructuredOutputValidationError from langchain.agents.structured\\_output import MultipleStructuredOutputsError def custom\\_error\\_handler(error: Exception) -> str: if isinstance(error, StructuredOutputValidationError): return \"There was an issue with the format. Try again. elif isinstance(error, MultipleStructuredOutputsError): return \"Multiple structured outputs were returned. Pick the most relevant one.\" else: return f\"Error: {str(error)}\" agent = create\\_agent( model=\"gpt-5\", tools=[], response\\_format=ToolStrategy( schema=Union[ContactInfo, EventDetails], handle\\_errors=custom\\_error\\_handler ) # Default: handle\\_errors=True ) result = agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"Extract info: John Doe (john@email.com) is organizing Tech Conference on March 15th\"}] }) for msg in result['messages']: # If message is actually a ToolMessage object (not a dict), check its class name if type(msg).\\_\\_name\\_\\_ == \"ToolMessage\": print(msg.content) # If message is a dictionary or you want a fallback elif isinstance(msg, dict) and msg.get('tool\\_call\\_id'): print(msg['content']) ``` On `StructuredOutputValidationError`: ``` ================================= Tool Message ================================= Name: ToolStrategy There was an issue with the format. Try again. ``` On `MultipleStructuredOutputsError`: ``` ================================= Tool Message ================================= Name: ToolStrategy Multiple structured outputs were returned. Pick the most relevant one. ``` On other errors: ``` ================================= Tool Message ================================= Name: ToolStrategy Error:  ``` \\*\\*No error handling:\\*\\* ```python theme={null} response\\_format = ToolStrategy( schema=ProductRating, handle\\_errors=False # All errors raised ) ``` \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/structured-output.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/studio.md",
      "title": null,
      "raw_content": "# LangSmith Studio When building agents with LangChain locally, it's helpful to visualize what's happening inside your agent, interact with it in real-time, and debug issues as they occur. \\*\\*LangSmith Studio\\*\\* is a free visual interface for developing and testing your LangChain agents from your local machine. Studio connects to your locally running agent to show you each step your agent takes: the prompts sent to the model, tool calls and their results, and the final output. You can test different inputs, inspect intermediate states, and iterate on your agent's behavior without additional code or deployment. This pages describes how to set up Studio with your local LangChain agent. ## Prerequisites Before you begin, ensure you have the following: \\* \\*\\*A LangSmith account\\*\\*: Sign up (for free) or log in at [smith.langchain.com](https://smith.langchain.com). \\* \\*\\*A LangSmith API key\\*\\*: Follow the [Create an API key](/langsmith/create-account-api-key#create-an-api-key) guide. \\* If you don't want data [traced](/langsmith/observability-concepts#traces) to LangSmith, set `LANGSMITH\\_TRACING=false` in your application's `.env` file. With tracing disabled, no data leaves your local server. ## Set up local Agent server ### 1. Install the LangGraph CLI The [LangGraph CLI](/langsmith/cli) provides a local development server (also called [Agent Server](/langsmith/agent-server)) that connects your agent to Studio. ```shell theme={null} # Python >= 3.11 is required. pip install --upgrade \"langgraph-cli[inmem]\" ``` ### 2. Prepare your agent If you already have a LangChain agent, you can use it directly. This example uses a simple email agent: ```python title=\"agent.py\" theme={null} from langchain.agents import create\\_agent def send\\_email(to: str, subject: str, body: str): \"\"\"Send an email\"\"\" email = { \"to\": to, \"subject\": subject, \"body\": body } # ... email sending logic return f\"Email sent to {to}\" agent = create\\_agent( \"gpt-4o\", tools=[send\\_email], system\\_prompt=\"You are an email assistant. Always use the send\\_email tool.\", ) ``` ### 3. Environment variables Studio requires a LangSmith API key to connect your local agent. Create a `.env` file in the root of your project and add your API key from [LangSmith](https://smith.langchain.com/settings).  Ensure your `.env` file is not committed to version control, such as Git.  ```bash .env theme={null} LANGSMITH\\_API\\_KEY=lsv2... ``` ### 4. Create a LangGraph config file The LangGraph CLI uses a configuration file to locate your agent and manage dependencies. Create a `langgraph.json` file in your app's directory: ```json title=\"langgraph.json\" theme={null} { \"dependencies\": [\".\"], \"graphs\": { \"agent\": \"./src/agent.py:agent\" }, \"env\": \".env\" } ``` The [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent) function automatically returns a compiled LangGraph graph, which is what the `graphs` key expects in the configuration file.  For detailed explanations of each key in the JSON object of the configuration file, refer to the [LangGraph configuration file reference](/langsmith/cli#configuration-file).  At this point, the project structure will look like this: ```bash theme={null} my-app/ \u251c\u2500\u2500 src \u2502 \u2514\u2500\u2500 agent.py \u251c\u2500\u2500 .env \u2514\u2500\u2500 langgraph.json ``` ### 5. Install dependencies Install your project dependencies from the root directory:  ```shell pip theme={null} pip install -e . ``` ```shell uv theme={null} uv sync ```  ### 6. View your agent in Studio Start the development server to connect your agent to Studio: ```shell theme={null} langgraph dev ```  Safari blocks `localhost` connections to Studio. To work around this, run the above command with `--tunnel` to access Studio via a secure tunnel.  Once the server is running, your agent is accessible both via API at `http://127.0.0.1:2024` and through the Studio UI at `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`: With Studio connected to your local agent, you can iterate quickly on your agent's behavior. Run a test input, inspect the full execution trace including prompts, tool arguments, return values, and token/latency metrics. When something goes wrong, Studio captures exceptions with the surrounding state to help you understand what happened. The development server supports hot-reloading\u2014make changes to prompts or tool signatures in your code, and Studio reflects them immediately. Re-run conversation threads from any step to test your changes without starting over. This workflow scales from simple single-tool agents to complex multi-node graphs. For more information on how to run Studio, refer to the following guides in the [LangSmith docs](/langsmith/home): \\* [Run application](/langsmith/use-studio#run-application) \\* [Manage assistants](/langsmith/use-studio#manage-assistants) \\* [Manage threads](/langsmith/use-studio#manage-threads) \\* [Iterate on prompts](/langsmith/observability-studio) \\* [Debug LangSmith traces](/langsmith/observability-studio#debug-langsmith-traces) \\* [Add node to dataset](/langsmith/observability-studio#add-node-to-dataset) ## Video guide  For more information about local and deployed agents, see [Set up local Agent Server](/oss/python/langchain/studio#setup-local-agent-server) and [Deploy](/oss/python/langchain/deploy).  \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/studio.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/supervisor.md",
      "title": null,
      "raw_content": "# Build a supervisor agent ## Overview The \\*\\*supervisor pattern\\*\\* is a [multi-agent](/oss/python/langchain/multi-agent) architecture where a central supervisor agent coordinates specialized worker agents. This approach excels when tasks require different types of expertise. Rather than building one agent that manages tool selection across domains, you create focused specialists coordinated by a supervisor who understands the overall workflow. In this tutorial, you'll build a personal assistant system that demonstrates these benefits through a realistic workflow. The system will coordinate two specialists with fundamentally different responsibilities: \\* A \\*\\*calendar agent\\*\\* that handles scheduling, availability checking, and event management. \\* An \\*\\*email agent\\*\\* that manages communication, drafts messages, and sends notifications. We will also incorporate [human-in-the-loop review](/oss/python/langchain/human-in-the-loop) to allow users to approve, edit, and reject actions (such as outbound emails) as desired. ### Why use a supervisor? Multi-agent architectures allow you to partition [tools](/oss/python/langchain/tools) across workers, each with their own individual prompts or instructions. Consider an agent with direct access to all calendar and email APIs: it must choose from many similar tools, understand exact formats for each API, and handle multiple domains simultaneously. If performance degrades, it may be helpful to separate related tools and associated prompts into logical groups (in part to manage iterative improvements). ### Concepts We will cover the following concepts: \\* [Multi-agent systems](/oss/python/langchain/multi-agent) \\* [Human-in-the-loop review](/oss/python/langchain/human-in-the-loop) ## Setup ### Installation This tutorial requires the `langchain` package:  ```bash pip theme={null} pip install langchain ``` ```bash conda theme={null} conda install langchain -c conda-forge ```  For more details, see our [Installation guide](/oss/python/langchain/install). ### LangSmith Set up [LangSmith](https://smith.langchain.com) to inspect what is happening inside your agent. Then set the following environment variables:  ```bash bash theme={null} export LANGSMITH\\_TRACING=\"true\" export LANGSMITH\\_API\\_KEY=\"...\" ``` ```python python theme={null} import getpass import os os.environ[\"LANGSMITH\\_TRACING\"] = \"true\" os.environ[\"LANGSMITH\\_API\\_KEY\"] = getpass.getpass() ```  ### Components We will need to select a chat model from LangChain's suite of integrations:   \ud83d\udc49 Read the [OpenAI chat model integration docs](/oss/python/integrations/chat/openai/) ```shell theme={null} pip install -U \"langchain[openai]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"OPENAI\\_API\\_KEY\"] = \"sk-...\" model = init\\_chat\\_model(\"gpt-4.1\") ``` ```python Model Class theme={null} import os from langchain\\_openai import ChatOpenAI os.environ[\"OPENAI\\_API\\_KEY\"] = \"sk-...\" model = ChatOpenAI(model=\"gpt-4.1\") ```    \ud83d\udc49 Read the [Anthropic chat model integration docs](/oss/python/integrations/chat/anthropic/) ```shell theme={null} pip install -U \"langchain[anthropic]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"ANTHROPIC\\_API\\_KEY\"] = \"sk-...\" model = init\\_chat\\_model(\"claude-sonnet-4-5-20250929\") ``` ```python Model Class theme={null} import os from langchain\\_anthropic import ChatAnthropic os.environ[\"ANTHROPIC\\_API\\_KEY\"] = \"sk-...\" model = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\") ```    \ud83d\udc49 Read the [Azure chat model integration docs](/oss/python/integrations/chat/azure\\_chat\\_openai/) ```shell theme={null} pip install -U \"langchain[openai]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"AZURE\\_OPENAI\\_API\\_KEY\"] = \"...\" os.environ[\"AZURE\\_OPENAI\\_ENDPOINT\"] = \"...\" os.environ[\"OPENAI\\_API\\_VERSION\"] = \"2025-03-01-preview\" model = init\\_chat\\_model( \"azure\\_openai:gpt-4.1\", azure\\_deployment=os.environ[\"AZURE\\_OPENAI\\_DEPLOYMENT\\_NAME\"], ) ``` ```python Model Class theme={null} import os from langchain\\_openai import AzureChatOpenAI os.environ[\"AZURE\\_OPENAI\\_API\\_KEY\"] = \"...\" os.environ[\"AZURE\\_OPENAI\\_ENDPOINT\"] = \"...\" os.environ[\"OPENAI\\_API\\_VERSION\"] = \"2025-03-01-preview\" model = AzureChatOpenAI( model=\"gpt-4.1\", azure\\_deployment=os.environ[\"AZURE\\_OPENAI\\_DEPLOYMENT\\_NAME\"] ) ```    \ud83d\udc49 Read the [Google GenAI chat model integration docs](/oss/python/integrations/chat/google\\_generative\\_ai/) ```shell theme={null} pip install -U \"langchain[google-genai]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"GOOGLE\\_API\\_KEY\"] = \"...\" model = init\\_chat\\_model(\"google\\_genai:gemini-2.5-flash-lite\") ``` ```python Model Class theme={null} import os from langchain\\_google\\_genai import ChatGoogleGenerativeAI os.environ[\"GOOGLE\\_API\\_KEY\"] = \"...\" model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\") ```    \ud83d\udc49 Read the [AWS Bedrock chat model integration docs](/oss/python/integrations/chat/bedrock/) ```shell theme={null} pip install -U \"langchain[aws]\" ```  ```python init\\_chat\\_model theme={null} from langchain.chat\\_models import init\\_chat\\_model # Follow the steps here to configure your credentials: # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html model = init\\_chat\\_model( \"anthropic.claude-3-5-sonnet-20240620-v1:0\", model\\_provider=\"bedrock\\_converse\", ) ``` ```python Model Class theme={null} from langchain\\_aws import ChatBedrock model = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\") ```   \ud83d\udc49 Read the [HuggingFace chat model integration docs](/oss/python/integrations/chat/huggingface/) ```shell theme={null} pip install -U \"langchain[huggingface]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"HUGGINGFACEHUB\\_API\\_TOKEN\"] = \"hf\\_...\" model = init\\_chat\\_model( \"microsoft/Phi-3-mini-4k-instruct\", model\\_provider=\"huggingface\", temperature=0.7, max\\_tokens=1024, ) ``` ```python Model Class theme={null} import os from langchain\\_huggingface import ChatHuggingFace, HuggingFaceEndpoint os.environ[\"HUGGINGFACEHUB\\_API\\_TOKEN\"] = \"hf\\_...\" llm = HuggingFaceEndpoint( repo\\_id=\"microsoft/Phi-3-mini-4k-instruct\", temperature=0.7, max\\_length=1024, ) model = ChatHuggingFace(llm=llm) ```     ## 1. Define tools Start by defining the tools that require structured inputs. In real applications, these would call actual APIs (Google Calendar, SendGrid, etc.). For this tutorial, you'll use stubs to demonstrate the pattern. ```python theme={null} from langchain.tools import tool @tool def create\\_calendar\\_event( title: str, start\\_time: str, # ISO format: \"2024-01-15T14:00:00\" end\\_time: str, # ISO format: \"2024-01-15T15:00:00\" attendees: list[str], # email addresses location: str = \"\" ) -> str: \"\"\"Create a calendar event. Requires exact ISO datetime format.\"\"\" # Stub: In practice, this would call Google Calendar API, Outlook API, etc. return f\"Event created: {title} from {start\\_time} to {end\\_time} with {len(attendees)} attendees\" @tool def send\\_email( to: list[str], # email addresses subject: str, body: str, cc: list[str] = [] ) -> str: \"\"\"Send an email via email API. Requires properly formatted addresses.\"\"\" # Stub: In practice, this would call SendGrid, Gmail API, etc. return f\"Email sent to {', '.join(to)} - Subject: {subject}\" @tool def get\\_available\\_time\\_slots( attendees: list[str], date: str, # ISO format: \"2024-01-15\" duration\\_minutes: int ) -> list[str]: \"\"\"Check calendar availability for given attendees on a specific date.\"\"\" # Stub: In practice, this would query calendar APIs return [\"09:00\", \"14:00\", \"16:00\"] ``` ## 2. Create specialized sub-agents Next, we'll create specialized sub-agents that handle each domain. ### Create a calendar agent The calendar agent understands natural language scheduling requests and translates them into precise API calls. It handles date parsing, availability checking, and event creation. ```python theme={null} from langchain.agents import create\\_agent CALENDAR\\_AGENT\\_PROMPT = ( \"You are a calendar scheduling assistant. \" \"Parse natural language scheduling requests (e.g., 'next Tuesday at 2pm') \" \"into proper ISO datetime formats. \" \"Use get\\_available\\_time\\_slots to check availability when needed. \" \"Use create\\_calendar\\_event to schedule events. \" \"Always confirm what was scheduled in your final response.\" ) calendar\\_agent = create\\_agent( model, tools=[create\\_calendar\\_event, get\\_available\\_time\\_slots], system\\_prompt=CALENDAR\\_AGENT\\_PROMPT, ) ``` Test the calendar agent to see how it handles natural language scheduling: ```python theme={null} query = \"Schedule a team meeting next Tuesday at 2pm for 1 hour\" for step in calendar\\_agent.stream( {\"messages\": [{\"role\": \"user\", \"content\": query}]} ): for update in step.values(): for message in update.get(\"messages\", []): message.pretty\\_print() ``` ``` ================================== Ai Message ================================== Tool Calls: get\\_available\\_time\\_slots (call\\_EIeoeIi1hE2VmwZSfHStGmXp) Call ID: call\\_EIeoeIi1hE2VmwZSfHStGmXp Args: attendees: [] date: 2024-06-18 duration\\_minutes: 60 ================================= Tool Message ================================= Name: get\\_available\\_time\\_slots [\"09:00\", \"14:00\", \"16:00\"] ================================== Ai Message ================================== Tool Calls: create\\_calendar\\_event (call\\_zgx3iJA66Ut0W8S3NpT93kEB) Call ID: call\\_zgx3iJA66Ut0W8S3NpT93kEB Args: title: Team Meeting start\\_time: 2024-06-18T14:00:00 end\\_time: 2024-06-18T15:00:00 attendees: [] ================================= Tool Message ================================= Name: create\\_calendar\\_event Event created: Team Meeting from 2024-06-18T14:00:00 to 2024-06-18T15:00:00 with 0 attendees ================================== Ai Message ================================== The team meeting has been scheduled for next Tuesday, June 18th, at 2:00 PM and will last for 1 hour. If you need to add attendees or a location, please let me know! ``` The agent parses \"next Tuesday at 2pm\" into ISO format (\"2024-01-16T14:00:00\"), calculates the end time, calls `create\\_calendar\\_event`, and returns a natural language confirmation. ### Create an email agent The email agent handles message composition and sending. It focuses on extracting recipient information, crafting appropriate subject lines and body text, and managing email communication. ```python theme={null} EMAIL\\_AGENT\\_PROMPT = ( \"You are an email assistant. \" \"Compose professional emails based on natural language requests. \" \"Extract recipient information and craft appropriate subject lines and body text. \" \"Use send\\_email to send the message. \" \"Always confirm what was sent in your final response.\" ) email\\_agent = create\\_agent( model, tools=[send\\_email], system\\_prompt=EMAIL\\_AGENT\\_PROMPT, ) ``` Test the email agent with a natural language request: ```python theme={null} query = \"Send the design team a reminder about reviewing the new mockups\" for step in email\\_agent.stream( {\"messages\": [{\"role\": \"user\", \"content\": query}]} ): for update in step.values(): for message in update.get(\"messages\", []): message.pretty\\_print() ``` ``` ================================== Ai Message ================================== Tool Calls: send\\_email (call\\_OMl51FziTVY6CRZvzYfjYOZr) Call ID: call\\_OMl51FziTVY6CRZvzYfjYOZr Args: to: ['design-team@example.com'] subject: Reminder: Please Review the New Mockups body: Hi Design Team, This is a friendly reminder to review the new mockups at your earliest convenience. Your feedback is important to ensure that we stay on track with our project timeline. Please let me know if you have any questions or need additional information. Thank you! Best regards, ================================= Tool Message ================================= Name: send\\_email Email sent to design-team@example.com - Subject: Reminder: Please Review the New Mockups ================================== Ai Message ================================== I've sent a reminder to the design team asking them to review the new mockups. If you need any further communication on this topic, just let me know! ``` The agent infers the recipient from the informal request, crafts a professional subject line and body, calls `send\\_email`, and returns a confirmation. Each sub-agent has a narrow focus with domain-specific tools and prompts, allowing it to excel at its specific task. ## 3. Wrap sub-agents as tools Now wrap each sub-agent as a tool that the supervisor can invoke. This is the key architectural step that creates the layered system. The supervisor will see high-level tools like \"schedule\\\\_event\", not low-level tools like \"create\\\\_calendar\\\\_event\". ```python theme={null} @tool def schedule\\_event(request: str) -> str: \"\"\"Schedule calendar events using natural language. Use this when the user wants to create, modify, or check calendar appointments. Handles date/time parsing, availability checking, and event creation. Input: Natural language scheduling request (e.g., 'meeting with design team next Tuesday at 2pm') \"\"\" result = calendar\\_agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": request}] }) return result[\"messages\"][-1].text @tool def manage\\_email(request: str) -> str: \"\"\"Send emails using natural language. Use this when the user wants to send notifications, reminders, or any email communication. Handles recipient extraction, subject generation, and email composition. Input: Natural language email request (e.g., 'send them a reminder about the meeting') \"\"\" result = email\\_agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": request}] }) return result[\"messages\"][-1].text ``` The tool descriptions help the supervisor decide when to use each tool, so make them clear and specific. We return only the sub-agent's final response, as the supervisor doesn't need to see intermediate reasoning or tool calls. ## 4. Create the supervisor agent Now create the supervisor that orchestrates the sub-agents. The supervisor only sees high-level tools and makes routing decisions at the domain level, not the individual API level. ```python theme={null} SUPERVISOR\\_PROMPT = ( \"You are a helpful personal assistant. \" \"You can schedule calendar events and send emails. \" \"Break down user requests into appropriate tool calls and coordinate the results. \" \"When a request involves multiple actions, use multiple tools in sequence.\" ) supervisor\\_agent = create\\_agent( model, tools=[schedule\\_event, manage\\_email], system\\_prompt=SUPERVISOR\\_PROMPT, ) ``` ## 5. Use the supervisor Now test your complete system with complex requests that require coordination across multiple domains: ### Example 1: Simple single-domain request ```python theme={null} query = \"Schedule a team standup for tomorrow at 9am\" for step in supervisor\\_agent.stream( {\"messages\": [{\"role\": \"user\", \"content\": query}]} ): for update in step.values(): for message in update.get(\"messages\", []): message.pretty\\_print() ``` ``` ================================== Ai Message ================================== Tool Calls: schedule\\_event (call\\_mXFJJDU8bKZadNUZPaag8Lct) Call ID: call\\_mXFJJDU8bKZadNUZPaag8Lct Args: request: Schedule a team standup for tomorrow at 9am with Alice and Bob. ================================= Tool Message ================================= Name: schedule\\_event The team standup has been scheduled for tomorrow at 9:00 AM with Alice and Bob. If you need to make any changes or add more details, just let me know! ================================== Ai Message ================================== The team standup with Alice and Bob is scheduled for tomorrow at 9:00 AM. If you need any further arrangements or adjustments, please let me know! ``` The supervisor identifies this as a calendar task, calls `schedule\\_event`, and the calendar agent handles date parsing and event creation.  For full transparency into the information flow, including prompts and responses for each chat model call, check out the [LangSmith trace](https://smith.langchain.com/public/91a9a95f-fba9-4e84-aff0-371861ad2f4a/r) for the above run.  ### Example 2: Complex multi-domain request ```python theme={null} query = ( \"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \" \"and send them an email reminder about reviewing the new mockups.\" ) for step in supervisor\\_agent.stream( {\"messages\": [{\"role\": \"user\", \"content\": query}]} ): for update in step.values(): for message in update.get(\"messages\", []): message.pretty\\_print() ``` ``` ================================== Ai Message ================================== Tool Calls: schedule\\_event (call\\_YA68mqF0koZItCFPx0kGQfZi) Call ID: call\\_YA68mqF0koZItCFPx0kGQfZi Args: request: meeting with the design team next Tuesday at 2pm for 1 hour manage\\_email (call\\_XxqcJBvVIuKuRK794ZIzlLxx) Call ID: call\\_XxqcJBvVIuKuRK794ZIzlLxx Args: request: send the design team an email reminder about reviewing the new mockups ================================= Tool Message ================================= Name: schedule\\_event Your meeting with the design team is scheduled for next Tuesday, June 18th, from 2:00pm to 3:00pm. Let me know if you need to add more details or make any changes! ================================= Tool Message ================================= Name: manage\\_email I've sent an email reminder to the design team requesting them to review the new mockups. If you need to include more information or recipients, just let me know! ================================== Ai Message ================================== Your meeting with the design team is scheduled for next Tuesday, June 18th, from 2:00pm to 3:00pm. I've also sent an email reminder to the design team, asking them to review the new mockups. Let me know if you'd like to add more details to the meeting or include additional information in the email! ``` The supervisor recognizes this requires both calendar and email actions, calls `schedule\\_event` for the meeting, then calls `manage\\_email` for the reminder. Each sub-agent completes its task, and the supervisor synthesizes both results into a coherent response.  Refer to the [LangSmith trace](https://smith.langchain.com/public/95cd00a3-d1f9-4dba-9731-7bf733fb6a3c/r) to see the detailed information flow for the above run, including individual chat model prompts and responses.  ### Complete working example Here's everything together in a runnable script:  ```python theme={null} \"\"\" Personal Assistant Supervisor Example This example demonstrates the tool calling pattern for multi-agent systems. A supervisor agent coordinates specialized sub-agents (calendar and email) that are wrapped as tools. \"\"\" from langchain.tools import tool from langchain.agents import create\\_agent from langchain.chat\\_models import init\\_chat\\_model # ============================================================================ # Step 1: Define low-level API tools (stubbed) # ============================================================================ @tool def create\\_calendar\\_event( title: str, start\\_time: str, # ISO format: \"2024-01-15T14:00:00\" end\\_time: str, # ISO format: \"2024-01-15T15:00:00\" attendees: list[str], # email addresses location: str = \"\" ) -> str: \"\"\"Create a calendar event. Requires exact ISO datetime format.\"\"\" return f\"Event created: {title} from {start\\_time} to {end\\_time} with {len(attendees)} attendees\" @tool def send\\_email( to: list[str], # email addresses subject: str, body: str, cc: list[str] = [] ) -> str: \"\"\"Send an email via email API. Requires properly formatted addresses.\"\"\" return f\"Email sent to {', '.join(to)} - Subject: {subject}\" @tool def get\\_available\\_time\\_slots( attendees: list[str], date: str, # ISO format: \"2024-01-15\" duration\\_minutes: int ) -> list[str]: \"\"\"Check calendar availability for given attendees on a specific date.\"\"\" return [\"09:00\", \"14:00\", \"16:00\"] # ============================================================================ # Step 2: Create specialized sub-agents # ============================================================================ model = init\\_chat\\_model(\"claude-haiku-4-5-20251001\") # for example calendar\\_agent = create\\_agent( model, tools=[create\\_calendar\\_event, get\\_available\\_time\\_slots], system\\_prompt=( \"You are a calendar scheduling assistant. \" \"Parse natural language scheduling requests (e.g., 'next Tuesday at 2pm') \" \"into proper ISO datetime formats. \" \"Use get\\_available\\_time\\_slots to check availability when needed. \" \"Use create\\_calendar\\_event to schedule events. \" \"Always confirm what was scheduled in your final response.\" ) ) email\\_agent = create\\_agent( model, tools=[send\\_email], system\\_prompt=( \"You are an email assistant. \" \"Compose professional emails based on natural language requests. \" \"Extract recipient information and craft appropriate subject lines and body text. \" \"Use send\\_email to send the message. \" \"Always confirm what was sent in your final response.\" ) ) # ============================================================================ # Step 3: Wrap sub-agents as tools for the supervisor # ============================================================================ @tool def schedule\\_event(request: str) -> str: \"\"\"Schedule calendar events using natural language. Use this when the user wants to create, modify, or check calendar appointments. Handles date/time parsing, availability checking, and event creation. Input: Natural language scheduling request (e.g., 'meeting with design team next Tuesday at 2pm') \"\"\" result = calendar\\_agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": request}] }) return result[\"messages\"][-1].text @tool def manage\\_email(request: str) -> str: \"\"\"Send emails using natural language. Use this when the user wants to send notifications, reminders, or any email communication. Handles recipient extraction, subject generation, and email composition. Input: Natural language email request (e.g., 'send them a reminder about the meeting') \"\"\" result = email\\_agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": request}] }) return result[\"messages\"][-1].text # ============================================================================ # Step 4: Create the supervisor agent # ============================================================================ supervisor\\_agent = create\\_agent( model, tools=[schedule\\_event, manage\\_email], system\\_prompt=( \"You are a helpful personal assistant. \" \"You can schedule calendar events and send emails. \" \"Break down user requests into appropriate tool calls and coordinate the results. \" \"When a request involves multiple actions, use multiple tools in sequence.\" ) ) # ============================================================================ # Step 5: Use the supervisor # ============================================================================ if \\_\\_name\\_\\_ == \"\\_\\_main\\_\\_\": # Example: User request requiring both calendar and email coordination user\\_request = ( \"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \" \"and send them an email reminder about reviewing the new mockups.\" ) print(\"User Request:\", user\\_request) print(\"\\n\" + \"=\"\\*80 + \"\\n\") for step in supervisor\\_agent.stream( {\"messages\": [{\"role\": \"user\", \"content\": user\\_request}]} ): for update in step.values(): for message in update.get(\"messages\", []): message.pretty\\_print() ```  ### Understanding the architecture Your system has three layers. The bottom layer contains rigid API tools that require exact formats. The middle layer contains sub-agents that accept natural language, translate it to structured API calls, and return natural language confirmations. The top layer contains the supervisor that routes to high-level capabilities and synthesizes results. This separation of concerns provides several benefits: each layer has a focused responsibility, you can add new domains without affecting existing ones, and you can test and iterate on each layer independently. ## 6. Add human-in-the-loop review It can be prudent to incorporate [human-in-the-loop review](/oss/python/langchain/human-in-the-loop) of sensitive actions. LangChain includes [built-in middleware](/oss/python/langchain/human-in-the-loop#configuring-interrupts) to review tool calls, in this case the tools invoked by sub-agents. Let's add human-in-the-loop review to both sub-agents: \\* We configure the `create\\_calendar\\_event` and `send\\_email` tools to interrupt, permitting all [response types](/oss/python/langchain/human-in-the-loop) (`approve`, `edit`, `reject`) \\* We add a [checkpointer](/oss/python/langchain/short-term-memory) \\*\\*only to the top-level agent\\*\\*. This is required to pause and resume execution. ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import HumanInTheLoopMiddleware # [!code highlight] from langgraph.checkpoint.memory import InMemorySaver # [!code highlight] calendar\\_agent = create\\_agent( model, tools=[create\\_calendar\\_event, get\\_available\\_time\\_slots], system\\_prompt=CALENDAR\\_AGENT\\_PROMPT, middleware=[ # [!code highlight] HumanInTheLoopMiddleware( # [!code highlight] interrupt\\_on={\"create\\_calendar\\_event\": True}, # [!code highlight] description\\_prefix=\"Calendar event pending approval\", # [!code highlight] ), # [!code highlight] ], # [!code highlight] ) email\\_agent = create\\_agent( model, tools=[send\\_email], system\\_prompt=EMAIL\\_AGENT\\_PROMPT, middleware=[ # [!code highlight] HumanInTheLoopMiddleware( # [!code highlight] interrupt\\_on={\"send\\_email\": True}, # [!code highlight] description\\_prefix=\"Outbound email pending approval\", # [!code highlight] ), # [!code highlight] ], # [!code highlight] ) supervisor\\_agent = create\\_agent( model, tools=[schedule\\_event, manage\\_email], system\\_prompt=SUPERVISOR\\_PROMPT, checkpointer=InMemorySaver(), # [!code highlight] ) ``` Let's repeat the query. Note that we gather interrupt events into a list to access downstream: ```python theme={null} query = ( \"Schedule a meeting with the design team next Tuesday at 2pm for 1 hour, \" \"and send them an email reminder about reviewing the new mockups.\" ) config = {\"configurable\": {\"thread\\_id\": \"6\"}} interrupts = [] for step in supervisor\\_agent.stream( {\"messages\": [{\"role\": \"user\", \"content\": query}]}, config, ): for update in step.values(): if isinstance(update, dict): for message in update.get(\"messages\", []): message.pretty\\_print() else: interrupt\\_ = update[0] interrupts.append(interrupt\\_) print(f\"\\nINTERRUPTED: {interrupt\\_.id}\") ``` ``` ================================== Ai Message ================================== Tool Calls: schedule\\_event (call\\_t4Wyn32ohaShpEZKuzZbl83z) Call ID: call\\_t4Wyn32ohaShpEZKuzZbl83z Args: request: Schedule a meeting with the design team next Tuesday at 2pm for 1 hour. manage\\_email (call\\_JWj4vDJ5VMnvkySymhCBm4IR) Call ID: call\\_JWj4vDJ5VMnvkySymhCBm4IR Args: request: Send an email reminder to the design team about reviewing the new mockups before our meeting next Tuesday at 2pm. INTERRUPTED: 4f994c9721682a292af303ec1a46abb7 INTERRUPTED: 2b56f299be313ad8bc689eff02973f16 ``` This time we've interrupted execution. Let's inspect the interrupt events: ```python theme={null} for interrupt\\_ in interrupts: for request in interrupt\\_.value[\"action\\_requests\"]: print(f\"INTERRUPTED: {interrupt\\_.id}\") print(f\"{request['description']}\\n\") ``` ``` INTERRUPTED: 4f994c9721682a292af303ec1a46abb7 Calendar event pending approval Tool: create\\_calendar\\_event Args: {'title': 'Meeting with the Design Team', 'start\\_time': '2024-06-18T14:00:00', 'end\\_time': '2024-06-18T15:00:00', 'attendees': ['design team']} INTERRUPTED: 2b56f299be313ad8bc689eff02973f16 Outbound email pending approval Tool: send\\_email Args: {'to': ['designteam@example.com'], 'subject': 'Reminder: Review New Mockups Before Meeting Next Tuesday at 2pm', 'body': \"Hello Team,\\n\\nThis is a reminder to review the new mockups ahead of our meeting scheduled for next Tuesday at 2pm. Your feedback and insights will be valuable for our discussion and next steps.\\n\\nPlease ensure you've gone through the designs and are ready to share your thoughts during the meeting.\\n\\nThank you!\\n\\nBest regards,\\n[Your Name]\"} ``` We can specify decisions for each interrupt by referring to its ID using a [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command). Refer to the [human-in-the-loop guide](/oss/python/langchain/human-in-the-loop) for additional details. For demonstration purposes, here we will accept the calendar event, but edit the subject of the outbound email: ```python theme={null} from langgraph.types import Command # [!code highlight] resume = {} for interrupt\\_ in interrupts: if interrupt\\_.id == \"2b56f299be313ad8bc689eff02973f16\": # Edit email edited\\_action = interrupt\\_.value[\"action\\_requests\"][0].copy() edited\\_action[\"arguments\"][\"subject\"] = \"Mockups reminder\" resume[interrupt\\_.id] = { \"decisions\": [{\"type\": \"edit\", \"edited\\_action\": edited\\_action}] } else: resume[interrupt\\_.id] = {\"decisions\": [{\"type\": \"approve\"}]} interrupts = [] for step in supervisor\\_agent.stream( Command(resume=resume), # [!code highlight] config, ): for update in step.values(): if isinstance(update, dict): for message in update.get(\"messages\", []): message.pretty\\_print() else: interrupt\\_ = update[0] interrupts.append(interrupt\\_) print(f\"\\nINTERRUPTED: {interrupt\\_.id}\") ``` ``` ================================= Tool Message ================================= Name: schedule\\_event Your meeting with the design team has been scheduled for next Tuesday, June 18th, from 2:00 pm to 3:00 pm. ================================= Tool Message ================================= Name: manage\\_email Your email reminder to the design team has been sent. Here\u2019s what was sent: - Recipient: designteam@example.com - Subject: Mockups reminder - Body: A reminder to review the new mockups before the meeting next Tuesday at 2pm, with a request for feedback and readiness for discussion. Let me know if you need any further assistance! ================================== Ai Message ================================== - Your meeting with the design team has been scheduled for next Tuesday, June 18th, from 2:00 pm to 3:00 pm. - An email reminder has been sent to the design team about reviewing the new mockups before the meeting. Let me know if you need any further assistance! ``` The run proceeds with our input. ## 7. Advanced: Control information flow By default, sub-agents receive only the request string from the supervisor. You might want to pass additional context, such as conversation history or user preferences. ### Pass additional conversational context to sub-agents ```python theme={null} from langchain.tools import tool, ToolRuntime @tool def schedule\\_event( request: str, runtime: ToolRuntime ) -> str: \"\"\"Schedule calendar events using natural language.\"\"\" # Customize context received by sub-agent original\\_user\\_message = next( message for message in runtime.state[\"messages\"] if message.type == \"human\" ) prompt = ( \"You are assisting with the following user inquiry:\\n\\n\" f\"{original\\_user\\_message.text}\\n\\n\" \"You are tasked with the following sub-request:\\n\\n\" f\"{request}\" ) result = calendar\\_agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": prompt}], }) return result[\"messages\"][-1].text ``` This allows sub-agents to see the full conversation context, which can be useful for resolving ambiguities like \"schedule it for the same time tomorrow\" (referencing a previous conversation).  You can see the full context received by the sub agent in the [chat model call](https://smith.langchain.com/public/c7d54882-afb8-4039-9c5a-4112d0f458b0/r/6803571e-af78-4c68-904a-ecf55771084d) of the LangSmith trace.  ### Control what supervisor receives You can also customize what information flows back to the supervisor: ```python theme={null} import json @tool def schedule\\_event(request: str) -> str: \"\"\"Schedule calendar events using natural language.\"\"\" result = calendar\\_agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": request}] }) # Option 1: Return just the confirmation message return result[\"messages\"][-1].text # Option 2: Return structured data # return json.dumps({ # \"status\": \"success\", # \"event\\_id\": \"evt\\_123\", # \"summary\": result[\"messages\"][-1].text # }) ``` \\*\\*Important:\\*\\* Make sure sub-agent prompts emphasize that their final message should contain all relevant information. A common failure mode is sub-agents that perform tool calls but don't include the results in their final response. ## 8. Key takeaways The supervisor pattern creates layers of abstraction where each layer has a clear responsibility. When designing a supervisor system, start with clear domain boundaries and give each sub-agent focused tools and prompts. Write clear tool descriptions for the supervisor, test each layer independently before integration, and control information flow based on your specific needs.  \\*\\*When to use the supervisor pattern\\*\\* Use the supervisor pattern when you have multiple distinct domains (calendar, email, CRM, database), each domain has multiple tools or complex logic, you want centralized workflow control, and sub-agents don't need to converse directly with users. For simpler cases with just a few tools, use a single agent. When agents need to have conversations with users, use [handoffs](/oss/python/langchain/multi-agent#handoffs) instead. For peer-to-peer collaboration between agents, consider other multi-agent patterns.  ## Next steps Learn about [handoffs](/oss/python/langchain/multi-agent#handoffs) for agent-to-agent conversations, explore [context engineering](/oss/python/langchain/context-engineering) to fine-tune information flow, read the [multi-agent overview](/oss/python/langchain/multi-agent) to compare different patterns, and use [LangSmith](https://smith.langchain.com) to debug and monitor your multi-agent system. \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/supervisor.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/test.md",
      "title": null,
      "raw_content": "# Test Agentic applications let an LLM decide its own next steps to solve a problem. That flexibility is powerful, but the model's black-box nature makes it hard to predict how a tweak in one part of your agent will affect the rest. To build production-ready agents, thorough testing is essential. There are a few approaches to testing your agents: \\* [Unit tests](#unit-testing) exercise small, deterministic pieces of your agent in isolation using in-memory fakes so you can assert exact behavior quickly and deterministically. \\* [Integration tests](#integration-testing) test the agent using real network calls to confirm that components work together, credentials and schemas line up, and latency is acceptable. Agentic applications tend to lean more on integration because they chain multiple components together and must deal with flakiness due to the nondeterministic nature of LLMs. ## Unit Testing ### Mocking Chat Model For logic not requiring API calls, you can use an in-memory stub for mocking responses. LangChain provides [`GenericFakeChatModel`](https://python.langchain.com/api\\_reference/core/language\\_models/langchain\\_core.language\\_models.fake\\_chat\\_models.GenericFakeChatModel.html) for mocking text responses. It accepts an iterator of responses (AIMessages or strings) and returns one per invocation. It supports both regular and streaming usage. ```python theme={null} from langchain\\_core.language\\_models.fake\\_chat\\_models import GenericFakeChatModel model = GenericFakeChatModel(messages=iter([ AIMessage(content=\"\", tool\\_calls=[ToolCall(name=\"foo\", args={\"bar\": \"baz\"}, id=\"call\\_1\")]), \"bar\" ])) model.invoke(\"hello\") # AIMessage(content='', ..., tool\\_calls=[{'name': 'foo', 'args': {'bar': 'baz'}, 'id': 'call\\_1', 'type': 'tool\\_call'}]) ``` If we invoke the model again, it will return the next item in the iterator: ```python theme={null} model.invoke(\"hello, again!\") # AIMessage(content='bar', ...) ``` ### InMemorySaver Checkpointer To enable persistence during testing, you can use the [`InMemorySaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.memory.InMemorySaver) checkpointer. This allows you to simulate multiple turns to test state-dependent behavior: ```python theme={null} from langgraph.checkpoint.memory import InMemorySaver agent = create\\_agent( model, tools=[], checkpointer=InMemorySaver() ) # First invocation agent.invoke(HumanMessage(content=\"I live in Sydney, Australia.\")) # Second invocation: the first message is persisted (Sydney location), so the model returns GMT+10 time agent.invoke(HumanMessage(content=\"What's my local time?\")) ``` ## Integration Testing Many agent behaviors only emerge when using a real LLM, such as which tool the agent decides to call, how it formats responses, or whether a prompt modification affects the entire execution trajectory. LangChain's [`agentevals`](https://github.com/langchain-ai/agentevals) package provides evaluators specifically designed for testing agent trajectories with live models. AgentEvals lets you easily evaluate the trajectory of your agent (the exact sequence of messages, including tool calls) by performing a \\*\\*trajectory match\\*\\* or by using an \\*\\*LLM judge\\*\\*:  Hard-code a reference trajectory for a given input and validate the run via a step-by-step comparison. Ideal for testing well-defined workflows where you know the expected behavior. Use when you have specific expectations about which tools should be called and in what order. This approach is deterministic, fast, and cost-effective since it doesn't require additional LLM calls.   Use a LLM to qualitatively validate your agent's execution trajectory. The \"judge\" LLM reviews the agent's decisions against a prompt rubric (which can include a reference trajectory). More flexible and can assess nuanced aspects like efficiency and appropriateness, but requires an LLM call and is less deterministic. Use when you want to evaluate the overall quality and reasonableness of the agent's trajectory without strict tool call or ordering requirements.  ### Installing AgentEvals ```bash theme={null} pip install agentevals ``` Or, clone the [AgentEvals repository](https://github.com/langchain-ai/agentevals) directly. ### Trajectory Match Evaluator AgentEvals offers the `create\\_trajectory\\_match\\_evaluator` function to match your agent's trajectory against a reference trajectory. There are four modes to choose from: | Mode | Description | Use Case | | ----------- | --------------------------------------------------------- | --------------------------------------------------------------------- | | `strict` | Exact match of messages and tool calls in the same order | Testing specific sequences (e.g., policy lookup before authorization) | | `unordered` | Same tool calls allowed in any order | Verifying information retrieval when order doesn't matter | | `subset` | Agent calls only tools from reference (no extras) | Ensuring agent doesn't exceed expected scope | | `superset` | Agent calls at least the reference tools (extras allowed) | Verifying minimum required actions are taken |  The `strict` mode ensures trajectories contain identical messages in the same order with the same tool calls, though it allows for differences in message content. This is useful when you need to enforce a specific sequence of operations, such as requiring a policy lookup before authorizing an action. ```python theme={null} from langchain.agents import create\\_agent from langchain.tools import tool from langchain.messages import HumanMessage, AIMessage, ToolMessage from agentevals.trajectory.match import create\\_trajectory\\_match\\_evaluator @tool def get\\_weather(city: str): \"\"\"Get weather information for a city.\"\"\" return f\"It's 75 degrees and sunny in {city}.\" agent = create\\_agent(\"gpt-4o\", tools=[get\\_weather]) evaluator = create\\_trajectory\\_match\\_evaluator( # [!code highlight] trajectory\\_match\\_mode=\"strict\", # [!code highlight] ) # [!code highlight] def test\\_weather\\_tool\\_called\\_strict(): result = agent.invoke({ \"messages\": [HumanMessage(content=\"What's the weather in San Francisco?\")] }) reference\\_trajectory = [ HumanMessage(content=\"What's the weather in San Francisco?\"), AIMessage(content=\"\", tool\\_calls=[ {\"id\": \"call\\_1\", \"name\": \"get\\_weather\", \"args\": {\"city\": \"San Francisco\"}} ]), ToolMessage(content=\"It's 75 degrees and sunny in San Francisco.\", tool\\_call\\_id=\"call\\_1\"), AIMessage(content=\"The weather in San Francisco is 75 degrees and sunny.\"), ] evaluation = evaluator( outputs=result[\"messages\"], reference\\_outputs=reference\\_trajectory ) # { # 'key': 'trajectory\\_strict\\_match', # 'score': True, # 'comment': None, # } assert evaluation[\"score\"] is True ```   The `unordered` mode allows the same tool calls in any order, which is helpful when you want to verify that specific information was retrieved but don't care about the sequence. For example, an agent might need to check both weather and events for a city, but the order doesn't matter. ```python theme={null} from langchain.agents import create\\_agent from langchain.tools import tool from langchain.messages import HumanMessage, AIMessage, ToolMessage from agentevals.trajectory.match import create\\_trajectory\\_match\\_evaluator @tool def get\\_weather(city: str): \"\"\"Get weather information for a city.\"\"\" return f\"It's 75 degrees and sunny in {city}.\" @tool def get\\_events(city: str): \"\"\"Get events happening in a city.\"\"\" return f\"Concert at the park in {city} tonight.\" agent = create\\_agent(\"gpt-4o\", tools=[get\\_weather, get\\_events]) evaluator = create\\_trajectory\\_match\\_evaluator( # [!code highlight] trajectory\\_match\\_mode=\"unordered\", # [!code highlight] ) # [!code highlight] def test\\_multiple\\_tools\\_any\\_order(): result = agent.invoke({ \"messages\": [HumanMessage(content=\"What's happening in SF today?\")] }) # Reference shows tools called in different order than actual execution reference\\_trajectory = [ HumanMessage(content=\"What's happening in SF today?\"), AIMessage(content=\"\", tool\\_calls=[ {\"id\": \"call\\_1\", \"name\": \"get\\_events\", \"args\": {\"city\": \"SF\"}}, {\"id\": \"call\\_2\", \"name\": \"get\\_weather\", \"args\": {\"city\": \"SF\"}}, ]), ToolMessage(content=\"Concert at the park in SF tonight.\", tool\\_call\\_id=\"call\\_1\"), ToolMessage(content=\"It's 75 degrees and sunny in SF.\", tool\\_call\\_id=\"call\\_2\"), AIMessage(content=\"Today in SF: 75 degrees and sunny with a concert at the park tonight.\"), ] evaluation = evaluator( outputs=result[\"messages\"], reference\\_outputs=reference\\_trajectory, ) # { # 'key': 'trajectory\\_unordered\\_match', # 'score': True, # } assert evaluation[\"score\"] is True ```   The `superset` and `subset` modes match partial trajectories. The `superset` mode verifies that the agent called at least the tools in the reference trajectory, allowing additional tool calls. The `subset` mode ensures the agent did not call any tools beyond those in the reference. ```python theme={null} from langchain.agents import create\\_agent from langchain.tools import tool from langchain.messages import HumanMessage, AIMessage, ToolMessage from agentevals.trajectory.match import create\\_trajectory\\_match\\_evaluator @tool def get\\_weather(city: str): \"\"\"Get weather information for a city.\"\"\" return f\"It's 75 degrees and sunny in {city}.\" @tool def get\\_detailed\\_forecast(city: str): \"\"\"Get detailed weather forecast for a city.\"\"\" return f\"Detailed forecast for {city}: sunny all week.\" agent = create\\_agent(\"gpt-4o\", tools=[get\\_weather, get\\_detailed\\_forecast]) evaluator = create\\_trajectory\\_match\\_evaluator( # [!code highlight] trajectory\\_match\\_mode=\"superset\", # [!code highlight] ) # [!code highlight] def test\\_agent\\_calls\\_required\\_tools\\_plus\\_extra(): result = agent.invoke({ \"messages\": [HumanMessage(content=\"What's the weather in Boston?\")] }) # Reference only requires get\\_weather, but agent may call additional tools reference\\_trajectory = [ HumanMessage(content=\"What's the weather in Boston?\"), AIMessage(content=\"\", tool\\_calls=[ {\"id\": \"call\\_1\", \"name\": \"get\\_weather\", \"args\": {\"city\": \"Boston\"}}, ]), ToolMessage(content=\"It's 75 degrees and sunny in Boston.\", tool\\_call\\_id=\"call\\_1\"), AIMessage(content=\"The weather in Boston is 75 degrees and sunny.\"), ] evaluation = evaluator( outputs=result[\"messages\"], reference\\_outputs=reference\\_trajectory, ) # { # 'key': 'trajectory\\_superset\\_match', # 'score': True, # 'comment': None, # } assert evaluation[\"score\"] is True ```   You can also set the `tool\\_args\\_match\\_mode` property and/or `tool\\_args\\_match\\_overrides` to customize how the evaluator considers equality between tool calls in the actual trajectory vs. the reference. By default, only tool calls with the same arguments to the same tool are considered equal. Visit the [repository](https://github.com/langchain-ai/agentevals?tab=readme-ov-file#tool-args-match-modes) for more details.  ### LLM-as-Judge Evaluator You can also use an LLM to evaluate the agent's execution path with the `create\\_trajectory\\_llm\\_as\\_judge` function. Unlike the trajectory match evaluators, it doesn't require a reference trajectory, but one can be provided if available.  ```python theme={null} from langchain.agents import create\\_agent from langchain.tools import tool from langchain.messages import HumanMessage, AIMessage, ToolMessage from agentevals.trajectory.llm import create\\_trajectory\\_llm\\_as\\_judge, TRAJECTORY\\_ACCURACY\\_PROMPT @tool def get\\_weather(city: str): \"\"\"Get weather information for a city.\"\"\" return f\"It's 75 degrees and sunny in {city}.\" agent = create\\_agent(\"gpt-4o\", tools=[get\\_weather]) evaluator = create\\_trajectory\\_llm\\_as\\_judge( # [!code highlight] model=\"openai:o3-mini\", # [!code highlight] prompt=TRAJECTORY\\_ACCURACY\\_PROMPT, # [!code highlight] ) # [!code highlight] def test\\_trajectory\\_quality(): result = agent.invoke({ \"messages\": [HumanMessage(content=\"What's the weather in Seattle?\")] }) evaluation = evaluator( outputs=result[\"messages\"], ) # { # 'key': 'trajectory\\_accuracy', # 'score': True, # 'comment': 'The provided agent trajectory is reasonable...' # } assert evaluation[\"score\"] is True ```   If you have a reference trajectory, you can add an extra variable to your prompt and pass in the reference trajectory. Below, we use the prebuilt `TRAJECTORY\\_ACCURACY\\_PROMPT\\_WITH\\_REFERENCE` prompt and configure the `reference\\_outputs` variable: ```python theme={null} evaluator = create\\_trajectory\\_llm\\_as\\_judge( model=\"openai:o3-mini\", prompt=TRAJECTORY\\_ACCURACY\\_PROMPT\\_WITH\\_REFERENCE, ) evaluation = judge\\_with\\_reference( outputs=result[\"messages\"], reference\\_outputs=reference\\_trajectory, ) ```   For more configurability over how the LLM evaluates the trajectory, visit the [repository](https://github.com/langchain-ai/agentevals?tab=readme-ov-file#trajectory-llm-as-judge).  ### Async Support All `agentevals` evaluators support Python asyncio. For evaluators that use factory functions, async versions are available by adding `async` after `create\\_` in the function name.  ```python theme={null} from agentevals.trajectory.llm import create\\_async\\_trajectory\\_llm\\_as\\_judge, TRAJECTORY\\_ACCURACY\\_PROMPT from agentevals.trajectory.match import create\\_async\\_trajectory\\_match\\_evaluator async\\_judge = create\\_async\\_trajectory\\_llm\\_as\\_judge( model=\"openai:o3-mini\", prompt=TRAJECTORY\\_ACCURACY\\_PROMPT, ) async\\_evaluator = create\\_async\\_trajectory\\_match\\_evaluator( trajectory\\_match\\_mode=\"strict\", ) async def test\\_async\\_evaluation(): result = await agent.ainvoke({ \"messages\": [HumanMessage(content=\"What's the weather?\")] }) evaluation = await async\\_judge(outputs=result[\"messages\"]) assert evaluation[\"score\"] is True ```  ## LangSmith Integration For tracking experiments over time, you can log evaluator results to [LangSmith](https://smith.langchain.com/), a platform for building production-grade LLM applications that includes tracing, evaluation, and experimentation tools. First, set up LangSmith by setting the required environment variables: ```bash theme={null} export LANGSMITH\\_API\\_KEY=\"your\\_langsmith\\_api\\_key\" export LANGSMITH\\_TRACING=\"true\" ``` LangSmith offers two main approaches for running evaluations: [pytest](/langsmith/pytest) integration and the `evaluate` function.  ```python theme={null} import pytest from langsmith import testing as t from agentevals.trajectory.llm import create\\_trajectory\\_llm\\_as\\_judge, TRAJECTORY\\_ACCURACY\\_PROMPT trajectory\\_evaluator = create\\_trajectory\\_llm\\_as\\_judge( model=\"openai:o3-mini\", prompt=TRAJECTORY\\_ACCURACY\\_PROMPT, ) @pytest.mark.langsmith def test\\_trajectory\\_accuracy(): result = agent.invoke({ \"messages\": [HumanMessage(content=\"What's the weather in SF?\")] }) reference\\_trajectory = [ HumanMessage(content=\"What's the weather in SF?\"), AIMessage(content=\"\", tool\\_calls=[ {\"id\": \"call\\_1\", \"name\": \"get\\_weather\", \"args\": {\"city\": \"SF\"}}, ]), ToolMessage(content=\"It's 75 degrees and sunny in SF.\", tool\\_call\\_id=\"call\\_1\"), AIMessage(content=\"The weather in SF is 75 degrees and sunny.\"), ] # Log inputs, outputs, and reference outputs to LangSmith t.log\\_inputs({}) t.log\\_outputs({\"messages\": result[\"messages\"]}) t.log\\_reference\\_outputs({\"messages\": reference\\_trajectory}) trajectory\\_evaluator( outputs=result[\"messages\"], reference\\_outputs=reference\\_trajectory ) ``` Run the evaluation with pytest: ```bash theme={null} pytest test\\_trajectory.py --langsmith-output ``` Results will be automatically logged to LangSmith.   Alternatively, you can create a dataset in LangSmith and use the `evaluate` function: ```python theme={null} from langsmith import Client from agentevals.trajectory.llm import create\\_trajectory\\_llm\\_as\\_judge, TRAJECTORY\\_ACCURACY\\_PROMPT client = Client() trajectory\\_evaluator = create\\_trajectory\\_llm\\_as\\_judge( model=\"openai:o3-mini\", prompt=TRAJECTORY\\_ACCURACY\\_PROMPT, ) def run\\_agent(inputs): \"\"\"Your agent function that returns trajectory messages.\"\"\" return agent.invoke(inputs)[\"messages\"] experiment\\_results = client.evaluate( run\\_agent, data=\"your\\_dataset\\_name\", evaluators=[trajectory\\_evaluator] ) ``` Results will be automatically logged to LangSmith.   To learn more about evaluating your agent, see the [LangSmith docs](/langsmith/pytest).  ## Recording & Replaying HTTP Calls Integration tests that call real LLM APIs can be slow and expensive, especially when run frequently in CI/CD pipelines. We recommend using a library for recording HTTP requests and responses, then replaying them in subsequent runs without making actual network calls. You can use [`vcrpy`](https://pypi.org/project/vcrpy/1.5.2/) to achieve this. If you're using `pytest`, the [`pytest-recording` plugin](https://pypi.org/project/pytest-recording/) provides a simple way to enable this with minimal configuration. Request/responses are recorded in cassettes, which are then used to mock the real network calls in subsequent runs. Set up your `conftest.py` file to filter out sensitive information from the cassettes: ```py conftest.py theme={null} import pytest @pytest.fixture(scope=\"session\") def vcr\\_config(): return { \"filter\\_headers\": [ (\"authorization\", \"XXXX\"), (\"x-api-key\", \"XXXX\"), # ... other headers you want to mask ], \"filter\\_query\\_parameters\": [ (\"api\\_key\", \"XXXX\"), (\"key\", \"XXXX\"), ], } ``` Then configure your project to recognise the `vcr` marker:  ```ini pytest.ini theme={null} [pytest] markers = vcr: record/replay HTTP via VCR addopts = --record-mode=once ``` ```toml pyproject.toml theme={null} [tool.pytest.ini\\_options] markers = [ \"vcr: record/replay HTTP via VCR\" ] addopts = \"--record-mode=once\" ```   The `--record-mode=once` option records HTTP interactions on the first run and replays them on subsequent runs.  Now, simply decorate your tests with the `vcr` marker: ```python theme={null} @pytest.mark.vcr() def test\\_agent\\_trajectory(): # ... ``` The first time you run this test, your agent will make real network calls and pytest will generate a cassette file `test\\_agent\\_trajectory.yaml` in the `tests/cassettes` directory. Subsequent runs will use that cassette to mock the real network calls, granted the agent's requests don't change from the previous run. If they do, the test will fail and you'll need to delete the cassette and rerun the test to record fresh interactions.  When you modify prompts, add new tools, or change expected trajectories, your saved cassettes will become outdated and your existing tests \\*\\*will fail\\*\\*. You should delete the corresponding cassette files and rerun the tests to record fresh interactions.  \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/test.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/tools.md",
      "title": null,
      "raw_content": "# Tools Many AI applications interact with users via natural language. However, some use cases require models to interface directly with external systems\u2014such as APIs, databases, or file systems\u2014using structured input. Tools are components that [agents](/oss/python/langchain/agents) call to perform actions. They extend model capabilities by letting them interact with the world through well-defined inputs and outputs. Tools encapsulate a callable function and its input schema. These can be passed to compatible [chat models](/oss/python/langchain/models), allowing the model to decide whether to invoke a tool and with what arguments. In these scenarios, tool calling enables models to generate requests that conform to a specified input schema.  \\*\\*Server-side tool use\\*\\* Some chat models (e.g., [OpenAI](/oss/python/integrations/chat/openai), [Anthropic](/oss/python/integrations/chat/anthropic), and [Gemini](/oss/python/integrations/chat/google\\_generative\\_ai)) feature [built-in tools](/oss/python/langchain/models#server-side-tool-use) that are executed server-side, such as web search and code interpreters. Refer to the [provider overview](/oss/python/integrations/providers/overview) to learn how to access these tools with your specific chat model.  ## Create tools ### Basic tool definition The simplest way to create a tool is with the [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool) decorator. By default, the function's docstring becomes the tool's description that helps the model understand when to use it: ```python theme={null} from langchain.tools import tool @tool def search\\_database(query: str, limit: int = 10) -> str: \"\"\"Search the customer database for records matching the query. Args: query: Search terms to look for limit: Maximum number of results to return \"\"\" return f\"Found {limit} results for '{query}'\" ``` Type hints are \\*\\*required\\*\\* as they define the tool's input schema. The docstring should be informative and concise to help the model understand the tool's purpose. ### Customize tool properties #### Custom tool name By default, the tool name comes from the function name. Override it when you need something more descriptive: ```python theme={null} @tool(\"web\\_search\") # Custom name def search(query: str) -> str: \"\"\"Search the web for information.\"\"\" return f\"Results for: {query}\" print(search.name) # web\\_search ``` #### Custom tool description Override the auto-generated tool description for clearer model guidance: ```python theme={null} @tool(\"calculator\", description=\"Performs arithmetic calculations. Use this for any math problems.\") def calc(expression: str) -> str: \"\"\"Evaluate mathematical expressions.\"\"\" return str(eval(expression)) ``` ### Advanced schema definition Define complex inputs with Pydantic models or JSON schemas:  ```python Pydantic model theme={null} from pydantic import BaseModel, Field from typing import Literal class WeatherInput(BaseModel): \"\"\"Input for weather queries.\"\"\" location: str = Field(description=\"City name or coordinates\") units: Literal[\"celsius\", \"fahrenheit\"] = Field( default=\"celsius\", description=\"Temperature unit preference\" ) include\\_forecast: bool = Field( default=False, description=\"Include 5-day forecast\" ) @tool(args\\_schema=WeatherInput) def get\\_weather(location: str, units: str = \"celsius\", include\\_forecast: bool = False) -> str: \"\"\"Get current weather and optional forecast.\"\"\" temp = 22 if units == \"celsius\" else 72 result = f\"Current weather in {location}: {temp} degrees {units[0].upper()}\" if include\\_forecast: result += \"\\nNext 5 days: Sunny\" return result ``` ```python JSON Schema theme={null} weather\\_schema = { \"type\": \"object\", \"properties\": { \"location\": {\"type\": \"string\"}, \"units\": {\"type\": \"string\"}, \"include\\_forecast\": {\"type\": \"boolean\"} }, \"required\": [\"location\", \"units\", \"include\\_forecast\"] } @tool(args\\_schema=weather\\_schema) def get\\_weather(location: str, units: str = \"celsius\", include\\_forecast: bool = False) -> str: \"\"\"Get current weather and optional forecast.\"\"\" temp = 22 if units == \"celsius\" else 72 result = f\"Current weather in {location}: {temp} degrees {units[0].upper()}\" if include\\_forecast: result += \"\\nNext 5 days: Sunny\" return result ```  ### Reserved argument names The following parameter names are reserved and cannot be used as tool arguments. Using these names will cause runtime errors. | Parameter name | Purpose | | -------------- | ---------------------------------------------------------------------- | | `config` | Reserved for passing `RunnableConfig` to tools internally | | `runtime` | Reserved for `ToolRuntime` parameter (accessing state, context, store) | To access runtime information, use the [`ToolRuntime`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.ToolRuntime) parameter instead of naming your own arguments `config` or `runtime`. ## Accessing Context  \\*\\*Why this matters:\\*\\* Tools are most powerful when they can access agent state, runtime context, and long-term memory. This enables tools to make context-aware decisions, personalize responses, and maintain information across conversations. Runtime context provides a way to inject dependencies (like database connections, user IDs, or configuration) into your tools at runtime, making them more testable and reusable.  Tools can access runtime information through the `ToolRuntime` parameter, which provides: \\* \\*\\*State\\*\\* - Mutable data that flows through execution (e.g., messages, counters, custom fields) \\* \\*\\*Context\\*\\* - Immutable configuration like user IDs, session details, or application-specific configuration \\* \\*\\*Store\\*\\* - Persistent long-term memory across conversations \\* \\*\\*Stream Writer\\*\\* - Stream custom updates as tools execute \\* \\*\\*Config\\*\\* - `RunnableConfig` for the execution \\* \\*\\*Tool Call ID\\*\\* - ID of the current tool call ```mermaid theme={null} graph LR %% Runtime Context subgraph \"\ud83d\udd27 Tool Runtime Context\" A[Tool Call] --> B[ToolRuntime] B --> C[State Access] B --> D[Context Access] B --> E[Store Access] B --> F[Stream Writer] end %% Available Resources subgraph \"\ud83d\udcca Available Resources\" C --> G[Messages] C --> H[Custom State] D --> I[User ID] D --> J[Session Info] E --> K[Long-term Memory] E --> L[User Preferences] end %% Tool Capabilities subgraph \"\u26a1 Enhanced Tool Capabilities\" M[Context-Aware Tools] N[Stateful Tools] O[Memory-Enabled Tools] P[Streaming Tools] end %% Connections G --> M H --> N I --> M J --> M K --> O L --> O F --> P ``` ### `ToolRuntime` Use `ToolRuntime` to access all runtime information in a single parameter. Simply add `runtime: ToolRuntime` to your tool signature, and it will be automatically injected without being exposed to the LLM.  \\*\\*`ToolRuntime`\\*\\*: A unified parameter that provides tools access to state, context, store, streaming, config, and tool call ID. This replaces the older pattern of using separate [`InjectedState`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.tool\\_node.InjectedState), [`InjectedStore`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.tool\\_node.InjectedStore), [`get\\_runtime`](https://reference.langchain.com/python/langgraph/runtime/#langgraph.runtime.get\\_runtime), and [`InjectedToolCallId`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.InjectedToolCallId) annotations. The runtime automatically provides these capabilities to your tool functions without you having to pass them explicitly or use global state.  \\*\\*Accessing state:\\*\\* Tools can access the current graph state using `ToolRuntime`: ```python theme={null} from langchain.tools import tool, ToolRuntime # Access the current conversation state @tool def summarize\\_conversation( runtime: ToolRuntime ) -> str: \"\"\"Summarize the conversation so far.\"\"\" messages = runtime.state[\"messages\"] human\\_msgs = sum(1 for m in messages if m.\\_\\_class\\_\\_.\\_\\_name\\_\\_ == \"HumanMessage\") ai\\_msgs = sum(1 for m in messages if m.\\_\\_class\\_\\_.\\_\\_name\\_\\_ == \"AIMessage\") tool\\_msgs = sum(1 for m in messages if m.\\_\\_class\\_\\_.\\_\\_name\\_\\_ == \"ToolMessage\") return f\"Conversation has {human\\_msgs} user messages, {ai\\_msgs} AI responses, and {tool\\_msgs} tool results\" # Access custom state fields @tool def get\\_user\\_preference( pref\\_name: str, runtime: ToolRuntime # ToolRuntime parameter is not visible to the model ) -> str: \"\"\"Get a user preference value.\"\"\" preferences = runtime.state.get(\"user\\_preferences\", {}) return preferences.get(pref\\_name, \"Not set\") ```  The `runtime` parameter is hidden from the model. For the example above, the model only sees `pref\\_name` in the tool schema - `runtime` is \\*not\\* included in the request.  \\*\\*Updating state:\\*\\* Use [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) to update the agent's state or control the graph's execution flow: ```python theme={null} from langgraph.types import Command from langchain.messages import RemoveMessage from langgraph.graph.message import REMOVE\\_ALL\\_MESSAGES from langchain.tools import tool, ToolRuntime # Update the conversation history by removing all messages @tool def clear\\_conversation() -> Command: \"\"\"Clear the conversation history.\"\"\" return Command( update={ \"messages\": [RemoveMessage(id=REMOVE\\_ALL\\_MESSAGES)], } ) # Update the user\\_name in the agent state @tool def update\\_user\\_name( new\\_name: str, runtime: ToolRuntime ) -> Command: \"\"\"Update the user's name.\"\"\" return Command(update={\"user\\_name\": new\\_name}) ``` #### Context Access immutable configuration and contextual data like user IDs, session details, or application-specific configuration through `runtime.context`. Tools can access runtime context through `ToolRuntime`: ```python theme={null} from dataclasses import dataclass from langchain\\_openai import ChatOpenAI from langchain.agents import create\\_agent from langchain.tools import tool, ToolRuntime USER\\_DATABASE = { \"user123\": { \"name\": \"Alice Johnson\", \"account\\_type\": \"Premium\", \"balance\": 5000, \"email\": \"alice@example.com\" }, \"user456\": { \"name\": \"Bob Smith\", \"account\\_type\": \"Standard\", \"balance\": 1200, \"email\": \"bob@example.com\" } } @dataclass class UserContext: user\\_id: str @tool def get\\_account\\_info(runtime: ToolRuntime[UserContext]) -> str: \"\"\"Get the current user's account information.\"\"\" user\\_id = runtime.context.user\\_id if user\\_id in USER\\_DATABASE: user = USER\\_DATABASE[user\\_id] return f\"Account holder: {user['name']}\\nType: {user['account\\_type']}\\nBalance: ${user['balance']}\" return \"User not found\" model = ChatOpenAI(model=\"gpt-4o\") agent = create\\_agent( model, tools=[get\\_account\\_info], context\\_schema=UserContext, system\\_prompt=\"You are a financial assistant.\" ) result = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"What's my current balance?\"}]}, context=UserContext(user\\_id=\"user123\") ) ``` #### Memory (Store) Access persistent data across conversations using the store. The store is accessed via `runtime.store` and allows you to save and retrieve user-specific or application-specific data. Tools can access and update the store through `ToolRuntime`: ```python expandable theme={null} from typing import Any from langgraph.store.memory import InMemoryStore from langchain.agents import create\\_agent from langchain.tools import tool, ToolRuntime # Access memory @tool def get\\_user\\_info(user\\_id: str, runtime: ToolRuntime) -> str: \"\"\"Look up user info.\"\"\" store = runtime.store user\\_info = store.get((\"users\",), user\\_id) return str(user\\_info.value) if user\\_info else \"Unknown user\" # Update memory @tool def save\\_user\\_info(user\\_id: str, user\\_info: dict[str, Any], runtime: ToolRuntime) -> str: \"\"\"Save user info.\"\"\" store = runtime.store store.put((\"users\",), user\\_id, user\\_info) return \"Successfully saved user info.\" store = InMemoryStore() agent = create\\_agent( model, tools=[get\\_user\\_info, save\\_user\\_info], store=store ) # First session: save user info agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"Save the following user: userid: abc123, name: Foo, age: 25, email: foo@langchain.dev\"}] }) # Second session: get user info agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"Get user info for user with id 'abc123'\"}] }) # Here is the user info for user with ID \"abc123\": # - Name: Foo # - Age: 25 # - Email: foo@langchain.dev ``` #### Stream Writer Stream custom updates from tools as they execute using `runtime.stream\\_writer`. This is useful for providing real-time feedback to users about what a tool is doing. ```python theme={null} from langchain.tools import tool, ToolRuntime @tool def get\\_weather(city: str, runtime: ToolRuntime) -> str: \"\"\"Get weather for a given city.\"\"\" writer = runtime.stream\\_writer # Stream custom updates as the tool executes writer(f\"Looking up data for city: {city}\") writer(f\"Acquired data for city: {city}\") return f\"It's always sunny in {city}!\" ```  If you use `runtime.stream\\_writer` inside your tool, the tool must be invoked within a LangGraph execution context. See [Streaming](/oss/python/langchain/streaming) for more details.  \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/tools.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langchain/ui.md",
      "title": null,
      "raw_content": "# Agent Chat UI LangChain provides a powerful prebuilt user interface that work seamlessly with agents created using [`create\\_agent`](/oss/python/langchain/agents). This UI is designed to provide rich, interactive experiences for your agents with minimal setup, whether you're running locally or in a deployed context (such as [LangSmith](/langsmith/)). ## Agent Chat UI [Agent Chat UI](https://github.com/langchain-ai/agent-chat-ui) is a Next.js application that provides a conversational interface for interacting with any LangChain agent. It supports real-time chat, tool visualization, and advanced features like time-travel debugging and state forking. Agent Chat UI is open source and can be adapted to your application needs. ### Features  Studio automatically renders tool calls and results in an intuitive interface.   Navigate through conversation history and fork from any point   View and modify agent state at any point during execution   Built-in support for reviewing and responding to agent requests   You can use generative UI in the Agent Chat UI. For more information, see [Implement generative user interfaces with LangGraph](/langsmith/generative-ui-react).  ### Quick start The fastest way to get started is using the hosted version: 1. \\*\\*Visit [Agent Chat UI](https://agentchat.vercel.app)\\*\\* 2. \\*\\*Connect your agent\\*\\* by entering your deployment URL or local server address 3. \\*\\*Start chatting\\*\\* - the UI will automatically detect and render tool calls and interrupts ### Local development For customization or local development, you can run Agent Chat UI locally:  ```bash Use npx theme={null} # Create a new Agent Chat UI project npx create-agent-chat-app --project-name my-chat-ui cd my-chat-ui # Install dependencies and start pnpm install pnpm dev ``` ```bash Clone repository theme={null} # Clone the repository git clone https://github.com/langchain-ai/agent-chat-ui.git cd agent-chat-ui # Install dependencies and start pnpm install pnpm dev ```  ### Connect to your agent Agent Chat UI can connect to both [local](/oss/python/langchain/studio#setup-local-agent-server) and [deployed agents](/oss/python/langchain/deploy). After starting Agent Chat UI, you'll need to configure it to connect to your agent: 1. \\*\\*Graph ID\\*\\*: Enter your graph name (find this under `graphs` in your `langgraph.json` file) 2. \\*\\*Deployment URL\\*\\*: Your Agent server's endpoint (e.g., `http://localhost:2024` for local development, or your deployed agent's URL) 3. \\*\\*LangSmith API key (optional)\\*\\*: Add your LangSmith API key (not required if you're using a local Agent server) Once configured, Agent Chat UI will automatically fetch and display any interrupted threads from your agent.  Agent Chat UI has out-of-the-box support for rendering tool calls and tool result messages. To customize what messages are shown, see [Hiding Messages in the Chat](https://github.com/langchain-ai/agent-chat-ui?tab=readme-ov-file#hiding-messages-in-the-chat).  \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langchain/ui.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/add-memory.md",
      "title": null,
      "raw_content": "# Memory AI applications need [memory](/oss/python/concepts/memory) to share context across multiple interactions. In LangGraph, you can add two types of memory: \\* [Add short-term memory](#add-short-term-memory) as a part of your agent's [state](/oss/python/langgraph/graph-api#state) to enable multi-turn conversations. \\* [Add long-term memory](#add-long-term-memory) to store user-specific or application-level data across sessions. ## Add short-term memory \\*\\*Short-term\\*\\* memory (thread-level [persistence](/oss/python/langgraph/persistence)) enables agents to track multi-turn conversations. To add short-term memory: ```python theme={null} from langgraph.checkpoint.memory import InMemorySaver # [!code highlight] from langgraph.graph import StateGraph checkpointer = InMemorySaver() # [!code highlight] builder = StateGraph(...) graph = builder.compile(checkpointer=checkpointer) # [!code highlight] graph.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"hi! i am Bob\"}]}, {\"configurable\": {\"thread\\_id\": \"1\"}}, # [!code highlight] ) ``` ### Use in production In production, use a checkpointer backed by a database: ```python theme={null} from langgraph.checkpoint.postgres import PostgresSaver DB\\_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\" with PostgresSaver.from\\_conn\\_string(DB\\_URI) as checkpointer: # [!code highlight] builder = StateGraph(...) graph = builder.compile(checkpointer=checkpointer) # [!code highlight] ```  ``` pip install -U \"psycopg[binary,pool]\" langgraph langgraph-checkpoint-postgres ```  You need to call `checkpointer.setup()` the first time you're using Postgres checkpointer    ```python theme={null} from langchain.chat\\_models import init\\_chat\\_model from langgraph.graph import StateGraph, MessagesState, START from langgraph.checkpoint.postgres import PostgresSaver # [!code highlight] model = init\\_chat\\_model(model=\"claude-haiku-4-5-20251001\") DB\\_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\" with PostgresSaver.from\\_conn\\_string(DB\\_URI) as checkpointer: # [!code highlight] # checkpointer.setup() def call\\_model(state: MessagesState): response = model.invoke(state[\"messages\"]) return {\"messages\": response} builder = StateGraph(MessagesState) builder.add\\_node(call\\_model) builder.add\\_edge(START, \"call\\_model\") graph = builder.compile(checkpointer=checkpointer) # [!code highlight] config = { \"configurable\": { \"thread\\_id\": \"1\" # [!code highlight] } } for chunk in graph.stream( {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]}, config, # [!code highlight] stream\\_mode=\"values\" ): chunk[\"messages\"][-1].pretty\\_print() for chunk in graph.stream( {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]}, config, # [!code highlight] stream\\_mode=\"values\" ): chunk[\"messages\"][-1].pretty\\_print() ```   ```python theme={null} from langchain.chat\\_models import init\\_chat\\_model from langgraph.graph import StateGraph, MessagesState, START from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver # [!code highlight] model = init\\_chat\\_model(model=\"claude-haiku-4-5-20251001\") DB\\_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\" async with AsyncPostgresSaver.from\\_conn\\_string(DB\\_URI) as checkpointer: # [!code highlight] # await checkpointer.setup() async def call\\_model(state: MessagesState): response = await model.ainvoke(state[\"messages\"]) return {\"messages\": response} builder = StateGraph(MessagesState) builder.add\\_node(call\\_model) builder.add\\_edge(START, \"call\\_model\") graph = builder.compile(checkpointer=checkpointer) # [!code highlight] config = { \"configurable\": { \"thread\\_id\": \"1\" # [!code highlight] } } async for chunk in graph.astream( {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]}, config, # [!code highlight] stream\\_mode=\"values\" ): chunk[\"messages\"][-1].pretty\\_print() async for chunk in graph.astream( {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]}, config, # [!code highlight] stream\\_mode=\"values\" ): chunk[\"messages\"][-1].pretty\\_print() ```     ``` pip install -U pymongo langgraph langgraph-checkpoint-mongodb ```  \\*\\*Setup\\*\\* To use the MongoDB checkpointer, you will need a MongoDB cluster. Follow [this guide](https://www.mongodb.com/docs/guides/atlas/cluster/) to create a cluster if you don't already have one.    ```python theme={null} from langchain.chat\\_models import init\\_chat\\_model from langgraph.graph import StateGraph, MessagesState, START from langgraph.checkpoint.mongodb import MongoDBSaver # [!code highlight] model = init\\_chat\\_model(model=\"claude-haiku-4-5-20251001\") DB\\_URI = \"localhost:27017\" with MongoDBSaver.from\\_conn\\_string(DB\\_URI) as checkpointer: # [!code highlight] def call\\_model(state: MessagesState): response = model.invoke(state[\"messages\"]) return {\"messages\": response} builder = StateGraph(MessagesState) builder.add\\_node(call\\_model) builder.add\\_edge(START, \"call\\_model\") graph = builder.compile(checkpointer=checkpointer) # [!code highlight] config = { \"configurable\": { \"thread\\_id\": \"1\" # [!code highlight] } } for chunk in graph.stream( {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]}, config, # [!code highlight] stream\\_mode=\"values\" ): chunk[\"messages\"][-1].pretty\\_print() for chunk in graph.stream( {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]}, config, # [!code highlight] stream\\_mode=\"values\" ): chunk[\"messages\"][-1].pretty\\_print() ```   ```python theme={null} from langchain.chat\\_models import init\\_chat\\_model from langgraph.graph import StateGraph, MessagesState, START from langgraph.checkpoint.mongodb.aio import AsyncMongoDBSaver # [!code highlight] model = init\\_chat\\_model(model=\"claude-haiku-4-5-20251001\") DB\\_URI = \"localhost:27017\" async with AsyncMongoDBSaver.from\\_conn\\_string(DB\\_URI) as checkpointer: # [!code highlight] async def call\\_model(state: MessagesState): response = await model.ainvoke(state[\"messages\"]) return {\"messages\": response} builder = StateGraph(MessagesState) builder.add\\_node(call\\_model) builder.add\\_edge(START, \"call\\_model\") graph = builder.compile(checkpointer=checkpointer) # [!code highlight] config = { \"configurable\": { \"thread\\_id\": \"1\" # [!code highlight] } } async for chunk in graph.astream( {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]}, config, # [!code highlight] stream\\_mode=\"values\" ): chunk[\"messages\"][-1].pretty\\_print() async for chunk in graph.astream( {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]}, config, # [!code highlight] stream\\_mode=\"values\" ): chunk[\"messages\"][-1].pretty\\_print() ```     ``` pip install -U langgraph langgraph-checkpoint-redis ```  You need to call `checkpointer.setup()` the first time you're using Redis checkpointer.    ```python theme={null} from langchain.chat\\_models import init\\_chat\\_model from langgraph.graph import StateGraph, MessagesState, START from langgraph.checkpoint.redis import RedisSaver # [!code highlight] model = init\\_chat\\_model(model=\"claude-haiku-4-5-20251001\") DB\\_URI = \"redis://localhost:6379\" with RedisSaver.from\\_conn\\_string(DB\\_URI) as checkpointer: # [!code highlight] # checkpointer.setup() def call\\_model(state: MessagesState): response = model.invoke(state[\"messages\"]) return {\"messages\": response} builder = StateGraph(MessagesState) builder.add\\_node(call\\_model) builder.add\\_edge(START, \"call\\_model\") graph = builder.compile(checkpointer=checkpointer) # [!code highlight] config = { \"configurable\": { \"thread\\_id\": \"1\" # [!code highlight] } } for chunk in graph.stream( {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]}, config, # [!code highlight] stream\\_mode=\"values\" ): chunk[\"messages\"][-1].pretty\\_print() for chunk in graph.stream( {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]}, config, # [!code highlight] stream\\_mode=\"values\" ): chunk[\"messages\"][-1].pretty\\_print() ```   ```python theme={null} from langchain.chat\\_models import init\\_chat\\_model from langgraph.graph import StateGraph, MessagesState, START from langgraph.checkpoint.redis.aio import AsyncRedisSaver # [!code highlight] model = init\\_chat\\_model(model=\"claude-haiku-4-5-20251001\") DB\\_URI = \"redis://localhost:6379\" async with AsyncRedisSaver.from\\_conn\\_string(DB\\_URI) as checkpointer: # [!code highlight] # await checkpointer.asetup() async def call\\_model(state: MessagesState): response = await model.ainvoke(state[\"messages\"]) return {\"messages\": response} builder = StateGraph(MessagesState) builder.add\\_node(call\\_model) builder.add\\_edge(START, \"call\\_model\") graph = builder.compile(checkpointer=checkpointer) # [!code highlight] config = { \"configurable\": { \"thread\\_id\": \"1\" # [!code highlight] } } async for chunk in graph.astream( {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]}, config, # [!code highlight] stream\\_mode=\"values\" ): chunk[\"messages\"][-1].pretty\\_print() async for chunk in graph.astream( {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]}, config, # [!code highlight] stream\\_mode=\"values\" ): chunk[\"messages\"][-1].pretty\\_print() ```    ### Use in subgraphs If your graph contains [subgraphs](/oss/python/langgraph/use-subgraphs), you only need to provide the checkpointer when compiling the parent graph. LangGraph will automatically propagate the checkpointer to the child subgraphs. ```python theme={null} from langgraph.graph import START, StateGraph from langgraph.checkpoint.memory import InMemorySaver from typing import TypedDict class State(TypedDict): foo: str # Subgraph def subgraph\\_node\\_1(state: State): return {\"foo\": state[\"foo\"] + \"bar\"} subgraph\\_builder = StateGraph(State) subgraph\\_builder.add\\_node(subgraph\\_node\\_1) subgraph\\_builder.add\\_edge(START, \"subgraph\\_node\\_1\") subgraph = subgraph\\_builder.compile() # [!code highlight] # Parent graph builder = StateGraph(State) builder.add\\_node(\"node\\_1\", subgraph) # [!code highlight] builder.add\\_edge(START, \"node\\_1\") checkpointer = InMemorySaver() graph = builder.compile(checkpointer=checkpointer) # [!code highlight] ``` If you want the subgraph to have its own memory, you can compile it with the appropriate checkpointer option. This is useful in [multi-agent](/oss/python/langchain/multi-agent) systems, if you want agents to keep track of their internal message histories. ```python theme={null} subgraph\\_builder = StateGraph(...) subgraph = subgraph\\_builder.compile(checkpointer=True) # [!code highlight] ``` ## Add long-term memory Use long-term memory to store user-specific or application-specific data across conversations. ```python theme={null} from langgraph.store.memory import InMemoryStore # [!code highlight] from langgraph.graph import StateGraph store = InMemoryStore() # [!code highlight] builder = StateGraph(...) graph = builder.compile(store=store) # [!code highlight] ``` ### Use in production In production, use a store backed by a database: ```python theme={null} from langgraph.store.postgres import PostgresStore DB\\_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\" with PostgresStore.from\\_conn\\_string(DB\\_URI) as store: # [!code highlight] builder = StateGraph(...) graph = builder.compile(store=store) # [!code highlight] ```  ``` pip install -U \"psycopg[binary,pool]\" langgraph langgraph-checkpoint-postgres ```  You need to call `store.setup()` the first time you're using Postgres store    ```python theme={null} from langchain\\_core.runnables import RunnableConfig from langchain.chat\\_models import init\\_chat\\_model from langgraph.graph import StateGraph, MessagesState, START from langgraph.checkpoint.postgres import PostgresSaver from langgraph.store.postgres import PostgresStore # [!code highlight] from langgraph.store.base import BaseStore model = init\\_chat\\_model(model=\"claude-haiku-4-5-20251001\") DB\\_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\" with ( PostgresStore.from\\_conn\\_string(DB\\_URI) as store, # [!code highlight] PostgresSaver.from\\_conn\\_string(DB\\_URI) as checkpointer, ): # store.setup() # checkpointer.setup() def call\\_model( state: MessagesState, config: RunnableConfig, \\*, store: BaseStore, # [!code highlight] ): user\\_id = config[\"configurable\"][\"user\\_id\"] namespace = (\"memories\", user\\_id) memories = store.search(namespace, query=str(state[\"messages\"][-1].content)) # [!code highlight] info = \"\\n\".join([d.value[\"data\"] for d in memories]) system\\_msg = f\"You are a helpful assistant talking to the user. User info: {info}\" # Store new memories if the user asks the model to remember last\\_message = state[\"messages\"][-1] if \"remember\" in last\\_message.content.lower(): memory = \"User name is Bob\" store.put(namespace, str(uuid.uuid4()), {\"data\": memory}) # [!code highlight] response = model.invoke( [{\"role\": \"system\", \"content\": system\\_msg}] + state[\"messages\"] ) return {\"messages\": response} builder = StateGraph(MessagesState) builder.add\\_node(call\\_model) builder.add\\_edge(START, \"call\\_model\") graph = builder.compile( checkpointer=checkpointer, store=store, # [!code highlight] ) config = { \"configurable\": { \"thread\\_id\": \"1\", # [!code highlight] \"user\\_id\": \"1\", # [!code highlight] } } for chunk in graph.stream( {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}]}, config, # [!code highlight] stream\\_mode=\"values\", ): chunk[\"messages\"][-1].pretty\\_print() config = { \"configurable\": { \"thread\\_id\": \"2\", # [!code highlight] \"user\\_id\": \"1\", } } for chunk in graph.stream( {\"messages\": [{\"role\": \"user\", \"content\": \"what is my name?\"}]}, config, # [!code highlight] stream\\_mode=\"values\", ): chunk[\"messages\"][-1].pretty\\_print() ```   ```python theme={null} from langchain\\_core.runnables import RunnableConfig from langchain.chat\\_models import init\\_chat\\_model from langgraph.graph import StateGraph, MessagesState, START from langgraph.checkpoint.postgres.aio import AsyncPostgresSaver from langgraph.store.postgres.aio import AsyncPostgresStore # [!code highlight] from langgraph.store.base import BaseStore model = init\\_chat\\_model(model=\"claude-haiku-4-5-20251001\") DB\\_URI = \"postgresql://postgres:postgres@localhost:5442/postgres?sslmode=disable\" async with ( AsyncPostgresStore.from\\_conn\\_string(DB\\_URI) as store, # [!code highlight] AsyncPostgresSaver.from\\_conn\\_string(DB\\_URI) as checkpointer, ): # await store.setup() # await checkpointer.setup() async def call\\_model( state: MessagesState, config: RunnableConfig, \\*, store: BaseStore, # [!code highlight] ): user\\_id = config[\"configurable\"][\"user\\_id\"] namespace = (\"memories\", user\\_id) memories = await store.asearch(namespace, query=str(state[\"messages\"][-1].content)) # [!code highlight] info = \"\\n\".join([d.value[\"data\"] for d in memories]) system\\_msg = f\"You are a helpful assistant talking to the user. User info: {info}\" # Store new memories if the user asks the model to remember last\\_message = state[\"messages\"][-1] if \"remember\" in last\\_message.content.lower(): memory = \"User name is Bob\" await store.aput(namespace, str(uuid.uuid4()), {\"data\": memory}) # [!code highlight] response = await model.ainvoke( [{\"role\": \"system\", \"content\": system\\_msg}] + state[\"messages\"] ) return {\"messages\": response} builder = StateGraph(MessagesState) builder.add\\_node(call\\_model) builder.add\\_edge(START, \"call\\_model\") graph = builder.compile( checkpointer=checkpointer, store=store, # [!code highlight] ) config = { \"configurable\": { \"thread\\_id\": \"1\", # [!code highlight] \"user\\_id\": \"1\", # [!code highlight] } } async for chunk in graph.astream( {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}]}, config, # [!code highlight] stream\\_mode=\"values\", ): chunk[\"messages\"][-1].pretty\\_print() config = { \"configurable\": { \"thread\\_id\": \"2\", # [!code highlight] \"user\\_id\": \"1\", } } async for chunk in graph.astream( {\"messages\": [{\"role\": \"user\", \"content\": \"what is my name?\"}]}, config, # [!code highlight] stream\\_mode=\"values\", ): chunk[\"messages\"][-1].pretty\\_print() ```     ``` pip install -U langgraph langgraph-checkpoint-redis ```  You need to call `store.setup()` the first time you're using [Redis store](https://pypi.org/project/langgraph-checkpoint-redis/).    ```python theme={null} from langchain\\_core.runnables import RunnableConfig from langchain.chat\\_models import init\\_chat\\_model from langgraph.graph import StateGraph, MessagesState, START from langgraph.checkpoint.redis import RedisSaver from langgraph.store.redis import RedisStore # [!code highlight] from langgraph.store.base import BaseStore model = init\\_chat\\_model(model=\"claude-haiku-4-5-20251001\") DB\\_URI = \"redis://localhost:6379\" with ( RedisStore.from\\_conn\\_string(DB\\_URI) as store, # [!code highlight] RedisSaver.from\\_conn\\_string(DB\\_URI) as checkpointer, ): store.setup() checkpointer.setup() def call\\_model( state: MessagesState, config: RunnableConfig, \\*, store: BaseStore, # [!code highlight] ): user\\_id = config[\"configurable\"][\"user\\_id\"] namespace = (\"memories\", user\\_id) memories = store.search(namespace, query=str(state[\"messages\"][-1].content)) # [!code highlight] info = \"\\n\".join([d.value[\"data\"] for d in memories]) system\\_msg = f\"You are a helpful assistant talking to the user. User info: {info}\" # Store new memories if the user asks the model to remember last\\_message = state[\"messages\"][-1] if \"remember\" in last\\_message.content.lower(): memory = \"User name is Bob\" store.put(namespace, str(uuid.uuid4()), {\"data\": memory}) # [!code highlight] response = model.invoke( [{\"role\": \"system\", \"content\": system\\_msg}] + state[\"messages\"] ) return {\"messages\": response} builder = StateGraph(MessagesState) builder.add\\_node(call\\_model) builder.add\\_edge(START, \"call\\_model\") graph = builder.compile( checkpointer=checkpointer, store=store, # [!code highlight] ) config = { \"configurable\": { \"thread\\_id\": \"1\", # [!code highlight] \"user\\_id\": \"1\", # [!code highlight] } } for chunk in graph.stream( {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}]}, config, # [!code highlight] stream\\_mode=\"values\", ): chunk[\"messages\"][-1].pretty\\_print() config = { \"configurable\": { \"thread\\_id\": \"2\", # [!code highlight] \"user\\_id\": \"1\", } } for chunk in graph.stream( {\"messages\": [{\"role\": \"user\", \"content\": \"what is my name?\"}]}, config, # [!code highlight] stream\\_mode=\"values\", ): chunk[\"messages\"][-1].pretty\\_print() ```   ```python theme={null} from langchain\\_core.runnables import RunnableConfig from langchain.chat\\_models import init\\_chat\\_model from langgraph.graph import StateGraph, MessagesState, START from langgraph.checkpoint.redis.aio import AsyncRedisSaver from langgraph.store.redis.aio import AsyncRedisStore # [!code highlight] from langgraph.store.base import BaseStore model = init\\_chat\\_model(model=\"claude-haiku-4-5-20251001\") DB\\_URI = \"redis://localhost:6379\" async with ( AsyncRedisStore.from\\_conn\\_string(DB\\_URI) as store, # [!code highlight] AsyncRedisSaver.from\\_conn\\_string(DB\\_URI) as checkpointer, ): # await store.setup() # await checkpointer.asetup() async def call\\_model( state: MessagesState, config: RunnableConfig, \\*, store: BaseStore, # [!code highlight] ): user\\_id = config[\"configurable\"][\"user\\_id\"] namespace = (\"memories\", user\\_id) memories = await store.asearch(namespace, query=str(state[\"messages\"][-1].content)) # [!code highlight] info = \"\\n\".join([d.value[\"data\"] for d in memories]) system\\_msg = f\"You are a helpful assistant talking to the user. User info: {info}\" # Store new memories if the user asks the model to remember last\\_message = state[\"messages\"][-1] if \"remember\" in last\\_message.content.lower(): memory = \"User name is Bob\" await store.aput(namespace, str(uuid.uuid4()), {\"data\": memory}) # [!code highlight] response = await model.ainvoke( [{\"role\": \"system\", \"content\": system\\_msg}] + state[\"messages\"] ) return {\"messages\": response} builder = StateGraph(MessagesState) builder.add\\_node(call\\_model) builder.add\\_edge(START, \"call\\_model\") graph = builder.compile( checkpointer=checkpointer, store=store, # [!code highlight] ) config = { \"configurable\": { \"thread\\_id\": \"1\", # [!code highlight] \"user\\_id\": \"1\", # [!code highlight] } } async for chunk in graph.astream( {\"messages\": [{\"role\": \"user\", \"content\": \"Hi! Remember: my name is Bob\"}]}, config, # [!code highlight] stream\\_mode=\"values\", ): chunk[\"messages\"][-1].pretty\\_print() config = { \"configurable\": { \"thread\\_id\": \"2\", # [!code highlight] \"user\\_id\": \"1\", } } async for chunk in graph.astream( {\"messages\": [{\"role\": \"user\", \"content\": \"what is my name?\"}]}, config, # [!code highlight] stream\\_mode=\"values\", ): chunk[\"messages\"][-1].pretty\\_print() ```    ### Use semantic search Enable semantic search in your graph's memory store to let graph agents search for items in the store by semantic similarity. ```python theme={null} from langchain.embeddings import init\\_embeddings from langgraph.store.memory import InMemoryStore # Create store with semantic search enabled embeddings = init\\_embeddings(\"openai:text-embedding-3-small\") store = InMemoryStore( index={ \"embed\": embeddings, \"dims\": 1536, } ) store.put((\"user\\_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"}) store.put((\"user\\_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber\"}) items = store.search( (\"user\\_123\", \"memories\"), query=\"I'm hungry\", limit=1 ) ```  ```python theme={null} from langchain.embeddings import init\\_embeddings from langchain.chat\\_models import init\\_chat\\_model from langgraph.store.base import BaseStore from langgraph.store.memory import InMemoryStore from langgraph.graph import START, MessagesState, StateGraph model = init\\_chat\\_model(\"gpt-4o-mini\") # Create store with semantic search enabled embeddings = init\\_embeddings(\"openai:text-embedding-3-small\") store = InMemoryStore( index={ \"embed\": embeddings, \"dims\": 1536, } ) store.put((\"user\\_123\", \"memories\"), \"1\", {\"text\": \"I love pizza\"}) store.put((\"user\\_123\", \"memories\"), \"2\", {\"text\": \"I am a plumber\"}) def chat(state, \\*, store: BaseStore): # Search based on user's last message items = store.search( (\"user\\_123\", \"memories\"), query=state[\"messages\"][-1].content, limit=2 ) memories = \"\\n\".join(item.value[\"text\"] for item in items) memories = f\"## Memories of user\\n{memories}\" if memories else \"\" response = model.invoke( [ {\"role\": \"system\", \"content\": f\"You are a helpful assistant.\\n{memories}\"}, \\*state[\"messages\"], ] ) return {\"messages\": [response]} builder = StateGraph(MessagesState) builder.add\\_node(chat) builder.add\\_edge(START, \"chat\") graph = builder.compile(store=store) for message, metadata in graph.stream( input={\"messages\": [{\"role\": \"user\", \"content\": \"I'm hungry\"}]}, stream\\_mode=\"messages\", ): print(message.content, end=\"\") ```  ## Manage short-term memory With [short-term memory](#add-short-term-memory) enabled, long conversations can exceed the LLM's context window. Common solutions are: \\* [Trim messages](#trim-messages): Remove first or last N messages (before calling LLM) \\* [Delete messages](#delete-messages) from LangGraph state permanently \\* [Summarize messages](#summarize-messages): Summarize earlier messages in the history and replace them with a summary \\* [Manage checkpoints](#manage-checkpoints) to store and retrieve message history \\* Custom strategies (e.g., message filtering, etc.) This allows the agent to keep track of the conversation without exceeding the LLM's context window. ### Trim messages Most LLMs have a maximum supported context window (denominated in tokens). One way to decide when to truncate messages is to count the tokens in the message history and truncate whenever it approaches that limit. If you're using LangChain, you can use the trim messages utility and specify the number of tokens to keep from the list, as well as the `strategy` (e.g., keep the last `max\\_tokens`) to use for handling the boundary. To trim message history, use the [`trim\\_messages`](https://python.langchain.com/api\\_reference/core/messages/langchain\\_core.messages.utils.trim\\_messages.html) function: ```python theme={null} from langchain\\_core.messages.utils import ( # [!code highlight] trim\\_messages, # [!code highlight] count\\_tokens\\_approximately # [!code highlight] ) # [!code highlight] def call\\_model(state: MessagesState): messages = trim\\_messages( # [!code highlight] state[\"messages\"], strategy=\"last\", token\\_counter=count\\_tokens\\_approximately, max\\_tokens=128, start\\_on=\"human\", end\\_on=(\"human\", \"tool\"), ) response = model.invoke(messages) return {\"messages\": [response]} builder = StateGraph(MessagesState) builder.add\\_node(call\\_model) ... ```  ```python theme={null} from langchain\\_core.messages.utils import ( trim\\_messages, # [!code highlight] count\\_tokens\\_approximately # [!code highlight] ) from langchain.chat\\_models import init\\_chat\\_model from langgraph.graph import StateGraph, START, MessagesState model = init\\_chat\\_model(\"claude-sonnet-4-5-20250929\") summarization\\_model = model.bind(max\\_tokens=128) def call\\_model(state: MessagesState): messages = trim\\_messages( # [!code highlight] state[\"messages\"], strategy=\"last\", token\\_counter=count\\_tokens\\_approximately, max\\_tokens=128, start\\_on=\"human\", end\\_on=(\"human\", \"tool\"), ) response = model.invoke(messages) return {\"messages\": [response]} checkpointer = InMemorySaver() builder = StateGraph(MessagesState) builder.add\\_node(call\\_model) builder.add\\_edge(START, \"call\\_model\") graph = builder.compile(checkpointer=checkpointer) config = {\"configurable\": {\"thread\\_id\": \"1\"}} graph.invoke({\"messages\": \"hi, my name is bob\"}, config) graph.invoke({\"messages\": \"write a short poem about cats\"}, config) graph.invoke({\"messages\": \"now do the same but for dogs\"}, config) final\\_response = graph.invoke({\"messages\": \"what's my name?\"}, config) final\\_response[\"messages\"][-1].pretty\\_print() ``` ``` ================================== Ai Message ================================== Your name is Bob, as you mentioned when you first introduced yourself. ```  ### Delete messages You can delete messages from the graph state to manage the message history. This is useful when you want to remove specific messages or clear the entire message history. To delete messages from the graph state, you can use the `RemoveMessage`. For `RemoveMessage` to work, you need to use a state key with [`add\\_messages`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.message.add\\_messages) [reducer](/oss/python/langgraph/graph-api#reducers), like [`MessagesState`](/oss/python/langgraph/graph-api#messagesstate). To remove specific messages: ```python theme={null} from langchain.messages import RemoveMessage # [!code highlight] def delete\\_messages(state): messages = state[\"messages\"] if len(messages) > 2: # remove the earliest two messages return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]} # [!code highlight] ``` To remove \\*\\*all\\*\\* messages: ```python theme={null} from langgraph.graph.message import REMOVE\\_ALL\\_MESSAGES # [!code highlight] def delete\\_messages(state): return {\"messages\": [RemoveMessage(id=REMOVE\\_ALL\\_MESSAGES)]} # [!code highlight] ```  When deleting messages, \\*\\*make sure\\*\\* that the resulting message history is valid. Check the limitations of the LLM provider you're using. For example: \\* Some providers expect message history to start with a `user` message \\* Most providers require `assistant` messages with tool calls to be followed by corresponding `tool` result messages.   ```python theme={null} from langchain.messages import RemoveMessage # [!code highlight] def delete\\_messages(state): messages = state[\"messages\"] if len(messages) > 2: # remove the earliest two messages return {\"messages\": [RemoveMessage(id=m.id) for m in messages[:2]]} # [!code highlight] def call\\_model(state: MessagesState): response = model.invoke(state[\"messages\"]) return {\"messages\": response} builder = StateGraph(MessagesState) builder.add\\_sequence([call\\_model, delete\\_messages]) builder.add\\_edge(START, \"call\\_model\") checkpointer = InMemorySaver() app = builder.compile(checkpointer=checkpointer) for event in app.stream( {\"messages\": [{\"role\": \"user\", \"content\": \"hi! I'm bob\"}]}, config, stream\\_mode=\"values\" ): print([(message.type, message.content) for message in event[\"messages\"]]) for event in app.stream( {\"messages\": [{\"role\": \"user\", \"content\": \"what's my name?\"}]}, config, stream\\_mode=\"values\" ): print([(message.type, message.content) for message in event[\"messages\"]]) ``` ``` [('human', \"hi! I'm bob\")] [('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?')] [('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', \"what's my name?\")] [('human', \"hi! I'm bob\"), ('ai', 'Hi Bob! How are you doing today? Is there anything I can help you with?'), ('human', \"what's my name?\"), ('ai', 'Your name is Bob.')] [('human', \"what's my name?\"), ('ai', 'Your name is Bob.')] ```  ### Summarize messages The problem with trimming or removing messages, as shown above, is that you may lose information from culling of the message queue. Because of this, some applications benefit from a more sophisticated approach of summarizing the message history using a chat model. Prompting and orchestration logic can be used to summarize the message history. For example, in LangGraph you can extend the [`MessagesState`](/oss/python/langgraph/graph-api#working-with-messages-in-graph-state) to include a `summary` key: ```python theme={null} from langgraph.graph import MessagesState class State(MessagesState): summary: str ``` Then, you can generate a summary of the chat history, using any existing summary as context for the next summary. This `summarize\\_conversation` node can be called after some number of messages have accumulated in the `messages` state key. ```python theme={null} def summarize\\_conversation(state: State): # First, we get any existing summary summary = state.get(\"summary\", \"\") # Create our summarization prompt if summary: # A summary already exists summary\\_message = ( f\"This is a summary of the conversation to date: {summary}\\n\\n\" \"Extend the summary by taking into account the new messages above:\" ) else: summary\\_message = \"Create a summary of the conversation above:\" # Add prompt to our history messages = state[\"messages\"] + [HumanMessage(content=summary\\_message)] response = model.invoke(messages) # Delete all but the 2 most recent messages delete\\_messages = [RemoveMessage(id=m.id) for m in state[\"messages\"][:-2]] return {\"summary\": response.content, \"messages\": delete\\_messages} ```  ```python theme={null} from typing import Any, TypedDict from langchain.chat\\_models import init\\_chat\\_model from langchain.messages import AnyMessage from langchain\\_core.messages.utils import count\\_tokens\\_approximately from langgraph.graph import StateGraph, START, MessagesState from langgraph.checkpoint.memory import InMemorySaver from langmem.short\\_term import SummarizationNode, RunningSummary # [!code highlight] model = init\\_chat\\_model(\"claude-sonnet-4-5-20250929\") summarization\\_model = model.bind(max\\_tokens=128) class State(MessagesState): context: dict[str, RunningSummary] # [!code highlight] class LLMInputState(TypedDict): # [!code highlight] summarized\\_messages: list[AnyMessage] context: dict[str, RunningSummary] summarization\\_node = SummarizationNode( # [!code highlight] token\\_counter=count\\_tokens\\_approximately, model=summarization\\_model, max\\_tokens=256, max\\_tokens\\_before\\_summary=256, max\\_summary\\_tokens=128, ) def call\\_model(state: LLMInputState): # [!code highlight] response = model.invoke(state[\"summarized\\_messages\"]) return {\"messages\": [response]} checkpointer = InMemorySaver() builder = StateGraph(State) builder.add\\_node(call\\_model) builder.add\\_node(\"summarize\", summarization\\_node) # [!code highlight] builder.add\\_edge(START, \"summarize\") builder.add\\_edge(\"summarize\", \"call\\_model\") graph = builder.compile(checkpointer=checkpointer) # Invoke the graph config = {\"configurable\": {\"thread\\_id\": \"1\"}} graph.invoke({\"messages\": \"hi, my name is bob\"}, config) graph.invoke({\"messages\": \"write a short poem about cats\"}, config) graph.invoke({\"messages\": \"now do the same but for dogs\"}, config) final\\_response = graph.invoke({\"messages\": \"what's my name?\"}, config) final\\_response[\"messages\"][-1].pretty\\_print() print(\"\\nSummary:\", final\\_response[\"context\"][\"running\\_summary\"].summary) ``` 1. We will keep track of our running summary in the `context` field (expected by the `SummarizationNode`). 1. Define private state that will be used only for filtering the inputs to `call\\_model` node. 1. We're passing a private input state here to isolate the messages returned by the summarization node ``` ================================== Ai Message ================================== From our conversation, I can see that you introduced yourself as Bob. That's the name you shared with me when we began talking. Summary: In this conversation, I was introduced to Bob, who then asked me to write a poem about cats. I composed a poem titled \"The Mystery of Cats\" that captured cats' graceful movements, independent nature, and their special relationship with humans. Bob then requested a similar poem about dogs, so I wrote \"The Joy of Dogs,\" which highlighted dogs' loyalty, enthusiasm, and loving companionship. Both poems were written in a similar style but emphasized the distinct characteristics that make each pet special. ```  ### Manage checkpoints You can view and delete the information stored by the checkpointer.  #### View thread state   ```python theme={null} config = { \"configurable\": { \"thread\\_id\": \"1\", # [!code highlight] # optionally provide an ID for a specific checkpoint, # otherwise the latest checkpoint is shown # \"checkpoint\\_id\": \"1f029ca3-1f5b-6704-8004-820c16b69a5a\" # [!code highlight] } } graph.get\\_state(config) # [!code highlight] ``` ``` StateSnapshot( values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]}, next=(), config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}}, metadata={ 'source': 'loop', 'writes': {'call\\_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread\\_id': '1' }, created\\_at='2025-05-05T16:01:24.680462+00:00', parent\\_config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}}, tasks=(), interrupts=() ) ```   ```python theme={null} config = { \"configurable\": { \"thread\\_id\": \"1\", # [!code highlight] # optionally provide an ID for a specific checkpoint, # otherwise the latest checkpoint is shown # \"checkpoint\\_id\": \"1f029ca3-1f5b-6704-8004-820c16b69a5a\" # [!code highlight] } } checkpointer.get\\_tuple(config) # [!code highlight] ``` ``` CheckpointTuple( config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}}, checkpoint={ 'v': 3, 'ts': '2025-05-05T16:01:24.680462+00:00', 'id': '1f029ca3-1f5b-6704-8004-820c16b69a5a', 'channel\\_versions': {'\\_\\_start\\_\\_': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call\\_model': '00000000000000000000000000000006.0.14611156755133758'}, 'versions\\_seen': {'\\_\\_input\\_\\_': {}, '\\_\\_start\\_\\_': {'\\_\\_start\\_\\_': '00000000000000000000000000000004.0.5736472536395331'}, 'call\\_model': {'branch:to:call\\_model': '00000000000000000000000000000005.0.1410174088651449'}}, 'channel\\_values': {'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]}, }, metadata={ 'source': 'loop', 'writes': {'call\\_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread\\_id': '1' }, parent\\_config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}}, pending\\_writes=[] ) ```    #### View the history of the thread   ```python theme={null} config = { \"configurable\": { \"thread\\_id\": \"1\" # [!code highlight] } } list(graph.get\\_state\\_history(config)) # [!code highlight] ``` ``` [ StateSnapshot( values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]}, next=(), config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}}, metadata={'source': 'loop', 'writes': {'call\\_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread\\_id': '1'}, created\\_at='2025-05-05T16:01:24.680462+00:00', parent\\_config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}}, tasks=(), interrupts=() ), StateSnapshot( values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\")]}, next=('call\\_model',), config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}}, metadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread\\_id': '1'}, created\\_at='2025-05-05T16:01:23.863421+00:00', parent\\_config={...} tasks=(PregelTask(id='8ab4155e-6b15-b885-9ce5-bed69a2c305c', name='call\\_model', path=('\\_\\_pregel\\_pull', 'call\\_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Your name is Bob.')}),), interrupts=() ), StateSnapshot( values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]}, next=('\\_\\_start\\_\\_',), config={...}, metadata={'source': 'input', 'writes': {'\\_\\_start\\_\\_': {'messages': [{'role': 'user', 'content': \"what's my name?\"}]}}, 'step': 2, 'parents': {}, 'thread\\_id': '1'}, created\\_at='2025-05-05T16:01:23.863173+00:00', parent\\_config={...} tasks=(PregelTask(id='24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', name='\\_\\_start\\_\\_', path=('\\_\\_pregel\\_pull', '\\_\\_start\\_\\_'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': \"what's my name?\"}]}),), interrupts=() ), StateSnapshot( values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]}, next=(), config={...}, metadata={'source': 'loop', 'writes': {'call\\_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}}, 'step': 1, 'parents': {}, 'thread\\_id': '1'}, created\\_at='2025-05-05T16:01:23.862295+00:00', parent\\_config={...} tasks=(), interrupts=() ), StateSnapshot( values={'messages': [HumanMessage(content=\"hi! I'm bob\")]}, next=('call\\_model',), config={...}, metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread\\_id': '1'}, created\\_at='2025-05-05T16:01:22.278960+00:00', parent\\_config={...} tasks=(PregelTask(id='8cbd75e0-3720-b056-04f7-71ac805140a0', name='call\\_model', path=('\\_\\_pregel\\_pull', 'call\\_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}),), interrupts=() ), StateSnapshot( values={'messages': []}, next=('\\_\\_start\\_\\_',), config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565'}}, metadata={'source': 'input', 'writes': {'\\_\\_start\\_\\_': {'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}}, 'step': -1, 'parents': {}, 'thread\\_id': '1'}, created\\_at='2025-05-05T16:01:22.277497+00:00', parent\\_config=None, tasks=(PregelTask(id='d458367b-8265-812c-18e2-33001d199ce6', name='\\_\\_start\\_\\_', path=('\\_\\_pregel\\_pull', '\\_\\_start\\_\\_'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}),), interrupts=() ) ] ```   ```python theme={null} config = { \"configurable\": { \"thread\\_id\": \"1\" # [!code highlight] } } list(checkpointer.list(config)) # [!code highlight] ``` ``` [ CheckpointTuple( config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}}, checkpoint={ 'v': 3, 'ts': '2025-05-05T16:01:24.680462+00:00', 'id': '1f029ca3-1f5b-6704-8004-820c16b69a5a', 'channel\\_versions': {'\\_\\_start\\_\\_': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call\\_model': '00000000000000000000000000000006.0.14611156755133758'}, 'versions\\_seen': {'\\_\\_input\\_\\_': {}, '\\_\\_start\\_\\_': {'\\_\\_start\\_\\_': '00000000000000000000000000000004.0.5736472536395331'}, 'call\\_model': {'branch:to:call\\_model': '00000000000000000000000000000005.0.1410174088651449'}}, 'channel\\_values': {'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]}, }, metadata={'source': 'loop', 'writes': {'call\\_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread\\_id': '1'}, parent\\_config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}}, pending\\_writes=[] ), CheckpointTuple( config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}}, checkpoint={ 'v': 3, 'ts': '2025-05-05T16:01:23.863421+00:00', 'id': '1f029ca3-1790-6b0a-8003-baf965b6a38f', 'channel\\_versions': {'\\_\\_start\\_\\_': '00000000000000000000000000000005.0.5290678567601859', 'messages': '00000000000000000000000000000006.0.3205149138784782', 'branch:to:call\\_model': '00000000000000000000000000000006.0.14611156755133758'}, 'versions\\_seen': {'\\_\\_input\\_\\_': {}, '\\_\\_start\\_\\_': {'\\_\\_start\\_\\_': '00000000000000000000000000000004.0.5736472536395331'}, 'call\\_model': {'branch:to:call\\_model': '00000000000000000000000000000005.0.1410174088651449'}}, 'channel\\_values': {'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\")], 'branch:to:call\\_model': None} }, metadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread\\_id': '1'}, parent\\_config={...}, pending\\_writes=[('8ab4155e-6b15-b885-9ce5-bed69a2c305c', 'messages', AIMessage(content='Your name is Bob.'))] ), CheckpointTuple( config={...}, checkpoint={ 'v': 3, 'ts': '2025-05-05T16:01:23.863173+00:00', 'id': '1f029ca3-1790-616e-8002-9e021694a0cd', 'channel\\_versions': {'\\_\\_start\\_\\_': '00000000000000000000000000000004.0.5736472536395331', 'messages': '00000000000000000000000000000003.0.7056767754077798', 'branch:to:call\\_model': '00000000000000000000000000000003.0.22059023329132854'}, 'versions\\_seen': {'\\_\\_input\\_\\_': {}, '\\_\\_start\\_\\_': {'\\_\\_start\\_\\_': '00000000000000000000000000000001.0.7040775356287469'}, 'call\\_model': {'branch:to:call\\_model': '00000000000000000000000000000002.0.9300422176788571'}}, 'channel\\_values': {'\\_\\_start\\_\\_': {'messages': [{'role': 'user', 'content': \"what's my name?\"}]}, 'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]} }, metadata={'source': 'input', 'writes': {'\\_\\_start\\_\\_': {'messages': [{'role': 'user', 'content': \"what's my name?\"}]}}, 'step': 2, 'parents': {}, 'thread\\_id': '1'}, parent\\_config={...}, pending\\_writes=[('24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', 'messages', [{'role': 'user', 'content': \"what's my name?\"}]), ('24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', 'branch:to:call\\_model', None)] ), CheckpointTuple( config={...}, checkpoint={ 'v': 3, 'ts': '2025-05-05T16:01:23.862295+00:00', 'id': '1f029ca3-178d-6f54-8001-d7b180db0c89', 'channel\\_versions': {'\\_\\_start\\_\\_': '00000000000000000000000000000002.0.18673090920108737', 'messages': '00000000000000000000000000000003.0.7056767754077798', 'branch:to:call\\_model': '00000000000000000000000000000003.0.22059023329132854'}, 'versions\\_seen': {'\\_\\_input\\_\\_': {}, '\\_\\_start\\_\\_': {'\\_\\_start\\_\\_': '00000000000000000000000000000001.0.7040775356287469'}, 'call\\_model': {'branch:to:call\\_model': '00000000000000000000000000000002.0.9300422176788571'}}, 'channel\\_values': {'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]} }, metadata={'source': 'loop', 'writes': {'call\\_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}}, 'step': 1, 'parents': {}, 'thread\\_id': '1'}, parent\\_config={...}, pending\\_writes=[] ), CheckpointTuple( config={...}, checkpoint={ 'v': 3, 'ts': '2025-05-05T16:01:22.278960+00:00', 'id': '1f029ca3-0874-6612-8000-339f2abc83b1', 'channel\\_versions': {'\\_\\_start\\_\\_': '00000000000000000000000000000002.0.18673090920108737', 'messages': '00000000000000000000000000000002.0.30296526818059655', 'branch:to:call\\_model': '00000000000000000000000000000002.0.9300422176788571'}, 'versions\\_seen': {'\\_\\_input\\_\\_': {}, '\\_\\_start\\_\\_': {'\\_\\_start\\_\\_': '00000000000000000000000000000001.0.7040775356287469'}}, 'channel\\_values': {'messages': [HumanMessage(content=\"hi! I'm bob\")], 'branch:to:call\\_model': None} }, metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread\\_id': '1'}, parent\\_config={...}, pending\\_writes=[('8cbd75e0-3720-b056-04f7-71ac805140a0', 'messages', AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'))] ), CheckpointTuple( config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565'}}, checkpoint={ 'v': 3, 'ts': '2025-05-05T16:01:22.277497+00:00', 'id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565', 'channel\\_versions': {'\\_\\_start\\_\\_': '00000000000000000000000000000001.0.7040775356287469'}, 'versions\\_seen': {'\\_\\_input\\_\\_': {}}, 'channel\\_values': {'\\_\\_start\\_\\_': {'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}} }, metadata={'source': 'input', 'writes': {'\\_\\_start\\_\\_': {'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}}, 'step': -1, 'parents': {}, 'thread\\_id': '1'}, parent\\_config=None, pending\\_writes=[('d458367b-8265-812c-18e2-33001d199ce6', 'messages', [{'role': 'user', 'content': \"hi! I'm bob\"}]), ('d458367b-8265-812c-18e2-33001d199ce6', 'branch:to:call\\_model', None)] ) ] ```   #### Delete all checkpoints for a thread ```python theme={null} thread\\_id = \"1\" checkpointer.delete\\_thread(thread\\_id) ``` ## Prebuilt memory tools \\*\\*LangMem\\*\\* is a LangChain-maintained library that offers tools for managing long-term memories in your agent. See the [LangMem documentation](https://langchain-ai.github.io/langmem/) for usage examples. \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/add-memory.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/agentic-rag.md",
      "title": null,
      "raw_content": "# Build a custom RAG agent with LangGraph ## Overview In this tutorial we will build a [retrieval](/oss/python/langchain/retrieval) agent using LangGraph. LangChain offers built-in [agent](/oss/python/langchain/agents) implementations, implemented using [LangGraph](/oss/python/langgraph/overview) primitives. If deeper customization is required, agents can be implemented directly in LangGraph. This guide demonstrates an example implementation of a retrieval agent. [Retrieval](/oss/python/langchain/retrieval) agents are useful when you want an LLM to make a decision about whether to retrieve context from a vectorstore or respond to the user directly. By the end of the tutorial we will have done the following: 1. Fetch and preprocess documents that will be used for retrieval. 2. Index those documents for semantic search and create a retriever tool for the agent. 3. Build an agentic RAG system that can decide when to use the retriever tool. ### Concepts We will cover the following concepts: \\* [Retrieval](/oss/python/langchain/retrieval) using [document loaders](/oss/python/integrations/document\\_loaders), [text splitters](/oss/python/integrations/splitters), [embeddings](/oss/python/integrations/text\\_embedding), and [vector stores](/oss/python/integrations/vectorstores) \\* The LangGraph [Graph API](/oss/python/langgraph/graph-api), including state, nodes, edges, and conditional edges. ## Setup Let's download the required packages and set our API keys: ```python theme={null} pip install -U langgraph \"langchain[openai]\" langchain-community langchain-text-splitters bs4 ``` ```python theme={null} import getpass import os def \\_set\\_env(key: str): if key not in os.environ: os.environ[key] = getpass.getpass(f\"{key}:\") \\_set\\_env(\"OPENAI\\_API\\_KEY\") ```  Sign up for LangSmith to quickly spot issues and improve the performance of your LangGraph projects. [LangSmith](https://docs.smith.langchain.com) lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph.  ## 1. Preprocess documents 1. Fetch documents to use in our RAG system. We will use three of the most recent pages from [Lilian Weng's excellent blog](https://lilianweng.github.io/). We'll start by fetching the content of the pages using `WebBaseLoader` utility: ```python theme={null} from langchain\\_community.document\\_loaders import WebBaseLoader urls = [ \"https://lilianweng.github.io/posts/2024-11-28-reward-hacking/\", \"https://lilianweng.github.io/posts/2024-07-07-hallucination/\", \"https://lilianweng.github.io/posts/2024-04-12-diffusion-video/\", ] docs = [WebBaseLoader(url).load() for url in urls] ``` ```python theme={null} docs[0][0].page\\_content.strip()[:1000] ``` 2. Split the fetched documents into smaller chunks for indexing into our vectorstore: ```python theme={null} from langchain\\_text\\_splitters import RecursiveCharacterTextSplitter docs\\_list = [item for sublist in docs for item in sublist] text\\_splitter = RecursiveCharacterTextSplitter.from\\_tiktoken\\_encoder( chunk\\_size=100, chunk\\_overlap=50 ) doc\\_splits = text\\_splitter.split\\_documents(docs\\_list) ``` ```python theme={null} doc\\_splits[0].page\\_content.strip() ``` ## 2. Create a retriever tool Now that we have our split documents, we can index them into a vector store that we'll use for semantic search. 1. Use an in-memory vector store and OpenAI embeddings: ```python theme={null} from langchain\\_core.vectorstores import InMemoryVectorStore from langchain\\_openai import OpenAIEmbeddings vectorstore = InMemoryVectorStore.from\\_documents( documents=doc\\_splits, embedding=OpenAIEmbeddings() ) retriever = vectorstore.as\\_retriever() ``` 2. Create a retriever tool using LangChain's prebuilt `create\\_retriever\\_tool`: ```python theme={null} from langchain\\_classic.tools.retriever import create\\_retriever\\_tool retriever\\_tool = create\\_retriever\\_tool( retriever, \"retrieve\\_blog\\_posts\", \"Search and return information about Lilian Weng blog posts.\", ) ``` 3. Test the tool: ```python theme={null} retriever\\_tool.invoke({\"query\": \"types of reward hacking\"}) ``` ## 3. Generate query Now we will start building components ([nodes](/oss/python/langgraph/graph-api#nodes) and [edges](/oss/python/langgraph/graph-api#edges)) for our agentic RAG graph. Note that the components will operate on the [`MessagesState`](/oss/python/langgraph/graph-api#messagesstate) \u2014 graph state that contains a `messages` key with a list of [chat messages](https://python.langchain.com/docs/concepts/messages/). 1. Build a `generate\\_query\\_or\\_respond` node. It will call an LLM to generate a response based on the current graph state (list of messages). Given the input messages, it will decide to retrieve using the retriever tool, or respond directly to the user. Note that we're giving the chat model access to the `retriever\\_tool` we created earlier via `.bind\\_tools`: ```python theme={null} from langgraph.graph import MessagesState from langchain.chat\\_models import init\\_chat\\_model response\\_model = init\\_chat\\_model(\"gpt-4o\", temperature=0) def generate\\_query\\_or\\_respond(state: MessagesState): \"\"\"Call the model to generate a response based on the current state. Given the question, it will decide to retrieve using the retriever tool, or simply respond to the user. \"\"\" response = ( response\\_model .bind\\_tools([retriever\\_tool]).invoke(state[\"messages\"]) # [!code highlight] ) return {\"messages\": [response]} ``` 2. Try it on a random input: ```python theme={null} input = {\"messages\": [{\"role\": \"user\", \"content\": \"hello!\"}]} generate\\_query\\_or\\_respond(input)[\"messages\"][-1].pretty\\_print() ``` \\*\\*Output:\\*\\* ``` ================================== Ai Message ================================== Hello! How can I help you today? ``` 3. Ask a question that requires semantic search: ```python theme={null} input = { \"messages\": [ { \"role\": \"user\", \"content\": \"What does Lilian Weng say about types of reward hacking?\", } ] } generate\\_query\\_or\\_respond(input)[\"messages\"][-1].pretty\\_print() ``` \\*\\*Output:\\*\\* ``` ================================== Ai Message ================================== Tool Calls: retrieve\\_blog\\_posts (call\\_tYQxgfIlnQUDMdtAhdbXNwIM) Call ID: call\\_tYQxgfIlnQUDMdtAhdbXNwIM Args: query: types of reward hacking ``` ## 4. Grade documents 1. Add a [conditional edge](/oss/python/langgraph/graph-api#conditional-edges) \u2014 `grade\\_documents` \u2014 to determine whether the retrieved documents are relevant to the question. We will use a model with a structured output schema `GradeDocuments` for document grading. The `grade\\_documents` function will return the name of the node to go to based on the grading decision (`generate\\_answer` or `rewrite\\_question`): ```python theme={null} from pydantic import BaseModel, Field from typing import Literal GRADE\\_PROMPT = ( \"You are a grader assessing relevance of a retrieved document to a user question. \\n \" \"Here is the retrieved document: \\n\\n {context} \\n\\n\" \"Here is the user question: {question} \\n\" \"If the document contains keyword(s) or semantic meaning related to the user question, grade it as relevant. \\n\" \"Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\" ) class GradeDocuments(BaseModel): # [!code highlight] \"\"\"Grade documents using a binary score for relevance check.\"\"\" binary\\_score: str = Field( description=\"Relevance score: 'yes' if relevant, or 'no' if not relevant\" ) grader\\_model = init\\_chat\\_model(\"gpt-4o\", temperature=0) def grade\\_documents( state: MessagesState, ) -> Literal[\"generate\\_answer\", \"rewrite\\_question\"]: \"\"\"Determine whether the retrieved documents are relevant to the question.\"\"\" question = state[\"messages\"][0].content context = state[\"messages\"][-1].content prompt = GRADE\\_PROMPT.format(question=question, context=context) response = ( grader\\_model .with\\_structured\\_output(GradeDocuments).invoke( # [!code highlight] [{\"role\": \"user\", \"content\": prompt}] ) ) score = response.binary\\_score if score == \"yes\": return \"generate\\_answer\" else: return \"rewrite\\_question\" ``` 2. Run this with irrelevant documents in the tool response: ```python theme={null} from langchain\\_core.messages import convert\\_to\\_messages input = { \"messages\": convert\\_to\\_messages( [ { \"role\": \"user\", \"content\": \"What does Lilian Weng say about types of reward hacking?\", }, { \"role\": \"assistant\", \"content\": \"\", \"tool\\_calls\": [ { \"id\": \"1\", \"name\": \"retrieve\\_blog\\_posts\", \"args\": {\"query\": \"types of reward hacking\"}, } ], }, {\"role\": \"tool\", \"content\": \"meow\", \"tool\\_call\\_id\": \"1\"}, ] ) } grade\\_documents(input) ``` 3. Confirm that the relevant documents are classified as such: ```python theme={null} input = { \"messages\": convert\\_to\\_messages( [ { \"role\": \"user\", \"content\": \"What does Lilian Weng say about types of reward hacking?\", }, { \"role\": \"assistant\", \"content\": \"\", \"tool\\_calls\": [ { \"id\": \"1\", \"name\": \"retrieve\\_blog\\_posts\", \"args\": {\"query\": \"types of reward hacking\"}, } ], }, { \"role\": \"tool\", \"content\": \"reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering\", \"tool\\_call\\_id\": \"1\", }, ] ) } grade\\_documents(input) ``` ## 5. Rewrite question 1. Build the `rewrite\\_question` node. The retriever tool can return potentially irrelevant documents, which indicates a need to improve the original user question. To do so, we will call the `rewrite\\_question` node: ```python theme={null} from langchain.messages import HumanMessage REWRITE\\_PROMPT = ( \"Look at the input and try to reason about the underlying semantic intent / meaning.\\n\" \"Here is the initial question:\" \"\\n ------- \\n\" \"{question}\" \"\\n ------- \\n\" \"Formulate an improved question:\" ) def rewrite\\_question(state: MessagesState): \"\"\"Rewrite the original user question.\"\"\" messages = state[\"messages\"] question = messages[0].content prompt = REWRITE\\_PROMPT.format(question=question) response = response\\_model.invoke([{\"role\": \"user\", \"content\": prompt}]) return {\"messages\": [HumanMessage(content=response.content)]} ``` 2. Try it out: ```python theme={null} input = { \"messages\": convert\\_to\\_messages( [ { \"role\": \"user\", \"content\": \"What does Lilian Weng say about types of reward hacking?\", }, { \"role\": \"assistant\", \"content\": \"\", \"tool\\_calls\": [ { \"id\": \"1\", \"name\": \"retrieve\\_blog\\_posts\", \"args\": {\"query\": \"types of reward hacking\"}, } ], }, {\"role\": \"tool\", \"content\": \"meow\", \"tool\\_call\\_id\": \"1\"}, ] ) } response = rewrite\\_question(input) print(response[\"messages\"][-1].content) ``` \\*\\*Output:\\*\\* ``` What are the different types of reward hacking described by Lilian Weng, and how does she explain them? ``` ## 6. Generate an answer 1. Build `generate\\_answer` node: if we pass the grader checks, we can generate the final answer based on the original question and the retrieved context: ```python theme={null} GENERATE\\_PROMPT = ( \"You are an assistant for question-answering tasks. \" \"Use the following pieces of retrieved context to answer the question. \" \"If you don't know the answer, just say that you don't know. \" \"Use three sentences maximum and keep the answer concise.\\n\" \"Question: {question} \\n\" \"Context: {context}\" ) def generate\\_answer(state: MessagesState): \"\"\"Generate an answer.\"\"\" question = state[\"messages\"][0].content context = state[\"messages\"][-1].content prompt = GENERATE\\_PROMPT.format(question=question, context=context) response = response\\_model.invoke([{\"role\": \"user\", \"content\": prompt}]) return {\"messages\": [response]} ``` 2. Try it: ```python theme={null} input = { \"messages\": convert\\_to\\_messages( [ { \"role\": \"user\", \"content\": \"What does Lilian Weng say about types of reward hacking?\", }, { \"role\": \"assistant\", \"content\": \"\", \"tool\\_calls\": [ { \"id\": \"1\", \"name\": \"retrieve\\_blog\\_posts\", \"args\": {\"query\": \"types of reward hacking\"}, } ], }, { \"role\": \"tool\", \"content\": \"reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering\", \"tool\\_call\\_id\": \"1\", }, ] ) } response = generate\\_answer(input) response[\"messages\"][-1].pretty\\_print() ``` \\*\\*Output:\\*\\* ``` ================================== Ai Message ================================== Lilian Weng categorizes reward hacking into two types: environment or goal misspecification, and reward tampering. She considers reward hacking as a broad concept that includes both of these categories. Reward hacking occurs when an agent exploits flaws or ambiguities in the reward function to achieve high rewards without performing the intended behaviors. ``` ## 7. Assemble the graph Now we'll assemble all the nodes and edges into a complete graph: \\* Start with a `generate\\_query\\_or\\_respond` and determine if we need to call `retriever\\_tool` \\* Route to next step using `tools\\_condition`: \\* If `generate\\_query\\_or\\_respond` returned `tool\\_calls`, call `retriever\\_tool` to retrieve context \\* Otherwise, respond directly to the user \\* Grade retrieved document content for relevance to the question (`grade\\_documents`) and route to next step: \\* If not relevant, rewrite the question using `rewrite\\_question` and then call `generate\\_query\\_or\\_respond` again \\* If relevant, proceed to `generate\\_answer` and generate final response using the [`ToolMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage) with the retrieved document context ```python theme={null} from langgraph.graph import StateGraph, START, END from langgraph.prebuilt import ToolNode, tools\\_condition workflow = StateGraph(MessagesState) # Define the nodes we will cycle between workflow.add\\_node(generate\\_query\\_or\\_respond) workflow.add\\_node(\"retrieve\", ToolNode([retriever\\_tool])) workflow.add\\_node(rewrite\\_question) workflow.add\\_node(generate\\_answer) workflow.add\\_edge(START, \"generate\\_query\\_or\\_respond\") # Decide whether to retrieve workflow.add\\_conditional\\_edges( \"generate\\_query\\_or\\_respond\", # Assess LLM decision (call `retriever\\_tool` tool or respond to the user) tools\\_condition, { # Translate the condition outputs to nodes in our graph \"tools\": \"retrieve\", END: END, }, ) # Edges taken after the `action` node is called. workflow.add\\_conditional\\_edges( \"retrieve\", # Assess agent decision grade\\_documents, ) workflow.add\\_edge(\"generate\\_answer\", END) workflow.add\\_edge(\"rewrite\\_question\", \"generate\\_query\\_or\\_respond\") # Compile graph = workflow.compile() ``` Visualize the graph: ```python theme={null} from IPython.display import Image, display display(Image(graph.get\\_graph().draw\\_mermaid\\_png())) ``` ## 8. Run the agentic RAG Now let's test the complete graph by running it with a question: ```python theme={null} for chunk in graph.stream( { \"messages\": [ { \"role\": \"user\", \"content\": \"What does Lilian Weng say about types of reward hacking?\", } ] } ): for node, update in chunk.items(): print(\"Update from node\", node) update[\"messages\"][-1].pretty\\_print() print(\"\\n\\n\") ``` \\*\\*Output:\\*\\* ``` Update from node generate\\_query\\_or\\_respond ================================== Ai Message ================================== Tool Calls: retrieve\\_blog\\_posts (call\\_NYu2vq4km9nNNEFqJwefWKu1) Call ID: call\\_NYu2vq4km9nNNEFqJwefWKu1 Args: query: types of reward hacking Update from node retrieve ================================= Tool Message ================================== Name: retrieve\\_blog\\_posts (Note: Some work defines reward tampering as a distinct category of misalignment behavior from reward hacking. But I consider reward hacking as a broader concept here.) At a high level, reward hacking can be categorized into two types: environment or goal misspecification, and reward tampering. Why does Reward Hacking Exist?# Pan et al. (2022) investigated reward hacking as a function of agent capabilities, including (1) model size, (2) action space resolution, (3) observation space noise, and (4) training time. They also proposed a taxonomy of three types of misspecified proxy rewards: Let's Define Reward Hacking# Reward shaping in RL is challenging. Reward hacking occurs when an RL agent exploits flaws or ambiguities in the reward function to obtain high rewards without genuinely learning the intended behaviors or completing the task as designed. In recent years, several related concepts have been proposed, all referring to some form of reward hacking: Update from node generate\\_answer ================================== Ai Message ================================== Lilian Weng categorizes reward hacking into two types: environment or goal misspecification, and reward tampering. She considers reward hacking as a broad concept that includes both of these categories. Reward hacking occurs when an agent exploits flaws or ambiguities in the reward function to achieve high rewards without performing the intended behaviors. ``` \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/agentic-rag.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/application-structure.md",
      "title": null,
      "raw_content": "# Application structure A LangGraph application consists of one or more graphs, a configuration file (`langgraph.json`), a file that specifies dependencies, and an optional `.env` file that specifies environment variables. This guide shows a typical structure of an application and shows you how to provide the required configuration to deploy an application with [LangSmith Deployment](/langsmith/deployments).  LangSmith Deployment is a managed hosting platform for deploying and scaling LangGraph agents. It handles the infrastructure, scaling, and operational concerns so you can deploy your stateful, long-running agents directly from your repository. Learn more in the [Deployment documentation](/langsmith/deployments).  ## Key Concepts To deploy using the LangSmith, the following information should be provided: 1. A [LangGraph configuration file](#configuration-file-concepts) (`langgraph.json`) that specifies the dependencies, graphs, and environment variables to use for the application. 2. The [graphs](#graphs) that implement the logic of the application. 3. A file that specifies [dependencies](#dependencies) required to run the application. 4. [Environment variables](#environment-variables) that are required for the application to run. ## File structure Below are examples of directory structures for applications:   ```plaintext theme={null} my-app/ \u251c\u2500\u2500 my\\_agent # all project code lies within here \u2502 \u251c\u2500\u2500 utils # utilities for your graph \u2502 \u2502 \u251c\u2500\u2500 \\_\\_init\\_\\_.py \u2502 \u2502 \u251c\u2500\u2500 tools.py # tools for your graph \u2502 \u2502 \u251c\u2500\u2500 nodes.py # node functions for your graph \u2502 \u2502 \u2514\u2500\u2500 state.py # state definition of your graph \u2502 \u251c\u2500\u2500 \\_\\_init\\_\\_.py \u2502 \u2514\u2500\u2500 agent.py # code for constructing your graph \u251c\u2500\u2500 .env # environment variables \u251c\u2500\u2500 requirements.txt # package dependencies \u2514\u2500\u2500 langgraph.json # configuration file for LangGraph ```   ```plaintext theme={null} my-app/ \u251c\u2500\u2500 my\\_agent # all project code lies within here \u2502 \u251c\u2500\u2500 utils # utilities for your graph \u2502 \u2502 \u251c\u2500\u2500 \\_\\_init\\_\\_.py \u2502 \u2502 \u251c\u2500\u2500 tools.py # tools for your graph \u2502 \u2502 \u251c\u2500\u2500 nodes.py # node functions for your graph \u2502 \u2502 \u2514\u2500\u2500 state.py # state definition of your graph \u2502 \u251c\u2500\u2500 \\_\\_init\\_\\_.py \u2502 \u2514\u2500\u2500 agent.py # code for constructing your graph \u251c\u2500\u2500 .env # environment variables \u251c\u2500\u2500 langgraph.json # configuration file for LangGraph \u2514\u2500\u2500 pyproject.toml # dependencies for your project ```    The directory structure of a LangGraph application can vary depending on the programming language and the package manager used.   ## Configuration file The `langgraph.json` file is a JSON file that specifies the dependencies, graphs, environment variables, and other settings required to deploy a LangGraph application. See the [LangGraph configuration file reference](/langsmith/cli#configuration-file) for details on all supported keys in the JSON file.  The [LangGraph CLI](/langsmith/cli) defaults to using the configuration file `langgraph.json` in the current directory.  ### Examples \\* The dependencies involve a custom local package and the `langchain\\_openai` package. \\* A single graph will be loaded from the file `./your\\_package/your\\_file.py` with the variable `variable`. \\* The environment variables are loaded from the `.env` file. ```json theme={null} { \"dependencies\": [\"langchain\\_openai\", \"./your\\_package\"], \"graphs\": { \"my\\_agent\": \"./your\\_package/your\\_file.py:agent\" }, \"env\": \"./.env\" } ``` ## Dependencies A LangGraph application may depend on other Python packages. You will generally need to specify the following information for dependencies to be set up correctly: 1. A file in the directory that specifies the dependencies (e.g. `requirements.txt`, `pyproject.toml`, or `package.json`). 2. A `dependencies` key in the [LangGraph configuration file](#configuration-file-concepts) that specifies the dependencies required to run the LangGraph application. 3. Any additional binaries or system libraries can be specified using `dockerfile\\_lines` key in the [LangGraph configuration file](#configuration-file-concepts). ## Graphs Use the `graphs` key in the [LangGraph configuration file](#configuration-file-concepts) to specify which graphs will be available in the deployed LangGraph application. You can specify one or more graphs in the configuration file. Each graph is identified by a name (which should be unique) and a path for either: (1) the compiled graph or (2) a function that makes a graph is defined. ## Environment variables If you're working with a deployed LangGraph application locally, you can configure environment variables in the `env` key of the [LangGraph configuration file](#configuration-file-concepts). For a production deployment, you will typically want to configure the environment variables in the deployment environment. \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/application-structure.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/case-studies.md",
      "title": null,
      "raw_content": "# Case studies This list of companies using LangGraph and their success stories is compiled from public sources. If your company uses LangGraph, we'd love for you to share your story and add it to the list. You\u2019re also welcome to contribute updates based on publicly available information from other companies, such as blog posts or press releases. | Company | Industry | Use case | Reference | | --------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------ | ------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | | [AirTop](https://www.airtop.ai/) | Software & Technology (GenAI Native) | Browser automation for AI agents | [Case study, 2024](https://blog.langchain.dev/customers-airtop/) | | [AppFolio](https://www.appfolio.com/) | Real Estate | Copilot for domain-specific task | [Case study, 2024](https://blog.langchain.dev/customers-appfolio/) | | [Athena Intelligence](https://www.athenaintel.com/) | Software & Technology (GenAI Native) | Research & summarization | [Case study, 2024](https://blog.langchain.dev/customers-athena-intelligence/) | | [BlackRock](https://www.blackrock.com/) | Financial Services | Copilot for domain-specific task | [Interrupt talk, 2025](https://youtu.be/oyqeCHFM5U4?feature=shared) | | [Captide](https://www.captide.co/) | Software & Technology (GenAI Native) | Data extraction | [Case study, 2025](https://blog.langchain.dev/how-captide-is-redefining-equity-research-with-agentic-workflows-built-on-langgraph-and-langsmith/) | | [Cisco CX](https://www.cisco.com/site/us/en/services/modern-data-center/index.html?CCID=cc005911\\&DTID=eivtotr001480\\&OID=srwsas032775) | Software & Technology | Customer support | [Interrupt Talk, 2025](https://youtu.be/gPhyPRtIMn0?feature=shared) | | [Cisco Outshift](https://outshift.cisco.com/) | Software & Technology | DevOps | [Video story, 2025](https://www.youtube.com/watch?v=htcb-vGR\\_x0); [Case study, 2025](https://blog.langchain.com/cisco-outshift/); [Blog post, 2025](https://outshift.cisco.com/blog/build-react-agent-application-for-devops-tasks-using-rest-apis) | | [Cisco TAC](https://www.cisco.com/c/en/us/support/index.html) | Software & Technology | Customer support | [Video story, 2025](https://youtu.be/EAj0HBDGqaE?feature=shared) | | [City of Hope](https://www.cityofhope.org/) | Non-profit | Copilot for domain-specific task | [Video story, 2025](https://youtu.be/9ABwtK2gIZU?feature=shared) | | [C.H. Robinson](https://www.chrobinson.com/en-us/) | Logistics | Automation | [Case study, 2025](https://blog.langchain.dev/customers-chrobinson/) | | [Definely](https://www.definely.com/) | Legal | Copilot for domain-specific task | [Case study, 2025](https://blog.langchain.com/customers-definely/) | | [Docent Pro](https://docentpro.com/) | Travel | GenAI embedded product experiences | [Case study, 2025](https://blog.langchain.com/customers-docentpro/) | | [Elastic](https://www.elastic.co/) | Software & Technology | Copilot for domain-specific task | [Blog post, 2025](https://www.elastic.co/blog/elastic-security-generative-ai-features) | | [Exa](https://exa.ai/) | Software & Technology (GenAI Native) | Search | [Case study, 2025](https://blog.langchain.com/exa/) | | [GitLab](https://about.gitlab.com/) | Software & Technology | Code generation | [Duo workflow docs](https://handbook.gitlab.com/handbook/engineering/architecture/design-documents/duo\\_workflow/) | | [Harmonic](https://harmonic.ai/) | Software & Technology | Search | [Case study, 2025](https://blog.langchain.com/customers-harmonic/) | | [Inconvo](https://inconvo.ai/?ref=blog.langchain.dev) | Software & Technology | Code generation | [Case study, 2025](https://blog.langchain.dev/customers-inconvo/) | | [Infor](https://infor.com/) | Software & Technology | GenAI embedded product experiences; customer support; copilot | [Case study, 2025](https://blog.langchain.dev/customers-infor/) | | [J.P. Morgan](https://www.jpmorganchase.com/) | Financial Services | Copilot for domain-specific task | [Interrupt talk, 2025](https://youtu.be/yMalr0jiOAc?feature=shared) | | [Klarna](https://www.klarna.com/) | Fintech | Copilot for domain-specific task | [Case study, 2025](https://blog.langchain.dev/customers-klarna/) | | [Komodo Health](https://www.komodohealth.com/) | Healthcare | Copilot for domain-specific task | [Blog post](https://www.komodohealth.com/perspectives/new-gen-ai-assistant-empowers-the-enterprise/) | | [LinkedIn](https://www.linkedin.com/) | Social Media | Code generation; Search & discovery | [Interrupt talk, 2025](https://youtu.be/NmblVxyBhi8?feature=shared); [Blog post, 2025](https://www.linkedin.com/blog/engineering/ai/practical-text-to-sql-for-data-analytics); [Blog post, 2024](https://www.linkedin.com/blog/engineering/generative-ai/behind-the-platform-the-journey-to-create-the-linkedin-genai-application-tech-stack) | | [Minimal](https://gominimal.ai/) | E-commerce | Customer support | [Case study, 2025](https://blog.langchain.dev/how-minimal-built-a-multi-agent-customer-support-system-with-langgraph-langsmith/) | | [Modern Treasury](https://www.moderntreasury.com/) | Fintech | GenAI embedded product experiences | [Video story, 2025](https://youtu.be/AwAiffXqaCU?feature=shared) | | [Monday](https://monday.com/) | Software & Technology | GenAI embedded product experiences | [Interrupt talk, 2025](https://blog.langchain.dev/how-minimal-built-a-multi-agent-customer-support-system-with-langgraph-langsmith/) | | [Morningstar](https://www.morningstar.com/) | Financial Services | Research & summarization | [Video story, 2025](https://youtu.be/6LidoFXCJPs?feature=shared) | | [OpenRecovery](https://www.openrecovery.com/) | Healthcare | Copilot for domain-specific task | [Case study, 2024](https://blog.langchain.dev/customers-openrecovery/) | | [Pigment](https://www.pigment.com/) | Fintech | GenAI embedded product experiences | [Video story, 2025](https://youtu.be/5JVSO2KYOmE?feature=shared) | | [Prosper](https://www.prosper.com/) | Fintech | Customer support | [Video story, 2025](https://youtu.be/9RFNOYtkwsc?feature=shared) | | [Qodo](https://www.qodo.ai/) | Software & Technology (GenAI Native) | Code generation | [Blog post, 2025](https://www.qodo.ai/blog/why-we-chose-langgraph-to-build-our-coding-agent/) | | [Rakuten](https://www.rakuten.com/) | E-commerce / Fintech | Copilot for domain-specific task | [Video story, 2025](https://youtu.be/gD1LIjCkuA8?feature=shared); [Blog post, 2025](https://rakuten.today/blog/from-ai-hype-to-real-world-tools-rakuten-teams-up-with-langchain.html) | | [Replit](https://replit.com/) | Software & Technology | Code generation | [Blog post, 2024](https://blog.langchain.dev/customers-replit/); [Breakout agent story, 2024](https://www.langchain.com/breakoutagents/replit); [Fireside chat video, 2024](https://www.youtube.com/watch?v=ViykMqljjxU) | | [Rexera](https://www.rexera.com/) | Real Estate (GenAI Native) | Copilot for domain-specific task | [Case study, 2024](https://blog.langchain.dev/customers-rexera/) | | [Abu Dhabi Government](https://www.tamm.abudhabi/) | Government | Search | [Case study, 2025](https://blog.langchain.com/customers-abu-dhabi-government/) | | [Tradestack](https://www.tradestack.uk/) | Software & Technology (GenAI Native) | Copilot for domain-specific task | [Case study, 2024](https://blog.langchain.dev/customers-tradestack/) | | [Uber](https://www.uber.com/) | Transportation | Developer productivity; Code generation | [Interrupt talk, 2025](https://youtu.be/Bugs0dVcNI8?feature=shared); [Presentation, 2024](https://dpe.org/sessions/ty-smith-adam-huda/this-year-in-ubers-ai-driven-developer-productivity-revolution/); [Video, 2024](https://www.youtube.com/watch?v=8rkA5vWUE4Y) | | [Unify](https://www.unifygtm.com/) | Software & Technology (GenAI Native) | Copilot for domain-specific task | [Interrupt talk, 2025](https://youtu.be/pKk-LfhujwI?feature=shared); [Blog post, 2024](https://blog.langchain.dev/unify-launches-agents-for-account-qualification-using-langgraph-and-langsmith/) | | [Vizient](https://www.vizientinc.com/) | Healthcare | Copilot for domain-specific task | [Video story, 2025](https://www.youtube.com/watch?v=vrjJ6NuyTWA); [Case study, 2025](https://blog.langchain.dev/p/3d2cd58c-13a5-4df9-bd84-7d54ed0ed82c/) | | [Vodafone](https://www.vodafone.com/) | Telecommunications | Code generation; internal search | [Case study, 2025](https://blog.langchain.dev/customers-vodafone/) | | [WebToon](https://www.webtoons.com/en/) | Media & Entertainment | Data extraction | [Case study, 2025](https://blog.langchain.com/customers-webtoon/) | | [11x](https://www.11x.ai/) | Software & Technology (GenAI Native) | Research & outreach | [Interrupt talk, 2025](https://youtu.be/fegwPmaAPQk?feature=shared) | \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/case-studies.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/choosing-apis.md",
      "title": null,
      "raw_content": "# Choosing between Graph and Functional APIs LangGraph provides two different APIs to build agent workflows: the \\*\\*Graph API\\*\\* and the \\*\\*Functional API\\*\\*. Both APIs share the same underlying runtime and can be used together in the same application, but they are designed for different use cases and development preferences. This guide will help you understand when to use each API based on your specific requirements. ## Quick decision guide Use the \\*\\*Graph API\\*\\* when you need: \\* \\*\\*Complex workflow visualization\\*\\* for debugging and documentation \\* \\*\\*Explicit state management\\*\\* with shared data across multiple nodes \\* \\*\\*Conditional branching\\*\\* with multiple decision points \\* \\*\\*Parallel execution paths\\*\\* that need to merge later \\* \\*\\*Team collaboration\\*\\* where visual representation aids understanding Use the \\*\\*Functional API\\*\\* when you want: \\* \\*\\*Minimal code changes\\*\\* to existing procedural code \\* \\*\\*Standard control flow\\*\\* (if/else, loops, function calls) \\* \\*\\*Function-scoped state\\*\\* without explicit state management \\* \\*\\*Rapid prototyping\\*\\* with less boilerplate \\* \\*\\*Linear workflows\\*\\* with simple branching logic ## Detailed comparison ### When to use the Graph API The [Graph API](/oss/python/langgraph/graph-api) uses a declarative approach where you define nodes, edges, and shared state to create a visual graph structure. \\*\\*1. Complex decision trees and branching logic\\*\\* When your workflow has multiple decision points that depend on various conditions, the Graph API makes these branches explicit and easy to visualize. ```python theme={null} # Graph API: Clear visualization of decision paths from langgraph.graph import StateGraph from typing import TypedDict class AgentState(TypedDict): messages: list current\\_tool: str retry\\_count: int def should\\_continue(state): if state[\"retry\\_count\"] > 3: return \"end\" elif state[\"current\\_tool\"] == \"search\": return \"process\\_search\" else: return \"call\\_llm\" workflow = StateGraph(AgentState) workflow.add\\_node(\"call\\_llm\", call\\_llm\\_node) workflow.add\\_node(\"process\\_search\", search\\_node) workflow.add\\_conditional\\_edges(\"call\\_llm\", should\\_continue) ``` \\*\\*2. State management across multiple components\\*\\* When you need to share and coordinate state between different parts of your workflow, the Graph API's explicit state management is beneficial. ```python theme={null} # Multiple nodes can access and modify shared state class WorkflowState(TypedDict): user\\_input: str search\\_results: list generated\\_response: str validation\\_status: str def search\\_node(state): # Access shared state results = search(state[\"user\\_input\"]) return {\"search\\_results\": results} def validation\\_node(state): # Access results from previous node is\\_valid = validate(state[\"generated\\_response\"]) return {\"validation\\_status\": \"valid\" if is\\_valid else \"invalid\"} ``` \\*\\*3. Parallel processing with synchronization\\*\\* When you need to run multiple operations in parallel and then combine their results, the Graph API handles this naturally. ```python theme={null} # Parallel processing of multiple data sources workflow.add\\_node(\"fetch\\_news\", fetch\\_news) workflow.add\\_node(\"fetch\\_weather\", fetch\\_weather) workflow.add\\_node(\"fetch\\_stocks\", fetch\\_stocks) workflow.add\\_node(\"combine\\_data\", combine\\_all\\_data) # All fetch operations run in parallel workflow.add\\_edge(START, \"fetch\\_news\") workflow.add\\_edge(START, \"fetch\\_weather\") workflow.add\\_edge(START, \"fetch\\_stocks\") # Combine waits for all parallel operations to complete workflow.add\\_edge(\"fetch\\_news\", \"combine\\_data\") workflow.add\\_edge(\"fetch\\_weather\", \"combine\\_data\") workflow.add\\_edge(\"fetch\\_stocks\", \"combine\\_data\") ``` \\*\\*4. Team development and documentation\\*\\* The visual nature of the Graph API makes it easier for teams to understand, document, and maintain complex workflows. ```python theme={null} # Clear separation of concerns - each team member can work on different nodes workflow.add\\_node(\"data\\_ingestion\", data\\_team\\_function) workflow.add\\_node(\"ml\\_processing\", ml\\_team\\_function) workflow.add\\_node(\"business\\_logic\", product\\_team\\_function) workflow.add\\_node(\"output\\_formatting\", frontend\\_team\\_function) ``` ### When to use the Functional API The [Functional API](/oss/python/langgraph/functional-api) uses an imperative approach that integrates LangGraph features into standard procedural code. \\*\\*1. Existing procedural code\\*\\* When you have existing code that uses standard control flow and want to add LangGraph features with minimal refactoring. ```python theme={null} # Functional API: Minimal changes to existing code from langgraph.func import entrypoint, task @task def process\\_user\\_input(user\\_input: str) -> dict: # Existing function with minimal changes return {\"processed\": user\\_input.lower().strip()} @entrypoint(checkpointer=checkpointer) def workflow(user\\_input: str) -> str: # Standard Python control flow processed = process\\_user\\_input(user\\_input).result() if \"urgent\" in processed[\"processed\"]: response = handle\\_urgent\\_request(processed).result() else: response = handle\\_normal\\_request(processed).result() return response ``` \\*\\*2. Linear workflows with simple logic\\*\\* When your workflow is primarily sequential with straightforward conditional logic. ```python theme={null} @entrypoint(checkpointer=checkpointer) def essay\\_workflow(topic: str) -> dict: # Linear flow with simple branching outline = create\\_outline(topic).result() if len(outline[\"points\"]) < 3: outline = expand\\_outline(outline).result() draft = write\\_draft(outline).result() # Human review checkpoint feedback = interrupt({\"draft\": draft, \"action\": \"Please review\"}) if feedback == \"approve\": final\\_essay = draft else: final\\_essay = revise\\_essay(draft, feedback).result() return {\"essay\": final\\_essay} ``` \\*\\*3. Rapid prototyping\\*\\* When you want to quickly test ideas without the overhead of defining state schemas and graph structures. ```python theme={null} @entrypoint(checkpointer=checkpointer) def quick\\_prototype(data: dict) -> dict: # Fast iteration - no state schema needed step1\\_result = process\\_step1(data).result() step2\\_result = process\\_step2(step1\\_result).result() return {\"final\\_result\": step2\\_result} ``` \\*\\*4. Function-scoped state management\\*\\* When your state is naturally scoped to individual functions and doesn't need to be shared broadly. ```python theme={null} @task def analyze\\_document(document: str) -> dict: # Local state management within function sections = extract\\_sections(document) summaries = [summarize(section) for section in sections] key\\_points = extract\\_key\\_points(summaries) return { \"sections\": len(sections), \"summaries\": summaries, \"key\\_points\": key\\_points } @entrypoint(checkpointer=checkpointer) def document\\_processor(document: str) -> dict: analysis = analyze\\_document(document).result() # State is passed between functions as needed return generate\\_report(analysis).result() ``` ## Combining both APIs You can use both APIs together in the same application. This is useful when different parts of your system have different requirements. ```python theme={null} from langgraph.graph import StateGraph from langgraph.func import entrypoint # Complex multi-agent coordination using Graph API coordination\\_graph = StateGraph(CoordinationState) coordination\\_graph.add\\_node(\"orchestrator\", orchestrator\\_node) coordination\\_graph.add\\_node(\"agent\\_a\", agent\\_a\\_node) coordination\\_graph.add\\_node(\"agent\\_b\", agent\\_b\\_node) # Simple data processing using Functional API @entrypoint() def data\\_processor(raw\\_data: dict) -> dict: cleaned = clean\\_data(raw\\_data).result() transformed = transform\\_data(cleaned).result() return transformed # Use the functional API result in the graph def orchestrator\\_node(state): processed\\_data = data\\_processor.invoke(state[\"raw\\_data\"]) return {\"processed\\_data\": processed\\_data} ``` ## Migration between APIs ### From Functional to Graph API When your functional workflow grows complex, you can migrate to the Graph API: ```python theme={null} # Before: Functional API @entrypoint(checkpointer=checkpointer) def complex\\_workflow(input\\_data: dict) -> dict: step1 = process\\_step1(input\\_data).result() if step1[\"needs\\_analysis\"]: analysis = analyze\\_data(step1).result() if analysis[\"confidence\"] > 0.8: result = high\\_confidence\\_path(analysis).result() else: result = low\\_confidence\\_path(analysis).result() else: result = simple\\_path(step1).result() return result # After: Graph API class WorkflowState(TypedDict): input\\_data: dict step1\\_result: dict analysis: dict final\\_result: dict def should\\_analyze(state): return \"analyze\" if state[\"step1\\_result\"][\"needs\\_analysis\"] else \"simple\\_path\" def confidence\\_check(state): return \"high\\_confidence\" if state[\"analysis\"][\"confidence\"] > 0.8 else \"low\\_confidence\" workflow = StateGraph(WorkflowState) workflow.add\\_node(\"step1\", process\\_step1\\_node) workflow.add\\_conditional\\_edges(\"step1\", should\\_analyze) workflow.add\\_node(\"analyze\", analyze\\_data\\_node) workflow.add\\_conditional\\_edges(\"analyze\", confidence\\_check) # ... add remaining nodes and edges ``` ### From Graph to Functional API When your graph becomes overly complex for simple linear processes: ```python theme={null} # Before: Over-engineered Graph API class SimpleState(TypedDict): input: str step1: str step2: str result: str # After: Simplified Functional API @entrypoint(checkpointer=checkpointer) def simple\\_workflow(input\\_data: str) -> str: step1 = process\\_step1(input\\_data).result() step2 = process\\_step2(step1).result() return finalize\\_result(step2).result() ``` ## Summary Choose the \\*\\*Graph API\\*\\* when you need explicit control over workflow structure, complex branching, parallel processing, or team collaboration benefits. Choose the \\*\\*Functional API\\*\\* when you want to add LangGraph features to existing code with minimal changes, have simple linear workflows, or need rapid prototyping capabilities. Both APIs provide the same core LangGraph features (persistence, streaming, human-in-the-loop, memory) but package them in different paradigms to suit different development styles and use cases. \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/choosing-apis.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/deploy.md",
      "title": null,
      "raw_content": "# LangSmith Deployment When you're ready to deploy your agent to production, LangSmith provides a managed hosting platform designed for agent workloads. Traditional hosting platforms are built for stateless, short-lived web applications, while LangGraph is \\*\\*purpose-built for stateful, long-running agents\\*\\* that require persistent state and background execution. LangSmith handles the infrastructure, scaling, and operational concerns so you can deploy directly from your repository. ## Prerequisites Before you begin, ensure you have the following: \\* A [GitHub account](https://github.com/) \\* A [LangSmith account](https://smith.langchain.com/) (free to sign up) ## Deploy your agent ### 1. Create a repository on GitHub Your application's code must reside in a GitHub repository to be deployed on LangSmith. Both public and private repositories are supported. For this quickstart, first make sure your app is LangGraph-compatible by following the [local server setup guide](/oss/python/langgraph/studio#setup-local-agent-server). Then, push your code to the repository. ### 2. Deploy to LangSmith   Log in to [LangSmith](https://smith.langchain.com/). In the left sidebar, select \\*\\*Deployments\\*\\*.   Click the \\*\\*+ New Deployment\\*\\* button. A pane will open where you can fill in the required fields.   If you are a first time user or adding a private repository that has not been previously connected, click the \\*\\*Add new account\\*\\* button and follow the instructions to connect your GitHub account.   Select your application's repository. Click \\*\\*Submit\\*\\* to deploy. This may take about 15 minutes to complete. You can check the status in the \\*\\*Deployment details\\*\\* view.   ### 3. Test your application in Studio Once your application is deployed: 1. Select the deployment you just created to view more details. 2. Click the \\*\\*Studio\\*\\* button in the top right corner. Studio will open to display your graph. ### 4. Get the API URL for your deployment 1. In the \\*\\*Deployment details\\*\\* view in LangGraph, click the \\*\\*API URL\\*\\* to copy it to your clipboard. 2. Click the `URL` to copy it to the clipboard. ### 5. Test the API You can now test the API:   1. Install LangGraph Python: ```shell theme={null} pip install langgraph-sdk ``` 2. Send a message to the agent: ```python theme={null} from langgraph\\_sdk import get\\_sync\\_client # or get\\_client for async client = get\\_sync\\_client(url=\"your-deployment-url\", api\\_key=\"your-langsmith-api-key\") for chunk in client.runs.stream( None, # Threadless run \"agent\", # Name of agent. Defined in langgraph.json. input={ \"messages\": [{ \"role\": \"human\", \"content\": \"What is LangGraph?\", }], }, stream\\_mode=\"updates\", ): print(f\"Receiving new event of type: {chunk.event}...\") print(chunk.data) print(\"\\n\\n\") ```   ```bash theme={null} curl -s --request POST \\ --url /runs/stream \\ --header 'Content-Type: application/json' \\ --header \"X-Api-Key:  \\ --data \"{ \\\"assistant\\_id\\\": \\\"agent\\\", `# Name of agent. Defined in langgraph.json.` \\\"input\\\": { \\\"messages\\\": [ { \\\"role\\\": \\\"human\\\", \\\"content\\\": \\\"What is LangGraph?\\\" } ] }, \\\"stream\\_mode\\\": \\\"updates\\\" }\" ```    LangSmith offers additional hosting options, including self-hosted and hybrid. For more information, please see the [Platform setup overview](/langsmith/platform-setup).  \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/deploy.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/durable-execution.md",
      "title": null,
      "raw_content": "# Durable execution \\*\\*Durable execution\\*\\* is a technique in which a process or workflow saves its progress at key points, allowing it to pause and later resume exactly where it left off. This is particularly useful in scenarios that require [human-in-the-loop](/oss/python/langgraph/interrupts), where users can inspect, validate, or modify the process before continuing, and in long-running tasks that might encounter interruptions or errors (e.g., calls to an LLM timing out). By preserving completed work, durable execution enables a process to resume without reprocessing previous steps -- even after a significant delay (e.g., a week later). LangGraph's built-in [persistence](/oss/python/langgraph/persistence) layer provides durable execution for workflows, ensuring that the state of each execution step is saved to a durable store. This capability guarantees that if a workflow is interrupted -- whether by a system failure or for [human-in-the-loop](/oss/python/langgraph/interrupts) interactions -- it can be resumed from its last recorded state.  If you are using LangGraph with a checkpointer, you already have durable execution enabled. You can pause and resume workflows at any point, even after interruptions or failures. To make the most of durable execution, ensure that your workflow is designed to be [deterministic](#determinism-and-consistent-replay) and [idempotent](#determinism-and-consistent-replay) and wrap any side effects or non-deterministic operations inside [tasks](/oss/python/langgraph/functional-api#task). You can use [tasks](/oss/python/langgraph/functional-api#task) from both the [StateGraph (Graph API)](/oss/python/langgraph/graph-api) and the [Functional API](/oss/python/langgraph/functional-api).  ## Requirements To leverage durable execution in LangGraph, you need to: 1. Enable [persistence](/oss/python/langgraph/persistence) in your workflow by specifying a [checkpointer](/oss/python/langgraph/persistence#checkpointer-libraries) that will save workflow progress. 2. Specify a [thread identifier](/oss/python/langgraph/persistence#threads) when executing a workflow. This will track the execution history for a particular instance of the workflow. 3. Wrap any non-deterministic operations (e.g., random number generation) or operations with side effects (e.g., file writes, API calls) inside [`task`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.task) to ensure that when a workflow is resumed, these operations are not repeated for the particular run, and instead their results are retrieved from the persistence layer. For more information, see [Determinism and Consistent Replay](#determinism-and-consistent-replay). ## Determinism and Consistent Replay When you resume a workflow run, the code does \\*\\*NOT\\*\\* resume from the \\*\\*same line of code\\*\\* where execution stopped; instead, it will identify an appropriate [starting point](#starting-points-for-resuming-workflows) from which to pick up where it left off. This means that the workflow will replay all steps from the [starting point](#starting-points-for-resuming-workflows) until it reaches the point where it was stopped. As a result, when you are writing a workflow for durable execution, you must wrap any non-deterministic operations (e.g., random number generation) and any operations with side effects (e.g., file writes, API calls) inside [tasks](/oss/python/langgraph/functional-api#task) or [nodes](/oss/python/langgraph/graph-api#nodes). To ensure that your workflow is deterministic and can be consistently replayed, follow these guidelines: \\* \\*\\*Avoid Repeating Work\\*\\*: If a [node](/oss/python/langgraph/graph-api#nodes) contains multiple operations with side effects (e.g., logging, file writes, or network calls), wrap each operation in a separate \\*\\*task\\*\\*. This ensures that when the workflow is resumed, the operations are not repeated, and their results are retrieved from the persistence layer. \\* \\*\\*Encapsulate Non-Deterministic Operations:\\*\\* Wrap any code that might yield non-deterministic results (e.g., random number generation) inside \\*\\*tasks\\*\\* or \\*\\*nodes\\*\\*. This ensures that, upon resumption, the workflow follows the exact recorded sequence of steps with the same outcomes. \\* \\*\\*Use Idempotent Operations\\*\\*: When possible ensure that side effects (e.g., API calls, file writes) are idempotent. This means that if an operation is retried after a failure in the workflow, it will have the same effect as the first time it was executed. This is particularly important for operations that result in data writes. In the event that a \\*\\*task\\*\\* starts but fails to complete successfully, the workflow's resumption will re-run the \\*\\*task\\*\\*, relying on recorded outcomes to maintain consistency. Use idempotency keys or verify existing results to avoid unintended duplication, ensuring a smooth and predictable workflow execution. For some examples of pitfalls to avoid, see the [Common Pitfalls](/oss/python/langgraph/functional-api#common-pitfalls) section in the functional API, which shows how to structure your code using \\*\\*tasks\\*\\* to avoid these issues. The same principles apply to the [StateGraph (Graph API)](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph). ## Durability modes LangGraph supports three durability modes that allow you to balance performance and data consistency based on your application's requirements. A higher durability mode adds more overhead to the workflow execution. You can specify the durability mode when calling any graph execution method: ```python theme={null} graph.stream( {\"input\": \"test\"}, durability=\"sync\" ) ``` The durability modes, from least to most durable, are as follows: \\* `\"exit\"`: Changes are persisted only when graph execution completes (either successfully or with an error). This provides the best performance for long-running graphs but means intermediate state is not saved, so you cannot recover from mid-execution failures or interrupt the graph execution. \\* `\"async\"`: Changes are persisted asynchronously while the next step executes. This provides good performance and durability, but there's a small risk that checkpoints might not be written if the process crashes during execution. \\* `\"sync\"`: Changes are persisted synchronously before the next step starts. This ensures that every checkpoint is written before continuing execution, providing high durability at the cost of some performance overhead. ## Using tasks in nodes If a [node](/oss/python/langgraph/graph-api#nodes) contains multiple operations, you may find it easier to convert each operation into a \\*\\*task\\*\\* rather than refactor the operations into individual nodes.   ```python theme={null} from typing import NotRequired from typing\\_extensions import TypedDict import uuid from langgraph.checkpoint.memory import InMemorySaver from langgraph.graph import StateGraph, START, END import requests # Define a TypedDict to represent the state class State(TypedDict): url: str result: NotRequired[str] def call\\_api(state: State): \"\"\"Example node that makes an API request.\"\"\" result = requests.get(state['url']).text[:100] # Side-effect # [!code highlight] return { \"result\": result } # Create a StateGraph builder and add a node for the call\\_api function builder = StateGraph(State) builder.add\\_node(\"call\\_api\", call\\_api) # Connect the start and end nodes to the call\\_api node builder.add\\_edge(START, \"call\\_api\") builder.add\\_edge(\"call\\_api\", END) # Specify a checkpointer checkpointer = InMemorySaver() # Compile the graph with the checkpointer graph = builder.compile(checkpointer=checkpointer) # Define a config with a thread ID. thread\\_id = uuid.uuid4() config = {\"configurable\": {\"thread\\_id\": thread\\_id}} # Invoke the graph graph.invoke({\"url\": \"https://www.example.com\"}, config) ```   ```python theme={null} from typing import NotRequired from typing\\_extensions import TypedDict import uuid from langgraph.checkpoint.memory import InMemorySaver from langgraph.func import task from langgraph.graph import StateGraph, START, END import requests # Define a TypedDict to represent the state class State(TypedDict): urls: list[str] result: NotRequired[list[str]] @task def \\_make\\_request(url: str): \"\"\"Make a request.\"\"\" return requests.get(url).text[:100] # [!code highlight] def call\\_api(state: State): \"\"\"Example node that makes an API request.\"\"\" requests = [\\_make\\_request(url) for url in state['urls']] # [!code highlight] results = [request.result() for request in requests] return { \"results\": results } # Create a StateGraph builder and add a node for the call\\_api function builder = StateGraph(State) builder.add\\_node(\"call\\_api\", call\\_api) # Connect the start and end nodes to the call\\_api node builder.add\\_edge(START, \"call\\_api\") builder.add\\_edge(\"call\\_api\", END) # Specify a checkpointer checkpointer = InMemorySaver() # Compile the graph with the checkpointer graph = builder.compile(checkpointer=checkpointer) # Define a config with a thread ID. thread\\_id = uuid.uuid4() config = {\"configurable\": {\"thread\\_id\": thread\\_id}} # Invoke the graph graph.invoke({\"urls\": [\"https://www.example.com\"]}, config) ```   ## Resuming Workflows Once you have enabled durable execution in your workflow, you can resume execution for the following scenarios: \\* \\*\\*Pausing and Resuming Workflows:\\*\\* Use the [interrupt](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) function to pause a workflow at specific points and the [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) primitive to resume it with updated state. See [\\*\\*Interrupts\\*\\*](/oss/python/langgraph/interrupts) for more details. \\* \\*\\*Recovering from Failures:\\*\\* Automatically resume workflows from the last successful checkpoint after an exception (e.g., LLM provider outage). This involves executing the workflow with the same thread identifier by providing it with a `None` as the input value (see this [example](/oss/python/langgraph/use-functional-api#resuming-after-an-error) with the functional API). ## Starting Points for Resuming Workflows \\* If you're using a [StateGraph (Graph API)](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph), the starting point is the beginning of the [\\*\\*node\\*\\*](/oss/python/langgraph/graph-api#nodes) where execution stopped. \\* If you're making a subgraph call inside a node, the starting point will be the \\*\\*parent\\*\\* node that called the subgraph that was halted. Inside the subgraph, the starting point will be the specific [\\*\\*node\\*\\*](/oss/python/langgraph/graph-api#nodes) where execution stopped. \\* If you're using the Functional API, the starting point is the beginning of the [\\*\\*entrypoint\\*\\*](/oss/python/langgraph/functional-api#entrypoint) where execution stopped. \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/durable-execution.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/functional-api.md",
      "title": null,
      "raw_content": "# Functional API overview The \\*\\*Functional API\\*\\* allows you to add LangGraph's key features \u2014 [persistence](/oss/python/langgraph/persistence), [memory](/oss/python/langgraph/add-memory), [human-in-the-loop](/oss/python/langgraph/interrupts), and [streaming](/oss/python/langgraph/streaming) \u2014 to your applications with minimal changes to your existing code. It is designed to integrate these features into existing code that may use standard language primitives for branching and control flow, such as `if` statements, `for` loops, and function calls. Unlike many data orchestration frameworks that require restructuring code into an explicit pipeline or DAG, the Functional API allows you to incorporate these capabilities without enforcing a rigid execution model. The Functional API uses two key building blocks: \\* \\*\\*`@entrypoint`\\*\\* \u2013 Marks a function as the starting point of a workflow, encapsulating logic and managing execution flow, including handling long-running tasks and interrupts. \\* \\*\\*[`@task`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.task)\\*\\* \u2013 Represents a discrete unit of work, such as an API call or data processing step, that can be executed asynchronously within an entrypoint. Tasks return a future-like object that can be awaited or resolved synchronously. This provides a minimal abstraction for building workflows with state management and streaming.  For information on how to use the functional API, see [Use Functional API](/oss/python/langgraph/use-functional-api).  ## Functional API vs. Graph API For users who prefer a more declarative approach, LangGraph's [Graph API](/oss/python/langgraph/graph-api) allows you to define workflows using a Graph paradigm. Both APIs share the same underlying runtime, so you can use them together in the same application. Here are some key differences: \\* \\*\\*Control flow\\*\\*: The Functional API does not require thinking about graph structure. You can use standard Python constructs to define workflows. This will usually trim the amount of code you need to write. \\* \\*\\*Short-term memory\\*\\*: The \\*\\*GraphAPI\\*\\* requires declaring a [\\*\\*State\\*\\*](/oss/python/langgraph/graph-api#state) and may require defining [\\*\\*reducers\\*\\*](/oss/python/langgraph/graph-api#reducers) to manage updates to the graph state. `@entrypoint` and `@tasks` do not require explicit state management as their state is scoped to the function and is not shared across functions. \\* \\*\\*Checkpointing\\*\\*: Both APIs generate and use checkpoints. In the \\*\\*Graph API\\*\\* a new checkpoint is generated after every [superstep](/oss/python/langgraph/graph-api). In the \\*\\*Functional API\\*\\*, when tasks are executed, their results are saved to an existing checkpoint associated with the given entrypoint instead of creating a new checkpoint. \\* \\*\\*Visualization\\*\\*: The Graph API makes it easy to visualize the workflow as a graph which can be useful for debugging, understanding the workflow, and sharing with others. The Functional API does not support visualization as the graph is dynamically generated during runtime. ## Example Below we demonstrate a simple application that writes an essay and [interrupts](/oss/python/langgraph/interrupts) to request human review. ```python theme={null} from langgraph.checkpoint.memory import InMemorySaver from langgraph.func import entrypoint, task from langgraph.types import interrupt @task def write\\_essay(topic: str) -> str: \"\"\"Write an essay about the given topic.\"\"\" time.sleep(1) # A placeholder for a long-running task. return f\"An essay about topic: {topic}\" @entrypoint(checkpointer=InMemorySaver()) def workflow(topic: str) -> dict: \"\"\"A simple workflow that writes an essay and asks for a review.\"\"\" essay = write\\_essay(\"cat\").result() is\\_approved = interrupt({ # Any json-serializable payload provided to interrupt as argument. # It will be surfaced on the client side as an Interrupt when streaming data # from the workflow. \"essay\": essay, # The essay we want reviewed. # We can add any additional information that we need. # For example, introduce a key called \"action\" with some instructions. \"action\": \"Please approve/reject the essay\", }) return { \"essay\": essay, # The essay that was generated \"is\\_approved\": is\\_approved, # Response from HIL } ```  This workflow will write an essay about the topic \"cat\" and then pause to get a review from a human. The workflow can be interrupted for an indefinite amount of time until a review is provided. When the workflow is resumed, it executes from the very start, but because the result of the `writeEssay` task was already saved, the task result will be loaded from the checkpoint instead of being recomputed. ```python theme={null} import time import uuid from langgraph.func import entrypoint, task from langgraph.types import interrupt from langgraph.checkpoint.memory import InMemorySaver @task def write\\_essay(topic: str) -> str: \"\"\"Write an essay about the given topic.\"\"\" time.sleep(1) # This is a placeholder for a long-running task. return f\"An essay about topic: {topic}\" @entrypoint(checkpointer=InMemorySaver()) def workflow(topic: str) -> dict: \"\"\"A simple workflow that writes an essay and asks for a review.\"\"\" essay = write\\_essay(\"cat\").result() is\\_approved = interrupt( { # Any json-serializable payload provided to interrupt as argument. # It will be surfaced on the client side as an Interrupt when streaming data # from the workflow. \"essay\": essay, # The essay we want reviewed. # We can add any additional information that we need. # For example, introduce a key called \"action\" with some instructions. \"action\": \"Please approve/reject the essay\", } ) return { \"essay\": essay, # The essay that was generated \"is\\_approved\": is\\_approved, # Response from HIL } thread\\_id = str(uuid.uuid4()) config = {\"configurable\": {\"thread\\_id\": thread\\_id}} for item in workflow.stream(\"cat\", config): print(item) # > {'write\\_essay': 'An essay about topic: cat'} # > { # > '\\_\\_interrupt\\_\\_': ( # > Interrupt( # > value={ # > 'essay': 'An essay about topic: cat', # > 'action': 'Please approve/reject the essay' # > }, # > id='b9b2b9d788f482663ced6dc755c9e981' # > ), # > ) # > } ``` An essay has been written and is ready for review. Once the review is provided, we can resume the workflow: ```python theme={null} from langgraph.types import Command # Get review from a user (e.g., via a UI) # In this case, we're using a bool, but this can be any json-serializable value. human\\_review = True for item in workflow.stream(Command(resume=human\\_review), config): print(item) ``` ```pycon theme={null} {'workflow': {'essay': 'An essay about topic: cat', 'is\\_approved': False}} ``` The workflow has been completed and the review has been added to the essay.  ## Entrypoint The [`@entrypoint`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.entrypoint) decorator can be used to create a workflow from a function. It encapsulates workflow logic and manages execution flow, including handling \\*long-running tasks\\* and [interrupts](/oss/python/langgraph/interrupts). ### Definition An \\*\\*entrypoint\\*\\* is defined by decorating a function with the `@entrypoint` decorator. The function \\*\\*must accept a single positional argument\\*\\*, which serves as the workflow input. If you need to pass multiple pieces of data, use a dictionary as the input type for the first argument. Decorating a function with an `entrypoint` produces a [`Pregel`](https://reference.langchain.com/python/langgraph/pregel/#langgraph.pregel.Pregel.stream) instance which helps to manage the execution of the workflow (e.g., handles streaming, resumption, and checkpointing). You will usually want to pass a \\*\\*checkpointer\\*\\* to the `@entrypoint` decorator to enable persistence and use features like \\*\\*human-in-the-loop\\*\\*.   ```python theme={null} from langgraph.func import entrypoint @entrypoint(checkpointer=checkpointer) def my\\_workflow(some\\_input: dict) -> int: # some logic that may involve long-running tasks like API calls, # and may be interrupted for human-in-the-loop. ... return result ```   ```python theme={null} from langgraph.func import entrypoint @entrypoint(checkpointer=checkpointer) async def my\\_workflow(some\\_input: dict) -> int: # some logic that may involve long-running tasks like API calls, # and may be interrupted for human-in-the-loop ... return result ```    \\*\\*Serialization\\*\\* The \\*\\*inputs\\*\\* and \\*\\*outputs\\*\\* of entrypoints must be JSON-serializable to support checkpointing. Please see the [serialization](#serialization) section for more details.  ### Injectable parameters When declaring an `entrypoint`, you can request access to additional parameters that will be injected automatically at run time. These parameters include: | Parameter | Description | | ------------ | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | | \\*\\*previous\\*\\* | Access the state associated with the previous `checkpoint` for the given thread. See [short-term-memory](#short-term-memory). | | \\*\\*store\\*\\* | An instance of \\[BaseStore]\\[langgraph.store.base.BaseStore]. Useful for [long-term memory](/oss/python/langgraph/use-functional-api#long-term-memory). | | \\*\\*writer\\*\\* | Use to access the StreamWriter when working with Async Python \\< 3.11. See [streaming with functional API for details](/oss/python/langgraph/use-functional-api#streaming). | | \\*\\*config\\*\\* | For accessing run time configuration. See [RunnableConfig](https://python.langchain.com/docs/concepts/runnables/#runnableconfig) for information. |  Declare the parameters with the appropriate name and type annotation.   ```python theme={null} from langchain\\_core.runnables import RunnableConfig from langgraph.func import entrypoint from langgraph.store.base import BaseStore from langgraph.store.memory import InMemoryStore in\\_memory\\_store = InMemoryStore(...) # An instance of InMemoryStore for long-term memory @entrypoint( checkpointer=checkpointer, # Specify the checkpointer store=in\\_memory\\_store # Specify the store ) def my\\_workflow( some\\_input: dict, # The input (e.g., passed via `invoke`) \\*, previous: Any = None, # For short-term memory store: BaseStore, # For long-term memory writer: StreamWriter, # For streaming custom data config: RunnableConfig # For accessing the configuration passed to the entrypoint ) -> ...: ```  ### Executing Using the [`@entrypoint`](#entrypoint) yields a [`Pregel`](https://reference.langchain.com/python/langgraph/pregel/#langgraph.pregel.Pregel.stream) object that can be executed using the `invoke`, `ainvoke`, `stream`, and `astream` methods.   ```python theme={null} config = { \"configurable\": { \"thread\\_id\": \"some\\_thread\\_id\" } } my\\_workflow.invoke(some\\_input, config) # Wait for the result synchronously ```   ```python theme={null} config = { \"configurable\": { \"thread\\_id\": \"some\\_thread\\_id\" } } await my\\_workflow.ainvoke(some\\_input, config) # Await result asynchronously ```   ```python theme={null} config = { \"configurable\": { \"thread\\_id\": \"some\\_thread\\_id\" } } for chunk in my\\_workflow.stream(some\\_input, config): print(chunk) ```   ```python theme={null} config = { \"configurable\": { \"thread\\_id\": \"some\\_thread\\_id\" } } async for chunk in my\\_workflow.astream(some\\_input, config): print(chunk) ```   ### Resuming Resuming an execution after an [interrupt](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) can be done by passing a \\*\\*resume\\*\\* value to the [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) primitive.   ```python theme={null} from langgraph.types import Command config = { \"configurable\": { \"thread\\_id\": \"some\\_thread\\_id\" } } my\\_workflow.invoke(Command(resume=some\\_resume\\_value), config) ```   ```python theme={null} from langgraph.types import Command config = { \"configurable\": { \"thread\\_id\": \"some\\_thread\\_id\" } } await my\\_workflow.ainvoke(Command(resume=some\\_resume\\_value), config) ```   ```python theme={null} from langgraph.types import Command config = { \"configurable\": { \"thread\\_id\": \"some\\_thread\\_id\" } } for chunk in my\\_workflow.stream(Command(resume=some\\_resume\\_value), config): print(chunk) ```   ```python theme={null} from langgraph.types import Command config = { \"configurable\": { \"thread\\_id\": \"some\\_thread\\_id\" } } async for chunk in my\\_workflow.astream(Command(resume=some\\_resume\\_value), config): print(chunk) ```   \\*\\*Resuming after an error\\*\\* To resume after an error, run the `entrypoint` with a `None` and the same \\*\\*thread id\\*\\* (config). This assumes that the underlying \\*\\*error\\*\\* has been resolved and execution can proceed successfully.   ```python theme={null} config = { \"configurable\": { \"thread\\_id\": \"some\\_thread\\_id\" } } my\\_workflow.invoke(None, config) ```   ```python theme={null} config = { \"configurable\": { \"thread\\_id\": \"some\\_thread\\_id\" } } await my\\_workflow.ainvoke(None, config) ```   ```python theme={null} config = { \"configurable\": { \"thread\\_id\": \"some\\_thread\\_id\" } } for chunk in my\\_workflow.stream(None, config): print(chunk) ```   ```python theme={null} config = { \"configurable\": { \"thread\\_id\": \"some\\_thread\\_id\" } } async for chunk in my\\_workflow.astream(None, config): print(chunk) ```   ### Short-term memory When an `entrypoint` is defined with a `checkpointer`, it stores information between successive invocations on the same \\*\\*thread id\\*\\* in [checkpoints](/oss/python/langgraph/persistence#checkpoints). This allows accessing the state from the previous invocation using the `previous` parameter. By default, the `previous` parameter is the return value of the previous invocation. ```python theme={null} @entrypoint(checkpointer=checkpointer) def my\\_workflow(number: int, \\*, previous: Any = None) -> int: previous = previous or 0 return number + previous config = { \"configurable\": { \"thread\\_id\": \"some\\_thread\\_id\" } } my\\_workflow.invoke(1, config) # 1 (previous was None) my\\_workflow.invoke(2, config) # 3 (previous was 1 from the previous invocation) ``` #### `entrypoint.final` [`entrypoint.final`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.entrypoint.final) is a special primitive that can be returned from an entrypoint and allows \\*\\*decoupling\\*\\* the value that is \\*\\*saved in the checkpoint\\*\\* from the \\*\\*return value of the entrypoint\\*\\*. The first value is the return value of the entrypoint, and the second value is the value that will be saved in the checkpoint. The type annotation is `entrypoint.final[return\\_type, save\\_type]`. ```python theme={null} @entrypoint(checkpointer=checkpointer) def my\\_workflow(number: int, \\*, previous: Any = None) -> entrypoint.final[int, int]: previous = previous or 0 # This will return the previous value to the caller, saving # 2 \\* number to the checkpoint, which will be used in the next invocation # for the `previous` parameter. return entrypoint.final(value=previous, save=2 \\* number) config = { \"configurable\": { \"thread\\_id\": \"1\" } } my\\_workflow.invoke(3, config) # 0 (previous was None) my\\_workflow.invoke(1, config) # 6 (previous was 3 \\* 2 from the previous invocation) ``` ## Task A \\*\\*task\\*\\* represents a discrete unit of work, such as an API call or data processing step. It has two key characteristics: \\* \\*\\*Asynchronous Execution\\*\\*: Tasks are designed to be executed asynchronously, allowing multiple operations to run concurrently without blocking. \\* \\*\\*Checkpointing\\*\\*: Task results are saved to a checkpoint, enabling resumption of the workflow from the last saved state. (See [persistence](/oss/python/langgraph/persistence) for more details). ### Definition Tasks are defined using the `@task` decorator, which wraps a regular Python function. ```python theme={null} from langgraph.func import task @task() def slow\\_computation(input\\_value): # Simulate a long-running operation ... return result ```  \\*\\*Serialization\\*\\* The \\*\\*outputs\\*\\* of tasks must be JSON-serializable to support checkpointing.  ### Execution \\*\\*Tasks\\*\\* can only be called from within an \\*\\*entrypoint\\*\\*, another \\*\\*task\\*\\*, or a [state graph node](/oss/python/langgraph/graph-api#nodes). Tasks \\*cannot\\* be called directly from the main application code. When you call a \\*\\*task\\*\\*, it returns \\*immediately\\* with a future object. A future is a placeholder for a result that will be available later. To obtain the result of a \\*\\*task\\*\\*, you can either wait for it synchronously (using `result()`) or await it asynchronously (using `await`).   ```python theme={null} @entrypoint(checkpointer=checkpointer) def my\\_workflow(some\\_input: int) -> int: future = slow\\_computation(some\\_input) return future.result() # Wait for the result synchronously ```   ```python theme={null} @entrypoint(checkpointer=checkpointer) async def my\\_workflow(some\\_input: int) -> int: return await slow\\_computation(some\\_input) # Await result asynchronously ```   ## When to use a task \\*\\*Tasks\\*\\* are useful in the following scenarios: \\* \\*\\*Checkpointing\\*\\*: When you need to save the result of a long-running operation to a checkpoint, so you don't need to recompute it when resuming the workflow. \\* \\*\\*Human-in-the-loop\\*\\*: If you're building a workflow that requires human intervention, you MUST use \\*\\*tasks\\*\\* to encapsulate any randomness (e.g., API calls) to ensure that the workflow can be resumed correctly. See the [determinism](#determinism) section for more details. \\* \\*\\*Parallel Execution\\*\\*: For I/O-bound tasks, \\*\\*tasks\\*\\* enable parallel execution, allowing multiple operations to run concurrently without blocking (e.g., calling multiple APIs). \\* \\*\\*Observability\\*\\*: Wrapping operations in \\*\\*tasks\\*\\* provides a way to track the progress of the workflow and monitor the execution of individual operations using [LangSmith](https://docs.smith.langchain.com/). \\* \\*\\*Retryable Work\\*\\*: When work needs to be retried to handle failures or inconsistencies, \\*\\*tasks\\*\\* provide a way to encapsulate and manage the retry logic. ## Serialization There are two key aspects to serialization in LangGraph: 1. `entrypoint` inputs and outputs must be JSON-serializable. 2. `task` outputs must be JSON-serializable. These requirements are necessary for enabling checkpointing and workflow resumption. Use python primitives like dictionaries, lists, strings, numbers, and booleans to ensure that your inputs and outputs are serializable. Serialization ensures that workflow state, such as task results and intermediate values, can be reliably saved and restored. This is critical for enabling human-in-the-loop interactions, fault tolerance, and parallel execution. Providing non-serializable inputs or outputs will result in a runtime error when a workflow is configured with a checkpointer. ## Determinism To utilize features like \\*\\*human-in-the-loop\\*\\*, any randomness should be encapsulated inside of \\*\\*tasks\\*\\*. This guarantees that when execution is halted (e.g., for human in the loop) and then resumed, it will follow the same \\*sequence of steps\\*, even if \\*\\*task\\*\\* results are non-deterministic. LangGraph achieves this behavior by persisting \\*\\*task\\*\\* and [\\*\\*subgraph\\*\\*](/oss/python/langgraph/use-subgraphs) results as they execute. A well-designed workflow ensures that resuming execution follows the \\*same sequence of steps\\*, allowing previously computed results to be retrieved correctly without having to re-execute them. This is particularly useful for long-running \\*\\*tasks\\*\\* or \\*\\*tasks\\*\\* with non-deterministic results, as it avoids repeating previously done work and allows resuming from essentially the same. While different runs of a workflow can produce different results, resuming a \\*\\*specific\\*\\* run should always follow the same sequence of recorded steps. This allows LangGraph to efficiently look up \\*\\*task\\*\\* and \\*\\*subgraph\\*\\* results that were executed prior to the graph being interrupted and avoid recomputing them. ## Idempotency Idempotency ensures that running the same operation multiple times produces the same result. This helps prevent duplicate API calls and redundant processing if a step is rerun due to a failure. Always place API calls inside \\*\\*tasks\\*\\* functions for checkpointing, and design them to be idempotent in case of re-execution. Re-execution can occur if a \\*\\*task\\*\\* starts, but does not complete successfully. Then, if the workflow is resumed, the \\*\\*task\\*\\* will run again. Use idempotency keys or verify existing results to avoid duplication. ## Common Pitfalls ### Handling side effects Encapsulate side effects (e.g., writing to a file, sending an email) in tasks to ensure they are not executed multiple times when resuming a workflow.   In this example, a side effect (writing to a file) is directly included in the workflow, so it will be executed a second time when resuming the workflow. ```python theme={null} @entrypoint(checkpointer=checkpointer) def my\\_workflow(inputs: dict) -> int: # This code will be executed a second time when resuming the workflow. # Which is likely not what you want. with open(\"output.txt\", \"w\") as f: # [!code highlight] f.write(\"Side effect executed\") # [!code highlight] value = interrupt(\"question\") return value ```   In this example, the side effect is encapsulated in a task, ensuring consistent execution upon resumption. ```python theme={null} from langgraph.func import task @task # [!code highlight] def write\\_to\\_file(): # [!code highlight] with open(\"output.txt\", \"w\") as f: f.write(\"Side effect executed\") @entrypoint(checkpointer=checkpointer) def my\\_workflow(inputs: dict) -> int: # The side effect is now encapsulated in a task. write\\_to\\_file().result() value = interrupt(\"question\") return value ```   ### Non-deterministic control flow Operations that might give different results each time (like getting current time or random numbers) should be encapsulated in tasks to ensure that on resume, the same result is returned. \\* In a task: Get random number (5) \u2192 interrupt \u2192 resume \u2192 (returns 5 again) \u2192 ... \\* Not in a task: Get random number (5) \u2192 interrupt \u2192 resume \u2192 get new random number (7) \u2192 ... This is especially important when using \\*\\*human-in-the-loop\\*\\* workflows with multiple interrupts calls. LangGraph keeps a list of resume values for each task/entrypoint. When an interrupt is encountered, it's matched with the corresponding resume value. This matching is strictly \\*\\*index-based\\*\\*, so the order of the resume values should match the order of the interrupts. If order of execution is not maintained when resuming, one [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) call may be matched with the wrong `resume` value, leading to incorrect results. Please read the section on [determinism](#determinism) for more details.   In this example, the workflow uses the current time to determine which task to execute. This is non-deterministic because the result of the workflow depends on the time at which it is executed. ```python theme={null} from langgraph.func import entrypoint @entrypoint(checkpointer=checkpointer) def my\\_workflow(inputs: dict) -> int: t0 = inputs[\"t0\"] t1 = time.time() # [!code highlight] delta\\_t = t1 - t0 if delta\\_t > 1: result = slow\\_task(1).result() value = interrupt(\"question\") else: result = slow\\_task(2).result() value = interrupt(\"question\") return { \"result\": result, \"value\": value } ```   In this example, the workflow uses the input `t0` to determine which task to execute. This is deterministic because the result of the workflow depends only on the input. ```python theme={null} import time from langgraph.func import task @task # [!code highlight] def get\\_time() -> float: # [!code highlight] return time.time() @entrypoint(checkpointer=checkpointer) def my\\_workflow(inputs: dict) -> int: t0 = inputs[\"t0\"] t1 = get\\_time().result() # [!code highlight] delta\\_t = t1 - t0 if delta\\_t > 1: result = slow\\_task(1).result() value = interrupt(\"question\") else: result = slow\\_task(2).result() value = interrupt(\"question\") return { \"result\": result, \"value\": value } ```   \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/functional-api.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/graph-api.md",
      "title": "You can then create edges to/from this node by referencing it as `\"my_node\"",
      "raw_content": "```\n# Graph API overview\n\n<Warning>\n  **Alpha Notice:** These docs cover the [**v1-alpha**](../releases/langchain-v1) release. Content is incomplete and subject to change.\n\n  For the latest stable version, see the current [LangGraph Python](https://langchain-ai.github.io/langgraph/) or [LangGraph JavaScript](https://langchain-ai.github.io/langgraphjs/) docs.\n</Warning>\n\n## Graphs\n\nAt its core, LangGraph models agent workflows as graphs. You define the behavior of your agents using three key components:\n\n1. [`State`](#state): A shared data structure that represents the current snapshot of your application. It can be any data type, but is typically defined using a shared state schema.\n\n2. [`Nodes`](#nodes): Functions that encode the logic of your agents. They receive the current state as input, perform some computation or side-effect, and return an updated state.\n\n3. [`Edges`](#edges): Functions that determine which `Node` to execute next based on the current state. They can be conditional branches or fixed transitions.\n\nBy composing `Nodes` and `Edges`, you can create complex, looping workflows that evolve the state over time. The real power, though, comes from how LangGraph manages that state. To emphasize: `Nodes` and `Edges` are nothing more than functions - they can contain an LLM or just good ol' code.\n\nIn short: *nodes do the work, edges tell what to do next*.\n\nLangGraph's underlying graph algorithm uses [message passing](https://en.wikipedia.org/wiki/Message_passing) to define a general program. When a Node completes its operation, it sends messages along one or more edges to other node(s). These recipient nodes then execute their functions, pass the resulting messages to the next set of nodes, and the process continues. Inspired by Google's [Pregel](https://research.google/pubs/pregel-a-system-for-large-scale-graph-processing/) system, the program proceeds in discrete \"super-steps.\"\n\nA super-step can be considered a single iteration over the graph nodes. Nodes that run in parallel are part of the same super-step, while nodes that run sequentially belong to separate super-steps. At the start of graph execution, all nodes begin in an `inactive` state. A node becomes `active` when it receives a new message (state) on any of its incoming edges (or \"channels\"). The active node then runs its function and responds with updates. At the end of each super-step, nodes with no incoming messages vote to `halt` by marking themselves as `inactive`. The graph execution terminates when all nodes are `inactive` and no messages are in transit.\n\n### StateGraph\n\nThe `StateGraph` class is the main graph class to use. This is parameterized by a user defined `State` object.\n\n### Compiling your graph\n\nTo build your graph, you first define the [state](#state), you then add [nodes](#nodes) and [edges](#edges), and then you compile it. What exactly is compiling your graph and why is it needed?\n\nCompiling is a pretty simple step. It provides a few basic checks on the structure of your graph (no orphaned nodes, etc). It is also where you can specify runtime args like [checkpointers](/oss/python/langgraph/persistence) and breakpoints. You compile your graph by just calling the `.compile` method:\n\n```python\ngraph = graph_builder.compile(...)\n```\n\nYou **MUST** compile your graph before you can use it.\n\n## State\n\nThe first thing you do when you define a graph is define the `State` of the graph. The `State` consists of the [schema of the graph](#schema) as well as [`reducer` functions](#reducers) which specify how to apply updates to the state. The schema of the `State` will be the input schema to all `Nodes` and `Edges` in the graph, and can be either a `TypedDict` or a `Pydantic` model. All `Nodes` will emit updates to the `State` which are then applied using the specified `reducer` function.\n\n### Schema\n\nThe main documented way to specify the schema of a graph is by using a [`TypedDict`](https://docs.python.org/3/library/typing.html#typing.TypedDict). If you want to provide default values in your state, use a [`dataclass`](https://docs.python.org/3/library/dataclasses.html). We also support using a Pydantic [BaseModel](/oss/python/langgraph/graph-api.md#use-pydantic-models-for-graph-state) as your graph state if you want recursive data validation (though note that pydantic is less performant than a `TypedDict` or `dataclass`).\n\nBy default, the graph will have the same input and output schemas. If you want to change this, you can also specify explicit input and output schemas directly. This is useful when you have a lot of keys, and some are explicitly for input and others for output. See the [guide here](/oss/python/langgraph/graph-api.md#define-input-and-output-schemas) for how to use.\n\n#### Multiple schemas\n\nTypically, all graph nodes communicate with a single schema. This means that they will read and write to the same state channels. But, there are cases where we want more control over this:\n\n* Internal nodes can pass information that is not required in the graph's input / output.\n* We may also want to use different input / output schemas for the graph. The output might, for example, only contain a single relevant output key.\n\nIt is possible to have nodes write to private state channels inside the graph for internal node communication. We can simply define a private schema, `PrivateState`.\n\nIt is also possible to define explicit input and output schemas for a graph. In these cases, we define an \"internal\" schema that contains *all* keys relevant to graph operations. But, we also define `input` and `output` schemas that are sub-sets of the \"internal\" schema to constrain the input and output of the graph. See [this guide](/oss/python/langgraph/graph-api.md#define-input-and-output-schemas) for more detail.\n\nLet's look at an example:\n\n```python\nclass InputState(TypedDict):\n    user_input: str\n\nclass OutputState(TypedDict):\n    graph_output: str\n\nclass OverallState(TypedDict):\n    foo: str\n    user_input: str\n    graph_output: str\n\nclass PrivateState(TypedDict):\n    bar: str\n\ndef node_1(state: InputState) -> OverallState:\n    # Write to OverallState\n    return {\"foo\": state[\"user_input\"] + \" name\"}\n\ndef node_2(state: OverallState) -> PrivateState:\n    # Read from OverallState, write to PrivateState\n    return {\"bar\": state[\"foo\"] + \" is\"}\n\ndef node_3(state: PrivateState) -> OutputState:\n    # Read from PrivateState, write to OutputState\n    return {\"graph_output\": state[\"bar\"] + \" Lance\"}\n\nbuilder = StateGraph(OverallState,input_schema=InputState,output_schema=OutputState)\nbuilder.add_node(\"node_1\", node_1)\nbuilder.add_node(\"node_2\", node_2)\nbuilder.add_node(\"node_3\", node_3)\nbuilder.add_edge(START, \"node_1\")\nbuilder.add_edge(\"node_1\", \"node_2\")\nbuilder.add_edge(\"node_2\", \"node_3\")\nbuilder.add_edge(\"node_3\", END)\n\ngraph = builder.compile()\ngraph.invoke({\"user_input\":\"My\"})\n# {'graph_output': 'My name is Lance'}\n```\n\nThere are two subtle and important points to note here:\n\n1. We pass `state: InputState` as the input schema to `node_1`. But, we write out to `foo`, a channel in `OverallState`. How can we write out to a state channel that is not included in the input schema? This is because a node *can write to any state channel in the graph state.* The graph state is the union of the state channels defined at initialization, which includes `OverallState` and the filters `InputState` and `OutputState`.\n\n2. We initialize the graph with `StateGraph(OverallState,input_schema=InputState,output_schema=OutputState)`. So, how can we write to `PrivateState` in `node_2`? How does the graph gain access to this schema if it was not passed in the `StateGraph` initialization? We can do this because *nodes can also declare additional state channels* as long as the state schema definition exists. In this case, the `PrivateState` schema is defined, so we can add `bar` as a new state channel in the graph and write to it.\n\n### Reducers\n\nReducers are key to understanding how updates from nodes are applied to the `State`. Each key in the `State` has its own independent reducer function. If no reducer function is explicitly specified then it is assumed that all updates to that key should override it. There are a few different types of reducers, starting with the default type of reducer:\n\n#### Default Reducer\n\nThese two examples show how to use the default reducer:\n\n**Example A:**\n\n```python\nfrom typing_extensions import TypedDict\n\nclass State(TypedDict):\n    foo: int\n    bar: list[str]\n```\n\nIn this example, no reducer functions are specified for any key. Let's assume the input to the graph is:\n\n`{\"foo\": 1, \"bar\": [\"hi\"]}`. Let's then assume the first `Node` returns `{\"foo\": 2}`. This is treated as an update to the state. Notice that the `Node` does not need to return the whole `State` schema - just an update. After applying this update, the `State` would then be `{\"foo\": 2, \"bar\": [\"hi\"]}`. If the second node returns `{\"bar\": [\"bye\"]}` then the `State` would then be `{\"foo\": 2, \"bar\": [\"bye\"]}`\n\n**Example B:**\n\n```python\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\nfrom operator import add\n\nclass State(TypedDict):\n    foo: int\n    bar: Annotated[list[str], add]\n```\n\nIn this example, we've used the `Annotated` type to specify a reducer function (`operator.add`) for the second key (`bar`). Note that the first key remains unchanged. Let's assume the input to the graph is `{\"foo\": 1, \"bar\": [\"hi\"]}`. Let's then assume the first `Node` returns `{\"foo\": 2}`. This is treated as an update to the state. Notice that the `Node` does not need to return the whole `State` schema - just an update. After applying this update, the `State` would then be `{\"foo\": 2, \"bar\": [\"hi\"]}`. If the second node returns `{\"bar\": [\"bye\"]}` then the `State` would then be `{\"foo\": 2, \"bar\": [\"hi\", \"bye\"]}`. Notice here that the `bar` key is updated by adding the two lists together.\n\n### Working with Messages in Graph State\n\n#### Why use messages?\n\nMost modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain's [`ChatModel`](https://python.langchain.com/docs/concepts/#chat-models) in particular accepts a list of `Message` objects as inputs. These messages come in a variety of forms such as `HumanMessage` (user input) or `AIMessage` (LLM response). To read more about what message objects are, please refer to [this](https://python.langchain.com/docs/concepts/#messages) conceptual guide.\n\n#### Using Messages in your Graph\n\nIn many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of `Message` objects and annotate it with a reducer function (see `messages` key in the example below). The reducer function is vital to telling the graph how to update the list of `Message` objects in the state with each state update (for example, when a node sends an update). If you don't specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use `operator.add` as a reducer.\n\nHowever, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use `operator.add`, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt `add_messages` function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly.\n\n#### Serialization\n\nIn addition to keeping track of message IDs, the `add_messages` function will also try to deserialize messages into LangChain `Message` objects whenever a state update is received on the `messages` channel. See more information on LangChain serialization/deserialization [here](https://python.langchain.com/docs/how_to/serialization/). This allows sending graph inputs / state updates in the following format:\n\n```python\n# this is supported\n{\"messages\": [HumanMessage(content=\"message\")]}\n\n# and this is also supported\n{\"messages\": [{\"type\": \"human\", \"content\": \"message\"}]}\n```\n\nSince the state updates are always deserialized into LangChain `Messages` when using `add_messages`, you should use dot notation to access message attributes, like `state[\"messages\"][-1].content`. Below is an example of a graph that uses `add_messages` as its reducer function.\n\n```python\nfrom langchain_core.messages import AnyMessage\nfrom langgraph.graph.message import add_messages\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\n\nclass GraphState(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]\n```\n\n#### MessagesState\n\nSince having a list of messages in your state is so common, there exists a prebuilt state called `MessagesState` which makes it easy to use messages. `MessagesState` is defined with a single `messages` key which is a list of `AnyMessage` objects and uses the `add_messages` reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like:\n\n```python\nfrom langgraph.graph import MessagesState\n\nclass State(MessagesState):\n    documents: list[str]\n```\n\n## Nodes\n\nIn LangGraph, nodes are Python functions (either synchronous or asynchronous) that accept the following arguments:\n\n1. `state`: The [state](#state) of the graph\n2. `config`: A `RunnableConfig` object that contains configuration information like `thread_id` and tracing information like `tags`\n3. `runtime`: A `Runtime` object that contains [runtime `context`](#runtime-context) and other information like `store` and `stream_writer`\n\nSimilar to `NetworkX`, you add these nodes to a graph using the [add\\_node](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_node) method:\n\n```python\nfrom dataclasses import dataclass\nfrom typing_extensions import TypedDict\n\nfrom langchain_core.runnables import RunnableConfig\nfrom langgraph.graph import StateGraph\nfrom langgraph.runtime import Runtime\n\nclass State(TypedDict):\n    input: str\n    results: str\n\n@dataclass\nclass Context:\n    user_id: str\n\nbuilder = StateGraph(State)\n\ndef plain_node(state: State):\n    return state\n\ndef node_with_runtime(state: State, runtime: Runtime[Context]):\n    print(\"In node: \", runtime.context.user_id)\n    return {\"results\": f\"Hello, {state['input']}!\"}\n\ndef node_with_config(state: State, config: RunnableConfig):\n    print(\"In node with thread_id: \", config[\"configurable\"][\"thread_id\"])\n    return {\"results\": f\"Hello, {state['input']}!\"}\n\n\nbuilder.add_node(\"plain_node\", plain_node)\nbuilder.add_node(\"node_with_runtime\", node_with_runtime)\nbuilder.add_node(\"node_with_config\", node_with_config)\n...\n```\n\nBehind the scenes, functions are converted to [RunnableLambda](https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.RunnableLambda.html)s, which add batch and async support to your function, along with native tracing and debugging.\n\nIf you add a node to a graph without specifying a name, it will be given a default name equivalent to the function name.\n\n```python\nbuilder.add_node(my_node)\n# You can then create edges to/from this node by referencing it as `\"my_node\"`\n```\n\n### `START` Node\n\nThe `START` Node is a special node that represents the node that sends user input to the graph. The main purpose for referencing this node is to determine which nodes should be called first.\n\n```python\nfrom langgraph.graph import START\n\ngraph.add_edge(START, \"node_a\")\n```\n\n### `END` Node\n\nThe `END` Node is a special node that represents a terminal node. This node is referenced when you want to denote which edges have no actions after they are done.\n\n```python\nfrom langgraph.graph import END\n\ngraph.add_edge(\"node_a\", END)\n```\n\n### Node Caching\n\nLangGraph supports caching of tasks/nodes based on the input to the node. To use caching:\n\n* Specify a cache when compiling a graph (or specifying an entrypoint)\n* Specify a cache policy for nodes. Each cache policy supports:\n  * `key_func` used to generate a cache key based on the input to a node, which defaults to a `hash` of the input with pickle.\n  * `ttl`, the time to live for the cache in seconds. If not specified, the cache will never expire.\n\nFor example:\n\n```python\nimport time\nfrom typing_extensions import TypedDict\nfrom langgraph.graph import StateGraph\nfrom langgraph.cache.memory import InMemoryCache\nfrom langgraph.types import CachePolicy\n\n\nclass State(TypedDict):\n    x: int\n    result: int\n\n\nbuilder = StateGraph(State)\n\n\ndef expensive_node(state: State) -> dict[str, int]:\n    # expensive computation\n    time.sleep(2)\n    return {\"result\": state[\"x\"] * 2}\n\n\nbuilder.add_node(\"expensive_node\", expensive_node, cache_policy=CachePolicy(ttl=3))\nbuilder.set_entry_point(\"expensive_node\")\nbuilder.set_finish_point(\"expensive_node\")\n\ngraph = builder.compile(cache=InMemoryCache())\n\nprint(graph.invoke({\"x\": 5}, stream_mode='updates'))  # (1)!\n[{'expensive_node': {'result': 10}}]\nprint(graph.invoke({\"x\": 5}, stream_mode='updates'))  # (2)!\n[{'expensive_node': {'result': 10}, '__metadata__': {'cached': True}}]\n```\n\n1. First run takes two seconds to run (due to mocked expensive computation).\n2. Second run utilizes cache and returns quickly.\n\n## Edges\n\nEdges define how the logic is routed and how the graph decides to stop. This is a big part of how your agents work and how different nodes communicate with each other. There are a few key types of edges:\n\n* Normal Edges: Go directly from one node to the next.\n* Conditional Edges: Call a function to determine which node(s) to go to next.\n* Entry Point: Which node to call first when user input arrives.\n* Conditional Entry Point: Call a function to determine which node(s) to call first when user input arrives.\n\nA node can have MULTIPLE outgoing edges. If a node has multiple out-going edges, **all** of those destination nodes will be executed in parallel as a part of the next superstep.\n\n### Normal Edges\n\nIf you **always** want to go from node A to node B, you can use the [add\\_edge](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_edge) method directly.\n\n```python\ngraph.add_edge(\"node_a\", \"node_b\")\n```\n\n### Conditional Edges\n\nIf you want to **optionally** route to 1 or more edges (or optionally terminate), you can use the [add\\_conditional\\_edges](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_conditional_edges) method. This method accepts the name of a node and a \"routing function\" to call after that node is executed:\n\n```python\ngraph.add_conditional_edges(\"node_a\", routing_function)\n```\n\nSimilar to nodes, the `routing_function` accepts the current `state` of the graph and returns a value.\n\nBy default, the return value `routing_function` is used as the name of the node (or list of nodes) to send the state to next. All those nodes will be run in parallel as a part of the next superstep.\n\nYou can optionally provide a dictionary that maps the `routing_function`'s output to the name of the next node.\n\n```python\ngraph.add_conditional_edges(\"node_a\", routing_function, {True: \"node_b\", False: \"node_c\"})\n```\n\n<Tip>\n  Use [`Command`](#command) instead of conditional edges if you want to combine state updates and routing in a single function.\n</Tip>\n\n### Entry Point\n\nThe entry point is the first node(s) that are run when the graph starts. You can use the [`add_edge`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_edge) method from the virtual [`START`](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) node to the first node to execute to specify where to enter the graph.\n\n```python\nfrom langgraph.graph import START\n\ngraph.add_edge(START, \"node_a\")\n```\n\n### Conditional Entry Point\n\nA conditional entry point lets you start at different nodes depending on custom logic. You can use [`add_conditional_edges`](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph.add_conditional_edges) from the virtual [`START`](https://langchain-ai.github.io/langgraph/reference/constants/#langgraph.constants.START) node to accomplish this.\n\n```python\nfrom langgraph.graph import START\n\ngraph.add_conditional_edges(START, routing_function)\n```\n\nYou can optionally provide a dictionary that maps the `routing_function`'s output to the name of the next node.\n\n```python\ngraph.add_conditional_edges(START, routing_function, {True: \"node_b\", False: \"node_c\"})\n```\n\n## `Send`\n\nBy default, `Nodes` and `Edges` are defined ahead of time and operate on the same shared state. However, there can be cases where the exact edges are not known ahead of time and/or you may want different versions of `State` to exist at the same time. A common example of this is with [map-reduce](/oss/python/langgraph/graph-api#map-reduce-and-the-send-api) design patterns. In this design pattern, a first node may generate a list of objects, and you may want to apply some other node to all those objects. The number of objects may be unknown ahead of time (meaning the number of edges may not be known) and the input `State` to the downstream `Node` should be different (one for each generated object).\n\nTo support this design pattern, LangGraph supports returning [`Send`](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Send) objects from conditional edges. `Send` takes two arguments: first is the name of the node, and second is the state to pass to that node.\n\n```python\ndef continue_to_jokes(state: OverallState):\n    return [Send(\"generate_joke\", {\"subject\": s}) for s in state['subjects']]\n\ngraph.add_conditional_edges(\"node_a\", continue_to_jokes)\n```\n\n## `Command`\n\nIt can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a [`Command`](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Command) object from node functions:\n\n```python\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\n    return Command(\n        # state update\n        update={\"foo\": \"bar\"},\n        # control flow\n        goto=\"my_other_node\"\n    )\n```\n\nWith `Command` you can also achieve dynamic control flow behavior (identical to [conditional edges](#conditional-edges)):\n\n```python\ndef my_node(state: State) -> Command[Literal[\"my_other_node\"]]:\n    if state[\"foo\"] == \"bar\":\n        return Command(update={\"foo\": \"baz\"}, goto=\"my_other_node\")\n```\n\n<Note>\n  When returning `Command` in your node functions, you must add return type annotations with the list of node names the node is routing to, e.g. `Command[Literal[\"my_other_node\"]]`. This is necessary for the graph rendering and tells LangGraph that `my_node` can navigate to `my_other_node`.\n</Note>\n\nCheck out this [how-to guide](/oss/python/langgraph/graph-api.md#combine-control-flow-and-state-updates-with-command) for an end-to-end example of how to use `Command`.\n\n### When should I use Command instead of conditional edges?\n\n* Use `Command` when you need to **both** update the graph state **and** route to a different node. For example, when implementing [multi-agent handoffs](/oss/python/langchain/multi-agent#handoffs) where it's important to route to a different agent and pass some information to that agent.\n* Use [conditional edges](#conditional-edges) to route between nodes conditionally without updating the state.\n\n### Navigating to a node in a parent graph\n\nIf you are using [subgraphs](/oss/python/concepts/subgraphs), you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify `graph=Command.PARENT` in `Command`:\n\n```python\ndef my_node(state: State) -> Command[Literal[\"other_subgraph\"]]:\n    return Command(\n        update={\"foo\": \"bar\"},\n        goto=\"other_subgraph\",  # where `other_subgraph` is a node in the parent graph\n        graph=Command.PARENT\n    )\n```\n\n<Note>\n  Setting `graph` to `Command.PARENT` will navigate to the closest parent graph.\n\n  When you send updates from a subgraph node to a parent graph node for a key that's shared by both parent and subgraph [state schemas](#schema), you **must** define a [reducer](#reducers) for the key you're updating in the parent graph state. See this [example](/oss/python/langgraph/graph-api.md#navigate-to-a-node-in-a-parent-graph).\n</Note>\n\nThis is particularly useful when implementing [multi-agent handoffs](/oss/python/langchain/multi-agent#handoffs).\n\nCheck out [this guide](/oss/python/langgraph/graph-api.md#navigate-to-a-node-in-a-parent-graph) for detail.\n\n### Using inside tools\n\nA common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation.\n\nRefer to [this guide](/oss/python/langgraph/graph-api.md#use-inside-tools) for detail.\n\n### Human-in-the-loop\n\n`Command` is an important part of human-in-the-loop workflows: when using `interrupt()` to collect user input, `Command` is then used to supply the input and resume execution via `Command(resume=\"User input\")`. Check out [this conceptual guide](/oss/python/langgraph/add-human-in-the-loop) for more information.\n\n## Graph Migrations\n\nLangGraph can easily handle migrations of graph definitions (nodes, edges, and state) even when using a checkpointer to track state.\n\n* For threads at the end of the graph (i.e. not interrupted) you can change the entire topology of the graph (i.e. all nodes and edges, remove, add, rename, etc)\n* For threads currently interrupted, we support all topology changes other than renaming / removing nodes (as that thread could now be about to enter a node that no longer exists) -- if this is a blocker please reach out and we can prioritize a solution.\n* For modifying state, we have full backwards and forwards compatibility for adding and removing keys\n* State keys that are renamed lose their saved state in existing threads\n* State keys whose types change in incompatible ways could currently cause issues in threads with state from before the change -- if this is a blocker please reach out and we can prioritize a solution.\n\n## Runtime Context\n\nWhen creating a graph, you can specify a `context_schema` for runtime context passed to nodes. This is useful for passing\ninformation to nodes that is not part of the graph state. For example, you might want to pass dependencies such as model name or a database connection.\n\n```python\n@dataclass\nclass ContextSchema:\n    llm_provider: str = \"openai\"\n\ngraph = StateGraph(State, context_schema=ContextSchema)\n```\n\nYou can then pass this context into the graph using the `context` parameter of the `invoke` method.\n\n```python\ngraph.invoke(inputs, context={\"llm_provider\": \"anthropic\"})\n```\n\nYou can then access and use this context inside a node or conditional edge:\n\n```typescript\nimport { Runtime } from \"@langchain/langgraph\";\nimport * as z from \"zod\";\n\nconst nodeA = (\n  state: z.infer<typeof State>,\n  runtime: Runtime<z.infer<typeof ContextSchema>>,\n) => {\n  const llm = getLLM(runtime.context?.llm);\n  // ...\n};\n```\n\nSee [this guide](/oss/python/langgraph/use-graph-api#add-runtime-configuration) for a full breakdown on configuration.\n\n### Recursion Limit\n\nThe recursion limit sets the maximum number of [super-steps](#graphs) the graph can execute during a single execution. Once the limit is reached, LangGraph will raise `GraphRecursionError`. By default this value is set to 25 steps. The recursion limit can be set on any graph at runtime, and is passed to `.invoke`/`.stream` via the config dictionary. Importantly, `recursion_limit` is a standalone `config` key and should not be passed inside the `configurable` key as all other user-defined configuration. See the example below:\n\n```python\ngraph.invoke(inputs, config={\"recursion_limit\": 5}, context={\"llm\": \"anthropic\"})\n```\n\nRead [this how-to](/oss/python/langgraph/graph-api#impose-a-recursion-limit) to learn more about how the recursion limit works.\n\n## Visualization\n\nIt's often nice to be able to visualize graphs, especially as they get more complex. LangGraph comes with several built-in ways to visualize graphs. See [this how-to guide](/oss/python/langgraph/graph-api.md#visualize-your-graph) for more info.\n\n```",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/interrupts.md",
      "title": null,
      "raw_content": "# Interrupts Interrupts allow you to pause graph execution at specific points and wait for external input before continuing. This enables human-in-the-loop patterns where you need external input to proceed. When an interrupt is triggered, LangGraph saves the graph state using its [persistence](/oss/python/langgraph/persistence) layer and waits indefinitely until you resume execution. Interrupts work by calling the `interrupt()` function at any point in your graph nodes. The function accepts any JSON-serializable value which is surfaced to the caller. When you're ready to continue, you resume execution by re-invoking the graph using `Command`, which then becomes the return value of the `interrupt()` call from inside the node. Unlike static breakpoints (which pause before or after specific nodes), interrupts are \\*\\*dynamic\\*\\*\u2014they can be placed anywhere in your code and can be conditional based on your application logic. \\* \\*\\*Checkpointing keeps your place:\\*\\* the checkpointer writes the exact graph state so you can resume later, even when in an error state. \\* \\*\\*`thread\\_id` is your pointer:\\*\\* set `config={\"configurable\": {\"thread\\_id\": ...}}` to tell the checkpointer which state to load. \\* \\*\\*Interrupt payloads surface as `\\_\\_interrupt\\_\\_`:\\*\\* the values you pass to `interrupt()` return to the caller in the `\\_\\_interrupt\\_\\_` field so you know what the graph is waiting on. The `thread\\_id` you choose is effectively your persistent cursor. Reusing it resumes the same checkpoint; using a new value starts a brand-new thread with an empty state. ## Pause using `interrupt` The [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) function pauses graph execution and returns a value to the caller. When you call [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) within a node, LangGraph saves the current graph state and waits for you to resume execution with input. To use [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt), you need: 1. A \\*\\*checkpointer\\*\\* to persist the graph state (use a durable checkpointer in production) 2. A \\*\\*thread ID\\*\\* in your config so the runtime knows which state to resume from 3. To call `interrupt()` where you want to pause (payload must be JSON-serializable) ```python theme={null} from langgraph.types import interrupt def approval\\_node(state: State): # Pause and ask for approval approved = interrupt(\"Do you approve this action?\") # When you resume, Command(resume=...) returns that value here return {\"approved\": approved} ``` When you call [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt), here's what happens: 1. \\*\\*Graph execution gets suspended\\*\\* at the exact point where [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) is called 2. \\*\\*State is saved\\*\\* using the checkpointer so execution can be resumed later, In production, this should be a persistent checkpointer (e.g. backed by a database) 3. \\*\\*Value is returned\\*\\* to the caller under `\\_\\_interrupt\\_\\_`; it can be any JSON-serializable value (string, object, array, etc.) 4. \\*\\*Graph waits indefinitely\\*\\* until you resume execution with a response 5. \\*\\*Response is passed back\\*\\* into the node when you resume, becoming the return value of the `interrupt()` call ## Resuming interrupts After an interrupt pauses execution, you resume the graph by invoking it again with a `Command` that contains the resume value. The resume value is passed back to the `interrupt` call, allowing the node to continue execution with the external input. ```python theme={null} from langgraph.types import Command # Initial run - hits the interrupt and pauses # thread\\_id is the persistent pointer (stores a stable ID in production) config = {\"configurable\": {\"thread\\_id\": \"thread-1\"}} result = graph.invoke({\"input\": \"data\"}, config=config) # Check what was interrupted # \\_\\_interrupt\\_\\_ contains the payload that was passed to interrupt() print(result[\"\\_\\_interrupt\\_\\_\"]) # > [Interrupt(value='Do you approve this action?')] # Resume with the human's response # The resume payload becomes the return value of interrupt() inside the node graph.invoke(Command(resume=True), config=config) ``` \\*\\*Key points about resuming:\\*\\* \\* You must use the \\*\\*same thread ID\\*\\* when resuming that was used when the interrupt occurred \\* The value passed to `Command(resume=...)` becomes the return value of the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) call \\* The node restarts from the beginning of the node where the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) was called when resumed, so any code before the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) runs again \\* You can pass any JSON-serializable value as the resume value ## Common patterns The key thing that interrupts unlock is the ability to pause execution and wait for external input. This is useful for a variety of use cases, including: \\* [Approval workflows](#approve-or-reject): Pause before executing critical actions (API calls, database changes, financial transactions) \\* [Review and edit](#review-and-edit-state): Let humans review and modify LLM outputs or tool calls before continuing \\* [Interrupting tool calls](#interrupts-in-tools): Pause before executing tool calls to review and edit the tool call before execution \\* [Validating human input](#validating-human-input): Pause before proceeding to the next step to validate human input ### Approve or reject One of the most common uses of interrupts is to pause before a critical action and ask for approval. For example, you might want to ask a human to approve an API call, a database change, or any other important decision. ```python theme={null} from typing import Literal from langgraph.types import interrupt, Command def approval\\_node(state: State) -> Command[Literal[\"proceed\", \"cancel\"]]: # Pause execution; payload shows up under result[\"\\_\\_interrupt\\_\\_\"] is\\_approved = interrupt({ \"question\": \"Do you want to proceed with this action?\", \"details\": state[\"action\\_details\"] }) # Route based on the response if is\\_approved: return Command(goto=\"proceed\") # Runs after the resume payload is provided else: return Command(goto=\"cancel\") ``` When you resume the graph, pass `true` to approve or `false` to reject: ```python theme={null} # To approve graph.invoke(Command(resume=True), config=config) # To reject graph.invoke(Command(resume=False), config=config) ```  ```python theme={null} from typing import Literal, Optional, TypedDict from langgraph.checkpoint.memory import MemorySaver from langgraph.graph import StateGraph, START, END from langgraph.types import Command, interrupt class ApprovalState(TypedDict): action\\_details: str status: Optional[Literal[\"pending\", \"approved\", \"rejected\"]] def approval\\_node(state: ApprovalState) -> Command[Literal[\"proceed\", \"cancel\"]]: # Expose details so the caller can render them in a UI decision = interrupt({ \"question\": \"Approve this action?\", \"details\": state[\"action\\_details\"], }) # Route to the appropriate node after resume return Command(goto=\"proceed\" if decision else \"cancel\") def proceed\\_node(state: ApprovalState): return {\"status\": \"approved\"} def cancel\\_node(state: ApprovalState): return {\"status\": \"rejected\"} builder = StateGraph(ApprovalState) builder.add\\_node(\"approval\", approval\\_node) builder.add\\_node(\"proceed\", proceed\\_node) builder.add\\_node(\"cancel\", cancel\\_node) builder.add\\_edge(START, \"approval\") builder.add\\_edge(\"proceed\", END) builder.add\\_edge(\"cancel\", END) # Use a more durable checkpointer in production checkpointer = MemorySaver() graph = builder.compile(checkpointer=checkpointer) config = {\"configurable\": {\"thread\\_id\": \"approval-123\"}} initial = graph.invoke( {\"action\\_details\": \"Transfer $500\", \"status\": \"pending\"}, config=config, ) print(initial[\"\\_\\_interrupt\\_\\_\"]) # -> [Interrupt(value={'question': ..., 'details': ...})] # Resume with the decision; True routes to proceed, False to cancel resumed = graph.invoke(Command(resume=True), config=config) print(resumed[\"status\"]) # -> \"approved\" ```  ### Review and edit state Sometimes you want to let a human review and edit part of the graph state before continuing. This is useful for correcting LLMs, adding missing information, or making adjustments. ```python theme={null} from langgraph.types import interrupt def review\\_node(state: State): # Pause and show the current content for review (surfaces in result[\"\\_\\_interrupt\\_\\_\"]) edited\\_content = interrupt({ \"instruction\": \"Review and edit this content\", \"content\": state[\"generated\\_text\"] }) # Update the state with the edited version return {\"generated\\_text\": edited\\_content} ``` When resuming, provide the edited content: ```python theme={null} graph.invoke( Command(resume=\"The edited and improved text\"), # Value becomes the return from interrupt() config=config ) ```  ```python theme={null} import sqlite3 from typing import TypedDict from langgraph.checkpoint.memory import MemorySaver from langgraph.graph import StateGraph, START, END from langgraph.types import Command, interrupt class ReviewState(TypedDict): generated\\_text: str def review\\_node(state: ReviewState): # Ask a reviewer to edit the generated content updated = interrupt({ \"instruction\": \"Review and edit this content\", \"content\": state[\"generated\\_text\"], }) return {\"generated\\_text\": updated} builder = StateGraph(ReviewState) builder.add\\_node(\"review\", review\\_node) builder.add\\_edge(START, \"review\") builder.add\\_edge(\"review\", END) checkpointer = MemorySaver() graph = builder.compile(checkpointer=checkpointer) config = {\"configurable\": {\"thread\\_id\": \"review-42\"}} initial = graph.invoke({\"generated\\_text\": \"Initial draft\"}, config=config) print(initial[\"\\_\\_interrupt\\_\\_\"]) # -> [Interrupt(value={'instruction': ..., 'content': ...})] # Resume with the edited text from the reviewer final\\_state = graph.invoke( Command(resume=\"Improved draft after review\"), config=config, ) print(final\\_state[\"generated\\_text\"]) # -> \"Improved draft after review\" ```  ### Interrupts in tools You can also place interrupts directly inside tool functions. This makes the tool itself pause for approval whenever it's called, and allows for human review and editing of the tool call before it is executed. First, define a tool that uses [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt): ```python theme={null} from langchain.tools import tool from langgraph.types import interrupt @tool def send\\_email(to: str, subject: str, body: str): \"\"\"Send an email to a recipient.\"\"\" # Pause before sending; payload surfaces in result[\"\\_\\_interrupt\\_\\_\"] response = interrupt({ \"action\": \"send\\_email\", \"to\": to, \"subject\": subject, \"body\": body, \"message\": \"Approve sending this email?\" }) if response.get(\"action\") == \"approve\": # Resume value can override inputs before executing final\\_to = response.get(\"to\", to) final\\_subject = response.get(\"subject\", subject) final\\_body = response.get(\"body\", body) return f\"Email sent to {final\\_to} with subject '{final\\_subject}'\" return \"Email cancelled by user\" ``` This approach is useful when you want the approval logic to live with the tool itself, making it reusable across different parts of your graph. The LLM can call the tool naturally, and the interrupt will pause execution whenever the tool is invoked, allowing you to approve, edit, or cancel the action.  ```python theme={null} import sqlite3 from typing import TypedDict from langchain.tools import tool from langchain\\_anthropic import ChatAnthropic from langgraph.checkpoint.sqlite import SqliteSaver from langgraph.graph import StateGraph, START, END from langgraph.types import Command, interrupt class AgentState(TypedDict): messages: list[dict] @tool def send\\_email(to: str, subject: str, body: str): \"\"\"Send an email to a recipient.\"\"\" # Pause before sending; payload surfaces in result[\"\\_\\_interrupt\\_\\_\"] response = interrupt({ \"action\": \"send\\_email\", \"to\": to, \"subject\": subject, \"body\": body, \"message\": \"Approve sending this email?\", }) if response.get(\"action\") == \"approve\": final\\_to = response.get(\"to\", to) final\\_subject = response.get(\"subject\", subject) final\\_body = response.get(\"body\", body) # Actually send the email (your implementation here) print(f\"[send\\_email] to={final\\_to} subject={final\\_subject} body={final\\_body}\") return f\"Email sent to {final\\_to}\" return \"Email cancelled by user\" model = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\").bind\\_tools([send\\_email]) def agent\\_node(state: AgentState): # LLM may decide to call the tool; interrupt pauses before sending result = model.invoke(state[\"messages\"]) return {\"messages\": state[\"messages\"] + [result]} builder = StateGraph(AgentState) builder.add\\_node(\"agent\", agent\\_node) builder.add\\_edge(START, \"agent\") builder.add\\_edge(\"agent\", END) checkpointer = SqliteSaver(sqlite3.connect(\"tool-approval.db\")) graph = builder.compile(checkpointer=checkpointer) config = {\"configurable\": {\"thread\\_id\": \"email-workflow\"}} initial = graph.invoke( { \"messages\": [ {\"role\": \"user\", \"content\": \"Send an email to alice@example.com about the meeting\"} ] }, config=config, ) print(initial[\"\\_\\_interrupt\\_\\_\"]) # -> [Interrupt(value={'action': 'send\\_email', ...})] # Resume with approval and optionally edited arguments resumed = graph.invoke( Command(resume={\"action\": \"approve\", \"subject\": \"Updated subject\"}), config=config, ) print(resumed[\"messages\"][-1]) # -> Tool result returned by send\\_email ```  ### Validating human input Sometimes you need to validate input from humans and ask again if it's invalid. You can do this using multiple [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) calls in a loop. ```python theme={null} from langgraph.types import interrupt def get\\_age\\_node(state: State): prompt = \"What is your age?\" while True: answer = interrupt(prompt) # payload surfaces in result[\"\\_\\_interrupt\\_\\_\"] # Validate the input if isinstance(answer, int) and answer > 0: # Valid input - continue break else: # Invalid input - ask again with a more specific prompt prompt = f\"'{answer}' is not a valid age. Please enter a positive number.\" return {\"age\": answer} ``` Each time you resume the graph with invalid input, it will ask again with a clearer message. Once valid input is provided, the node completes and the graph continues.  ```python theme={null} import sqlite3 from typing import TypedDict from langgraph.checkpoint.sqlite import SqliteSaver from langgraph.graph import StateGraph, START, END from langgraph.types import Command, interrupt class FormState(TypedDict): age: int | None def get\\_age\\_node(state: FormState): prompt = \"What is your age?\" while True: answer = interrupt(prompt) # payload surfaces in result[\"\\_\\_interrupt\\_\\_\"] if isinstance(answer, int) and answer > 0: return {\"age\": answer} prompt = f\"'{answer}' is not a valid age. Please enter a positive number.\" builder = StateGraph(FormState) builder.add\\_node(\"collect\\_age\", get\\_age\\_node) builder.add\\_edge(START, \"collect\\_age\") builder.add\\_edge(\"collect\\_age\", END) checkpointer = SqliteSaver(sqlite3.connect(\"forms.db\")) graph = builder.compile(checkpointer=checkpointer) config = {\"configurable\": {\"thread\\_id\": \"form-1\"}} first = graph.invoke({\"age\": None}, config=config) print(first[\"\\_\\_interrupt\\_\\_\"]) # -> [Interrupt(value='What is your age?', ...)] # Provide invalid data; the node re-prompts retry = graph.invoke(Command(resume=\"thirty\"), config=config) print(retry[\"\\_\\_interrupt\\_\\_\"]) # -> [Interrupt(value=\"'thirty' is not a valid age...\", ...)] # Provide valid data; loop exits and state updates final = graph.invoke(Command(resume=30), config=config) print(final[\"age\"]) # -> 30 ```  ## Rules of interrupts When you call [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) within a node, LangGraph suspends execution by raising an exception that signals the runtime to pause. This exception propagates up through the call stack and is caught by the runtime, which notifies the graph to save the current state and wait for external input. When execution resumes (after you provide the requested input), the runtime restarts the entire node from the beginning\u2014it does not resume from the exact line where [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) was called. This means any code that ran before the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) will execute again. Because of this, there's a few important rules to follow when working with interrupts to ensure they behave as expected. ### Do not wrap `interrupt` calls in try/except The way that [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) pauses execution at the point of the call is by throwing a special exception. If you wrap the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) call in a try/except block, you will catch this exception and the interrupt will not be passed back to the graph. \\* \u2705 Separate [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) calls from error-prone code \\* \u2705 Use specific exception types in try/except blocks  ```python Separating logic theme={null} def node\\_a(state: State): # \u2705 Good: interrupting first, then handling # error conditions separately interrupt(\"What's your name?\") try: fetch\\_data() # This can fail except Exception as e: print(e) return state ``` ```python Explicit exception handling theme={null} def node\\_a(state: State): # \u2705 Good: catching specific exception types # will not catch the interrupt exception try: name = interrupt(\"What's your name?\") fetch\\_data() # This can fail except NetworkException as e: print(e) return state ```  \\* \ud83d\udd34 Do not wrap [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) calls in bare try/except blocks ```python theme={null} def node\\_a(state: State): # \u274c Bad: wrapping interrupt in bare try/except # will catch the interrupt exception try: interrupt(\"What's your name?\") except Exception as e: print(e) return state ``` ### Do not reorder `interrupt` calls within a node It's common to use multiple interrupts in a single node, however this can lead to unexpected behavior if not handled carefully. When a node contains multiple interrupt calls, LangGraph keeps a list of resume values specific to the task executing the node. Whenever execution resumes, it starts at the beginning of the node. For each interrupt encountered, LangGraph checks if a matching value exists in the task's resume list. Matching is \\*\\*strictly index-based\\*\\*, so the order of interrupt calls within the node is important. \\* \u2705 Keep [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) calls consistent across node executions ```python theme={null} def node\\_a(state: State): # \u2705 Good: interrupt calls happen in the same order every time name = interrupt(\"What's your name?\") age = interrupt(\"What's your age?\") city = interrupt(\"What's your city?\") return { \"name\": name, \"age\": age, \"city\": city } ``` \\* \ud83d\udd34 Do not conditionally skip [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) calls within a node \\* \ud83d\udd34 Do not loop [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) calls using logic that isn't deterministic across executions  ```python Skipping interrupts theme={null} def node\\_a(state: State): # \u274c Bad: conditionally skipping interrupts changes the order name = interrupt(\"What's your name?\") # On first run, this might skip the interrupt # On resume, it might not skip it - causing index mismatch if state.get(\"needs\\_age\"): age = interrupt(\"What's your age?\") city = interrupt(\"What's your city?\") return {\"name\": name, \"city\": city} ``` ```python Looping interrupts theme={null} def node\\_a(state: State): # \u274c Bad: looping based on non-deterministic data # The number of interrupts changes between executions results = [] for item in state.get(\"dynamic\\_list\", []): # List might change between runs result = interrupt(f\"Approve {item}?\") results.append(result) return {\"results\": results} ```  ### Do not return complex values in `interrupt` calls Depending on which checkpointer is used, complex values may not be serializable (e.g. you can't serialize a function). To make your graphs adaptable to any deployment, it's best practice to only use values that can be reasonably serialized. \\* \u2705 Pass simple, JSON-serializable types to [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) \\* \u2705 Pass dictionaries/objects with simple values  ```python Simple values theme={null} def node\\_a(state: State): # \u2705 Good: passing simple types that are serializable name = interrupt(\"What's your name?\") count = interrupt(42) approved = interrupt(True) return {\"name\": name, \"count\": count, \"approved\": approved} ``` ```python Structured data theme={null} def node\\_a(state: State): # \u2705 Good: passing dictionaries with simple values response = interrupt({ \"question\": \"Enter user details\", \"fields\": [\"name\", \"email\", \"age\"], \"current\\_values\": state.get(\"user\", {}) }) return {\"user\": response} ```  \\* \ud83d\udd34 Do not pass functions, class instances, or other complex objects to [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt)  ```python Functions theme={null} def validate\\_input(value): return len(value) > 0 def node\\_a(state: State): # \u274c Bad: passing a function to interrupt # The function cannot be serialized response = interrupt({ \"question\": \"What's your name?\", \"validator\": validate\\_input # This will fail }) return {\"name\": response} ``` ```python Class instances theme={null} class DataProcessor: def \\_\\_init\\_\\_(self, config): self.config = config def node\\_a(state: State): processor = DataProcessor({\"mode\": \"strict\"}) # \u274c Bad: passing a class instance to interrupt # The instance cannot be serialized response = interrupt({ \"question\": \"Enter data to process\", \"processor\": processor # This will fail }) return {\"result\": response} ```  ### Side effects called before `interrupt` must be idempotent Because interrupts work by re-running the nodes they were called from, side effects called before [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) should (ideally) be idempotent. For context, idempotency means that the same operation can be applied multiple times without changing the result beyond the initial execution. As an example, you might have an API call to update a record inside of a node. If [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) is called after that call is made, it will be re-run multiple times when the node is resumed, potentially overwriting the initial update or creating duplicate records. \\* \u2705 Use idempotent operations before [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) \\* \u2705 Place side effects after [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) calls \\* \u2705 Separate side effects into separate nodes when possible  ```python Idempotent operations theme={null} def node\\_a(state: State): # \u2705 Good: using upsert operation which is idempotent # Running this multiple times will have the same result db.upsert\\_user( user\\_id=state[\"user\\_id\"], status=\"pending\\_approval\" ) approved = interrupt(\"Approve this change?\") return {\"approved\": approved} ``` ```python Side effects after interrupt theme={null} def node\\_a(state: State): # \u2705 Good: placing side effect after the interrupt # This ensures it only runs once after approval is received approved = interrupt(\"Approve this change?\") if approved: db.create\\_audit\\_log( user\\_id=state[\"user\\_id\"], action=\"approved\" ) return {\"approved\": approved} ``` ```python Separating into different nodes theme={null} def approval\\_node(state: State): # \u2705 Good: only handling the interrupt in this node approved = interrupt(\"Approve this change?\") return {\"approved\": approved} def notification\\_node(state: State): # \u2705 Good: side effect happens in a separate node # This runs after approval, so it only executes once if (state.approved): send\\_notification( user\\_id=state[\"user\\_id\"], status=\"approved\" ) return state ```  \\* \ud83d\udd34 Do not perform non-idempotent operations before [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) \\* \ud83d\udd34 Do not create new records without checking if they exist  ```python Creating records theme={null} def node\\_a(state: State): # \u274c Bad: creating a new record before interrupt # This will create duplicate records on each resume audit\\_id = db.create\\_audit\\_log({ \"user\\_id\": state[\"user\\_id\"], \"action\": \"pending\\_approval\", \"timestamp\": datetime.now() }) approved = interrupt(\"Approve this change?\") return {\"approved\": approved, \"audit\\_id\": audit\\_id} ``` ```python Appending to lists theme={null} def node\\_a(state: State): # \u274c Bad: appending to a list before interrupt # This will add duplicate entries on each resume db.append\\_to\\_history(state[\"user\\_id\"], \"approval\\_requested\") approved = interrupt(\"Approve this change?\") return {\"approved\": approved} ```  ## Using with subgraphs called as functions When invoking a subgraph within a node, the parent graph will resume execution from the \\*\\*beginning of the node\\*\\* where the subgraph was invoked and the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) was triggered. Similarly, the \\*\\*subgraph\\*\\* will also resume from the beginning of the node where [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) was called. ```python theme={null} def node\\_in\\_parent\\_graph(state: State): some\\_code() # <-- This will re-execute when resumed # Invoke a subgraph as a function. # The subgraph contains an `interrupt` call. subgraph\\_result = subgraph.invoke(some\\_input) async function node\\_in\\_subgraph(state: State) { someOtherCode(); # <-- This will also re-execute when resumed result = interrupt(\"What's your name?\") ... } ``` ## Debugging with interrupts To debug and test a graph, you can use static interrupts as breakpoints to step through the graph execution one node at a time. Static interrupts are triggered at defined points either before or after a node executes. You can set these by specifying `interrupt\\_before` and `interrupt\\_after` when compiling the graph.  Static interrupts are \\*\\*not\\*\\* recommended for human-in-the-loop workflows. Use the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) method instead.    ```python theme={null} graph = builder.compile( interrupt\\_before=[\"node\\_a\"], # [!code highlight] interrupt\\_after=[\"node\\_b\", \"node\\_c\"], # [!code highlight] checkpointer=checkpointer, ) # Pass a thread ID to the graph config = { \"configurable\": { \"thread\\_id\": \"some\\_thread\" } } # Run the graph until the breakpoint graph.invoke(inputs, config=config) # [!code highlight] # Resume the graph graph.invoke(None, config=config) # [!code highlight] ``` 1. The breakpoints are set during `compile` time. 2. `interrupt\\_before` specifies the nodes where execution should pause before the node is executed. 3. `interrupt\\_after` specifies the nodes where execution should pause after the node is executed. 4. A checkpointer is required to enable breakpoints. 5. The graph is run until the first breakpoint is hit. 6. The graph is resumed by passing in `None` for the input. This will run the graph until the next breakpoint is hit.   ```python theme={null} config = { \"configurable\": { \"thread\\_id\": \"some\\_thread\" } } # Run the graph until the breakpoint graph.invoke( inputs, interrupt\\_before=[\"node\\_a\"], # [!code highlight] interrupt\\_after=[\"node\\_b\", \"node\\_c\"], # [!code highlight] config=config, ) # Resume the graph graph.invoke(None, config=config) # [!code highlight] ``` 1. `graph.invoke` is called with the `interrupt\\_before` and `interrupt\\_after` parameters. This is a run-time configuration and can be changed for every invocation. 2. `interrupt\\_before` specifies the nodes where execution should pause before the node is executed. 3. `interrupt\\_after` specifies the nodes where execution should pause after the node is executed. 4. The graph is run until the first breakpoint is hit. 5. The graph is resumed by passing in `None` for the input. This will run the graph until the next breakpoint is hit.   ### Using LangGraph Studio You can use [LangGraph Studio](/langsmith/studio) to set static interrupts in your graph in the UI before running the graph. You can also use the UI to inspect the graph state at any point in the execution. \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/interrupts.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/local-server.md",
      "title": null,
      "raw_content": "# Run a local server This guide shows you how to run a LangGraph application locally. ## Prerequisites Before you begin, ensure you have the following: \\* An API key for [LangSmith](https://smith.langchain.com/settings) - free to sign up ## 1. Install the LangGraph CLI  ```bash pip theme={null} # Python >= 3.11 is required. pip install -U \"langgraph-cli[inmem]\" ``` ```bash uv theme={null} # Python >= 3.11 is required. uv add 'langgraph-cli[inmem]' ```  ## 2. Create a LangGraph app Create a new app from the [`new-langgraph-project-python` template](https://github.com/langchain-ai/new-langgraph-project). This template demonstrates a single-node application you can extend with your own logic. ```shell theme={null} langgraph new path/to/your/app --template new-langgraph-project-python ```  \\*\\*Additional templates\\*\\* If you use `langgraph new` without specifying a template, you will be presented with an interactive menu that will allow you to choose from a list of available templates.  ## 3. Install dependencies In the root of your new LangGraph app, install the dependencies in `edit` mode so your local changes are used by the server:  ```bash pip theme={null} cd path/to/your/app pip install -e . ``` ```bash uv theme={null} cd path/to/your/app uv sync ```  ## 4. Create a `.env` file You will find a `.env.example` in the root of your new LangGraph app. Create a `.env` file in the root of your new LangGraph app and copy the contents of the `.env.example` file into it, filling in the necessary API keys: ```bash theme={null} LANGSMITH\\_API\\_KEY=lsv2... ``` ## 5. Launch Agent server Start the LangGraph API server locally: ```shell theme={null} langgraph dev ``` Sample output: ``` > Ready! > > - API: [http://localhost:2024](http://localhost:2024/) > > - Docs: http://localhost:2024/docs > > - LangGraph Studio Web UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024 ``` The `langgraph dev` command starts Agent Server in an in-memory mode. This mode is suitable for development and testing purposes. For production use, deploy Agent Server with access to a persistent storage backend. For more information, see the [Platform setup overview](/langsmith/platform-setup). ## 6. Test your application in Studio [Studio](/langsmith/studio) is a specialized UI that you can connect to LangGraph API server to visualize, interact with, and debug your application locally. Test your graph in Studio by visiting the URL provided in the output of the `langgraph dev` command: ``` > - LangGraph Studio Web UI: https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024 ``` For an Agent Server running on a custom host/port, update the `baseUrl` query parameter in the URL. For example, if your server is running on `http://myhost:3000`: ``` https://smith.langchain.com/studio/?baseUrl=http://myhost:3000 ```  Use the `--tunnel` flag with your command to create a secure tunnel, as Safari has limitations when connecting to localhost servers: ```shell theme={null} langgraph dev --tunnel ```  ## 7. Test the API   1. Install the LangGraph Python SDK: ```shell theme={null} pip install langgraph-sdk ``` 2. Send a message to the assistant (threadless run): ```python theme={null} from langgraph\\_sdk import get\\_client import asyncio client = get\\_client(url=\"http://localhost:2024\") async def main(): async for chunk in client.runs.stream( None, # Threadless run \"agent\", # Name of assistant. Defined in langgraph.json. input={ \"messages\": [{ \"role\": \"human\", \"content\": \"What is LangGraph?\", }], }, ): print(f\"Receiving new event of type: {chunk.event}...\") print(chunk.data) print(\"\\n\\n\") asyncio.run(main()) ```   1. Install the LangGraph Python SDK: ```shell theme={null} pip install langgraph-sdk ``` 2. Send a message to the assistant (threadless run): ```python theme={null} from langgraph\\_sdk import get\\_sync\\_client client = get\\_sync\\_client(url=\"http://localhost:2024\") for chunk in client.runs.stream( None, # Threadless run \"agent\", # Name of assistant. Defined in langgraph.json. input={ \"messages\": [{ \"role\": \"human\", \"content\": \"What is LangGraph?\", }], }, stream\\_mode=\"messages-tuple\", ): print(f\"Receiving new event of type: {chunk.event}...\") print(chunk.data) print(\"\\n\\n\") ```   ```bash theme={null} curl -s --request POST \\ --url \"http://localhost:2024/runs/stream\" \\ --header 'Content-Type: application/json' \\ --data \"{ \\\"assistant\\_id\\\": \\\"agent\\\", \\\"input\\\": { \\\"messages\\\": [ { \\\"role\\\": \\\"human\\\", \\\"content\\\": \\\"What is LangGraph?\\\" } ] }, \\\"stream\\_mode\\\": \\\"messages-tuple\\\" }\" ```   ## Next steps Now that you have a LangGraph app running locally, take your journey further by exploring deployment and advanced features: \\* [Deployment quickstart](/langsmith/deployment-quickstart): Deploy your LangGraph app using LangSmith. \\* [LangSmith](/langsmith/home): Learn about foundational LangSmith concepts. \\* [SDK Reference](https://reference.langchain.com/python/langsmith/deployment/sdk/): Explore the SDK API Reference. \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/local-server.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/observability.md",
      "title": null,
      "raw_content": "# LangSmith Observability Traces are a series of steps that your application takes to go from input to output. Each of these individual steps is represented by a run. You can use [LangSmith](https://smith.langchain.com/) to visualize these execution steps. To use it, [enable tracing for your application](/langsmith/trace-with-langgraph). This enables you to do the following: \\* [Debug a locally running application](/langsmith/observability-studio#debug-langsmith-traces). \\* [Evaluate the application performance](/oss/python/langchain/evals). \\* [Monitor the application](/langsmith/dashboards). ## Prerequisites Before you begin, ensure you have the following: \\* \\*\\*A LangSmith account\\*\\*: Sign up (for free) or log in at [smith.langchain.com](https://smith.langchain.com). \\* \\*\\*A LangSmith API key\\*\\*: Follow the [Create an API key](/langsmith/create-account-api-key#create-an-api-key) guide. ## Enable tracing To enable tracing for your application, set the following environment variables: ```python theme={null} export LANGSMITH\\_TRACING=true export LANGSMITH\\_API\\_KEY= ``` By default, the trace will be logged to the project with the name `default`. To configure a custom project name, see [Log to a project](#log-to-a-project). For more information, see [Trace with LangGraph](/langsmith/trace-with-langgraph). ## Trace selectively You may opt to trace specific invocations or parts of your application using LangSmith's `tracing\\_context` context manager: ```python theme={null} import langsmith as ls # This WILL be traced with ls.tracing\\_context(enabled=True): agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Send a test email to alice@example.com\"}]}) # This will NOT be traced (if LANGSMITH\\_TRACING is not set) agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"Send another email\"}]}) ``` ## Log to a project  You can set a custom project name for your entire application by setting the `LANGSMITH\\_PROJECT` environment variable: ```bash theme={null} export LANGSMITH\\_PROJECT=my-agent-project ```   You can set the project name programmatically for specific operations: ```python theme={null} import langsmith as ls with ls.tracing\\_context(project\\_name=\"email-agent-test\", enabled=True): response = agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"Send a welcome email\"}] }) ```  ## Add metadata to traces You can annotate your traces with custom metadata and tags: ```python theme={null} response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"Send a welcome email\"}]}, config={ \"tags\": [\"production\", \"email-assistant\", \"v1.0\"], \"metadata\": { \"user\\_id\": \"user\\_123\", \"session\\_id\": \"session\\_456\", \"environment\": \"production\" } } ) ``` `tracing\\_context` also accepts tags and metadata for fine-grained control: ```python theme={null} with ls.tracing\\_context( project\\_name=\"email-agent-test\", enabled=True, tags=[\"production\", \"email-assistant\", \"v1.0\"], metadata={\"user\\_id\": \"user\\_123\", \"session\\_id\": \"session\\_456\", \"environment\": \"production\"}): response = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"Send a welcome email\"}]} ) ``` This custom metadata and tags will be attached to the trace in LangSmith.  To learn more about how to use traces to debug, evaluate, and monitor your agents, see the [LangSmith documentation](/langsmith/home).  ## Use anonymizers to prevent logging of sensitive data in traces You may want to mask sensitive data to prevent it from being logged to LangSmith. You can create [anonymizers](/langsmith/mask-inputs-outputs#rule-based-masking-of-inputs-and-outputs) and apply them to your graph using configuration. This example will redact anything matching the Social Security Number format XXX-XX-XXXX from traces sent to LangSmith. ```python Python theme={null} from langchain\\_core.tracers.langchain import LangChainTracer from langgraph.graph import StateGraph, MessagesState from langsmith import Client from langsmith.anonymizer import create\\_anonymizer anonymizer = create\\_anonymizer([ # Matches SSNs { \"pattern\": r\"\\b\\d{3}-?\\d{2}-?\\d{4}\\b\", \"replace\": \"\" } ]) tracer\\_client = Client(anonymizer=anonymizer) tracer = LangChainTracer(client=tracer\\_client) # Define the graph graph = ( StateGraph(MessagesState) ... .compile() .with\\_config({'callbacks': [tracer]}) ) ``` \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/observability.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/overview.md",
      "title": null,
      "raw_content": "# LangGraph overview  \\*\\*LangGraph v1.x is now available!\\*\\* For a complete list of changes and instructions on how to upgrade your code, see the [release notes](/oss/python/releases/langgraph-v1) and [migration guide](/oss/python/migrate/langgraph-v1). If you encounter any issues or have feedback, please [open an issue](https://github.com/langchain-ai/docs/issues/new?template=02-langgraph.yml\\&labels=langgraph,python) so we can improve. To view v0.x documentation, [go to the archived content](https://github.com/langchain-ai/langgraph/tree/main/docs/docs).  Trusted by companies shaping the future of agents-- including Klarna, Replit, Elastic, and more-- LangGraph is a low-level orchestration framework and runtime for building, managing, and deploying long-running, stateful agents. LangGraph is very low-level, and focused entirely on agent \\*\\*orchestration\\*\\*. Before using LangGraph, we recommend you familiarize yourself with some of the components used to build agents, starting with [models](/oss/python/langchain/models) and [tools](/oss/python/langchain/tools). We will commonly use [LangChain](/oss/python/langchain/overview) components throughout the documentation to integrate models and tools, but you don't need to use LangChain to use LangGraph. If you are just getting started with agents or want a higher-level abstraction, we recommend you use LangChain's [agents](/oss/python/langchain/agents) that provide pre-built architectures for common LLM and tool-calling loops. LangGraph is focused on the underlying capabilities important for agent orchestration: durable execution, streaming, human-in-the-loop, and more. ## Install  ```bash pip theme={null} pip install -U langgraph ``` ```bash uv theme={null} uv add langgraph ```  Then, create a simple hello world example: ```python theme={null} from langgraph.graph import StateGraph, MessagesState, START, END def mock\\_llm(state: MessagesState): return {\"messages\": [{\"role\": \"ai\", \"content\": \"hello world\"}]} graph = StateGraph(MessagesState) graph.add\\_node(mock\\_llm) graph.add\\_edge(START, \"mock\\_llm\") graph.add\\_edge(\"mock\\_llm\", END) graph = graph.compile() graph.invoke({\"messages\": [{\"role\": \"user\", \"content\": \"hi!\"}]}) ``` ## Core benefits LangGraph provides low-level supporting infrastructure for \\*any\\* long-running, stateful workflow or agent. LangGraph does not abstract prompts or architecture, and provides the following central benefits: \\* [Durable execution](/oss/python/langgraph/durable-execution): Build agents that persist through failures and can run for extended periods, resuming from where they left off. \\* [Human-in-the-loop](/oss/python/langgraph/interrupts): Incorporate human oversight by inspecting and modifying agent state at any point. \\* [Comprehensive memory](/oss/python/concepts/memory): Create stateful agents with both short-term working memory for ongoing reasoning and long-term memory across sessions. \\* [Debugging with LangSmith](/langsmith/home): Gain deep visibility into complex agent behavior with visualization tools that trace execution paths, capture state transitions, and provide detailed runtime metrics. \\* [Production-ready deployment](/langsmith/deployments): Deploy sophisticated agent systems confidently with scalable infrastructure designed to handle the unique challenges of stateful, long-running workflows. ## LangGraph ecosystem While LangGraph can be used standalone, it also integrates seamlessly with any LangChain product, giving developers a full suite of tools for building agents. To improve your LLM application development, pair LangGraph with:   Trace requests, evaluate outputs, and monitor deployments in one place. Prototype locally with LangGraph, then move to production with integrated observability and evaluation to build more reliable agent systems.   Deploy and scale agents effortlessly with a purpose-built deployment platform for long running, stateful workflows. Discover, reuse, configure, and share agents across teams \u2014 and iterate quickly with visual prototyping in Studio.   Provides integrations and composable components to streamline LLM application development. Contains agent abstractions built on top of LangGraph.   ## Acknowledgements LangGraph is inspired by [Pregel](https://research.google/pubs/pub37252/) and [Apache Beam](https://beam.apache.org/). The public interface draws inspiration from [NetworkX](https://networkx.org/documentation/latest/). LangGraph is built by LangChain Inc, the creators of LangChain, but can be used without LangChain. \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/overview.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/persistence.md",
      "title": null,
      "raw_content": "# Persistence LangGraph has a built-in persistence layer, implemented through checkpointers. When you compile a graph with a checkpointer, the checkpointer saves a `checkpoint` of the graph state at every super-step. Those checkpoints are saved to a `thread`, which can be accessed after graph execution. Because `threads` allow access to graph's state after execution, several powerful capabilities including human-in-the-loop, memory, time travel, and fault-tolerance are all possible. Below, we'll discuss each of these concepts in more detail.  \\*\\*LangGraph API handles checkpointing automatically\\*\\* When using the LangGraph API, you don't need to implement or configure checkpointers manually. The API handles all persistence infrastructure for you behind the scenes.  ## Threads A thread is a unique ID or thread identifier assigned to each checkpoint saved by a checkpointer. It contains the accumulated state of a sequence of [runs](/langsmith/assistants#execution). When a run is executed, the [state](/oss/python/langgraph/graph-api#state) of the underlying graph of the assistant will be persisted to the thread. When invoking a graph with a checkpointer, you \\*\\*must\\*\\* specify a `thread\\_id` as part of the `configurable` portion of the config: ```python theme={null} {\"configurable\": {\"thread\\_id\": \"1\"}} ``` A thread's current and historical state can be retrieved. To persist state, a thread must be created prior to executing a run. The LangSmith API provides several endpoints for creating and managing threads and thread state. See the [API reference](https://reference.langchain.com/python/langsmith/) for more details. The checkpointer uses `thread\\_id` as the primary key for storing and retrieving checkpoints. Without it, the checkpointer cannot save state or resume execution after an [interrupt](/oss/python/langgraph/interrupts), since the checkpointer uses `thread\\_id` to load the saved state. ## Checkpoints The state of a thread at a particular point in time is called a checkpoint. Checkpoint is a snapshot of the graph state saved at each super-step and is represented by `StateSnapshot` object with the following key properties: \\* `config`: Config associated with this checkpoint. \\* `metadata`: Metadata associated with this checkpoint. \\* `values`: Values of the state channels at this point in time. \\* `next` A tuple of the node names to execute next in the graph. \\* `tasks`: A tuple of `PregelTask` objects that contain information about next tasks to be executed. If the step was previously attempted, it will include error information. If a graph was interrupted [dynamically](/oss/python/langgraph/interrupts#pause-using-interrupt) from within a node, tasks will contain additional data associated with interrupts. Checkpoints are persisted and can be used to restore the state of a thread at a later time. Let's see what checkpoints are saved when a simple graph is invoked as follows: ```python theme={null} from langgraph.graph import StateGraph, START, END from langgraph.checkpoint.memory import InMemorySaver from langchain\\_core.runnables import RunnableConfig from typing import Annotated from typing\\_extensions import TypedDict from operator import add class State(TypedDict): foo: str bar: Annotated[list[str], add] def node\\_a(state: State): return {\"foo\": \"a\", \"bar\": [\"a\"]} def node\\_b(state: State): return {\"foo\": \"b\", \"bar\": [\"b\"]} workflow = StateGraph(State) workflow.add\\_node(node\\_a) workflow.add\\_node(node\\_b) workflow.add\\_edge(START, \"node\\_a\") workflow.add\\_edge(\"node\\_a\", \"node\\_b\") workflow.add\\_edge(\"node\\_b\", END) checkpointer = InMemorySaver() graph = workflow.compile(checkpointer=checkpointer) config: RunnableConfig = {\"configurable\": {\"thread\\_id\": \"1\"}} graph.invoke({\"foo\": \"\", \"bar\":[]}, config) ``` After we run the graph, we expect to see exactly 4 checkpoints: \\* Empty checkpoint with [`START`](https://reference.langchain.com/python/langgraph/constants/#langgraph.constants.START) as the next node to be executed \\* Checkpoint with the user input `{'foo': '', 'bar': []}` and `node\\_a` as the next node to be executed \\* Checkpoint with the outputs of `node\\_a` `{'foo': 'a', 'bar': ['a']}` and `node\\_b` as the next node to be executed \\* Checkpoint with the outputs of `node\\_b` `{'foo': 'b', 'bar': ['a', 'b']}` and no next nodes to be executed Note that we `bar` channel values contain outputs from both nodes as we have a reducer for `bar` channel. ### Get state When interacting with the saved graph state, you \\*\\*must\\*\\* specify a [thread identifier](#threads). You can view the \\*latest\\* state of the graph by calling `graph.get\\_state(config)`. This will return a `StateSnapshot` object that corresponds to the latest checkpoint associated with the thread ID provided in the config or a checkpoint associated with a checkpoint ID for the thread, if provided. ```python theme={null} # get the latest state snapshot config = {\"configurable\": {\"thread\\_id\": \"1\"}} graph.get\\_state(config) # get a state snapshot for a specific checkpoint\\_id config = {\"configurable\": {\"thread\\_id\": \"1\", \"checkpoint\\_id\": \"1ef663ba-28fe-6528-8002-5a559208592c\"}} graph.get\\_state(config) ``` In our example, the output of `get\\_state` will look like this: ``` StateSnapshot( values={'foo': 'b', 'bar': ['a', 'b']}, next=(), config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1ef663ba-28fe-6528-8002-5a559208592c'}}, metadata={'source': 'loop', 'writes': {'node\\_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2}, created\\_at='2024-08-29T19:19:38.821749+00:00', parent\\_config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}}, tasks=() ) ``` ### Get state history You can get the full history of the graph execution for a given thread by calling [`graph.get\\_state\\_history(config)`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.get\\_state\\_history). This will return a list of `StateSnapshot` objects associated with the thread ID provided in the config. Importantly, the checkpoints will be ordered chronologically with the most recent checkpoint / `StateSnapshot` being the first in the list. ```python theme={null} config = {\"configurable\": {\"thread\\_id\": \"1\"}} list(graph.get\\_state\\_history(config)) ``` In our example, the output of [`get\\_state\\_history`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.get\\_state\\_history) will look like this: ``` [ StateSnapshot( values={'foo': 'b', 'bar': ['a', 'b']}, next=(), config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1ef663ba-28fe-6528-8002-5a559208592c'}}, metadata={'source': 'loop', 'writes': {'node\\_b': {'foo': 'b', 'bar': ['b']}}, 'step': 2}, created\\_at='2024-08-29T19:19:38.821749+00:00', parent\\_config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}}, tasks=(), ), StateSnapshot( values={'foo': 'a', 'bar': ['a']}, next=('node\\_b',), config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1ef663ba-28f9-6ec4-8001-31981c2c39f8'}}, metadata={'source': 'loop', 'writes': {'node\\_a': {'foo': 'a', 'bar': ['a']}}, 'step': 1}, created\\_at='2024-08-29T19:19:38.819946+00:00', parent\\_config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}}, tasks=(PregelTask(id='6fb7314f-f114-5413-a1f3-d37dfe98ff44', name='node\\_b', error=None, interrupts=()),), ), StateSnapshot( values={'foo': '', 'bar': []}, next=('node\\_a',), config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1ef663ba-28f4-6b4a-8000-ca575a13d36a'}}, metadata={'source': 'loop', 'writes': None, 'step': 0}, created\\_at='2024-08-29T19:19:38.817813+00:00', parent\\_config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}}, tasks=(PregelTask(id='f1b14528-5ee5-579c-949b-23ef9bfbed58', name='node\\_a', error=None, interrupts=()),), ), StateSnapshot( values={'bar': []}, next=('\\_\\_start\\_\\_',), config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1ef663ba-28f0-6c66-bfff-6723431e8481'}}, metadata={'source': 'input', 'writes': {'foo': ''}, 'step': -1}, created\\_at='2024-08-29T19:19:38.816205+00:00', parent\\_config=None, tasks=(PregelTask(id='6d27aa2e-d72b-5504-a36f-8620e54a76dd', name='\\_\\_start\\_\\_', error=None, interrupts=()),), ) ] ``` ### Replay It's also possible to play-back a prior graph execution. If we `invoke` a graph with a `thread\\_id` and a `checkpoint\\_id`, then we will \\*re-play\\* the previously executed steps \\*before\\* a checkpoint that corresponds to the `checkpoint\\_id`, and only execute the steps \\*after\\* the checkpoint. \\* `thread\\_id` is the ID of a thread. \\* `checkpoint\\_id` is an identifier that refers to a specific checkpoint within a thread. You must pass these when invoking the graph as part of the `configurable` portion of the config: ```python theme={null} config = {\"configurable\": {\"thread\\_id\": \"1\", \"checkpoint\\_id\": \"0c62ca34-ac19-445d-bbb0-5b4984975b2a\"}} graph.invoke(None, config=config) ``` Importantly, LangGraph knows whether a particular step has been executed previously. If it has, LangGraph simply \\*re-plays\\* that particular step in the graph and does not re-execute the step, but only for the steps \\*before\\* the provided `checkpoint\\_id`. All of the steps \\*after\\* `checkpoint\\_id` will be executed (i.e., a new fork), even if they have been executed previously. See this [how to guide on time-travel to learn more about replaying](/oss/python/langgraph/use-time-travel). ### Update state In addition to re-playing the graph from specific `checkpoints`, we can also \\*edit\\* the graph state. We do this using [`update\\_state`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.update\\_state). This method accepts three different arguments: #### `config` The config should contain `thread\\_id` specifying which thread to update. When only the `thread\\_id` is passed, we update (or fork) the current state. Optionally, if we include `checkpoint\\_id` field, then we fork that selected checkpoint. #### `values` These are the values that will be used to update the state. Note that this update is treated exactly as any update from a node is treated. This means that these values will be passed to the [reducer](/oss/python/langgraph/graph-api#reducers) functions, if they are defined for some of the channels in the graph state. This means that [`update\\_state`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.update\\_state) does NOT automatically overwrite the channel values for every channel, but only for the channels without reducers. Let's walk through an example. Let's assume you have defined the state of your graph with the following schema (see full example above): ```python theme={null} from typing import Annotated from typing\\_extensions import TypedDict from operator import add class State(TypedDict): foo: int bar: Annotated[list[str], add] ``` Let's now assume the current state of the graph is ``` {\"foo\": 1, \"bar\": [\"a\"]} ``` If you update the state as below: ```python theme={null} graph.update\\_state(config, {\"foo\": 2, \"bar\": [\"b\"]}) ``` Then the new state of the graph will be: ``` {\"foo\": 2, \"bar\": [\"a\", \"b\"]} ``` The `foo` key (channel) is completely changed (because there is no reducer specified for that channel, so [`update\\_state`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.update\\_state) overwrites it). However, there is a reducer specified for the `bar` key, and so it appends `\"b\"` to the state of `bar`. #### `as\\_node` The final thing you can optionally specify when calling [`update\\_state`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.update\\_state) is `as\\_node`. If you provided it, the update will be applied as if it came from node `as\\_node`. If `as\\_node` is not provided, it will be set to the last node that updated the state, if not ambiguous. The reason this matters is that the next steps to execute depend on the last node to have given an update, so this can be used to control which node executes next. See this [how to guide on time-travel to learn more about forking state](/oss/python/langgraph/use-time-travel). ## Memory Store A [state schema](/oss/python/langgraph/graph-api#schema) specifies a set of keys that are populated as a graph is executed. As discussed above, state can be written by a checkpointer to a thread at each graph step, enabling state persistence. But, what if we want to retain some information \\*across threads\\*? Consider the case of a chatbot where we want to retain specific information about the user across \\*all\\* chat conversations (e.g., threads) with that user! With checkpointers alone, we cannot share information across threads. This motivates the need for the [`Store`](https://reference.langchain.com/python/langgraph/store/) interface. As an illustration, we can define an `InMemoryStore` to store information about a user across threads. We simply compile our graph with a checkpointer, as before, and with our new `in\\_memory\\_store` variable.  \\*\\*LangGraph API handles stores automatically\\*\\* When using the LangGraph API, you don't need to implement or configure stores manually. The API handles all storage infrastructure for you behind the scenes.  ### Basic Usage First, let's showcase this in isolation without using LangGraph. ```python theme={null} from langgraph.store.memory import InMemoryStore in\\_memory\\_store = InMemoryStore() ``` Memories are namespaced by a `tuple`, which in this specific example will be `(, \"memories\")`. The namespace can be any length and represent anything, does not have to be user specific. ```python theme={null} user\\_id = \"1\" namespace\\_for\\_memory = (user\\_id, \"memories\") ``` We use the `store.put` method to save memories to our namespace in the store. When we do this, we specify the namespace, as defined above, and a key-value pair for the memory: the key is simply a unique identifier for the memory (`memory\\_id`) and the value (a dictionary) is the memory itself. ```python theme={null} memory\\_id = str(uuid.uuid4()) memory = {\"food\\_preference\" : \"I like pizza\"} in\\_memory\\_store.put(namespace\\_for\\_memory, memory\\_id, memory) ``` We can read out memories in our namespace using the `store.search` method, which will return all memories for a given user as a list. The most recent memory is the last in the list. ```python theme={null} memories = in\\_memory\\_store.search(namespace\\_for\\_memory) memories[-1].dict() {'value': {'food\\_preference': 'I like pizza'}, 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843', 'namespace': ['1', 'memories'], 'created\\_at': '2024-10-02T17:22:31.590602+00:00', 'updated\\_at': '2024-10-02T17:22:31.590605+00:00'} ``` Each memory type is a Python class ([`Item`](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.Item)) with certain attributes. We can access it as a dictionary by converting via `.dict` as above. The attributes it has are: \\* `value`: The value (itself a dictionary) of this memory \\* `key`: A unique key for this memory in this namespace \\* `namespace`: A list of strings, the namespace of this memory type \\* `created\\_at`: Timestamp for when this memory was created \\* `updated\\_at`: Timestamp for when this memory was updated ### Semantic Search Beyond simple retrieval, the store also supports semantic search, allowing you to find memories based on meaning rather than exact matches. To enable this, configure the store with an embedding model: ```python theme={null} from langchain.embeddings import init\\_embeddings store = InMemoryStore( index={ \"embed\": init\\_embeddings(\"openai:text-embedding-3-small\"), # Embedding provider \"dims\": 1536, # Embedding dimensions \"fields\": [\"food\\_preference\", \"$\"] # Fields to embed } ) ``` Now when searching, you can use natural language queries to find relevant memories: ```python theme={null} # Find memories about food preferences # (This can be done after putting memories into the store) memories = store.search( namespace\\_for\\_memory, query=\"What does the user like to eat?\", limit=3 # Return top 3 matches ) ``` You can control which parts of your memories get embedded by configuring the `fields` parameter or by specifying the `index` parameter when storing memories: ```python theme={null} # Store with specific fields to embed store.put( namespace\\_for\\_memory, str(uuid.uuid4()), { \"food\\_preference\": \"I love Italian cuisine\", \"context\": \"Discussing dinner plans\" }, index=[\"food\\_preference\"] # Only embed \"food\\_preferences\" field ) # Store without embedding (still retrievable, but not searchable) store.put( namespace\\_for\\_memory, str(uuid.uuid4()), {\"system\\_info\": \"Last updated: 2024-01-01\"}, index=False ) ``` ### Using in LangGraph With this all in place, we use the `in\\_memory\\_store` in LangGraph. The `in\\_memory\\_store` works hand-in-hand with the checkpointer: the checkpointer saves state to threads, as discussed above, and the `in\\_memory\\_store` allows us to store arbitrary information for access \\*across\\* threads. We compile the graph with both the checkpointer and the `in\\_memory\\_store` as follows. ```python theme={null} from langgraph.checkpoint.memory import InMemorySaver # We need this because we want to enable threads (conversations) checkpointer = InMemorySaver() # ... Define the graph ... # Compile the graph with the checkpointer and store graph = graph.compile(checkpointer=checkpointer, store=in\\_memory\\_store) ``` We invoke the graph with a `thread\\_id`, as before, and also with a `user\\_id`, which we'll use to namespace our memories to this particular user as we showed above. ```python theme={null} # Invoke the graph user\\_id = \"1\" config = {\"configurable\": {\"thread\\_id\": \"1\", \"user\\_id\": user\\_id}} # First let's just say hi to the AI for update in graph.stream( {\"messages\": [{\"role\": \"user\", \"content\": \"hi\"}]}, config, stream\\_mode=\"updates\" ): print(update) ``` We can access the `in\\_memory\\_store` and the `user\\_id` in \\*any node\\* by passing `store: BaseStore` and `config: RunnableConfig` as node arguments. Here's how we might use semantic search in a node to find relevant memories: ```python theme={null} def update\\_memory(state: MessagesState, config: RunnableConfig, \\*, store: BaseStore): # Get the user id from the config user\\_id = config[\"configurable\"][\"user\\_id\"] # Namespace the memory namespace = (user\\_id, \"memories\") # ... Analyze conversation and create a new memory # Create a new memory ID memory\\_id = str(uuid.uuid4()) # We create a new memory store.put(namespace, memory\\_id, {\"memory\": memory}) ``` As we showed above, we can also access the store in any node and use the `store.search` method to get memories. Recall the memories are returned as a list of objects that can be converted to a dictionary. ```python theme={null} memories[-1].dict() {'value': {'food\\_preference': 'I like pizza'}, 'key': '07e0caf4-1631-47b7-b15f-65515d4c1843', 'namespace': ['1', 'memories'], 'created\\_at': '2024-10-02T17:22:31.590602+00:00', 'updated\\_at': '2024-10-02T17:22:31.590605+00:00'} ``` We can access the memories and use them in our model call. ```python theme={null} def call\\_model(state: MessagesState, config: RunnableConfig, \\*, store: BaseStore): # Get the user id from the config user\\_id = config[\"configurable\"][\"user\\_id\"] # Namespace the memory namespace = (user\\_id, \"memories\") # Search based on the most recent message memories = store.search( namespace, query=state[\"messages\"][-1].content, limit=3 ) info = \"\\n\".join([d.value[\"memory\"] for d in memories]) # ... Use memories in the model call ``` If we create a new thread, we can still access the same memories so long as the `user\\_id` is the same. ```python theme={null} # Invoke the graph config = {\"configurable\": {\"thread\\_id\": \"2\", \"user\\_id\": \"1\"}} # Let's say hi again for update in graph.stream( {\"messages\": [{\"role\": \"user\", \"content\": \"hi, tell me about my memories\"}]}, config, stream\\_mode=\"updates\" ): print(update) ``` When we use the LangSmith, either locally (e.g., in [Studio](/langsmith/studio)) or [hosted with LangSmith](/langsmith/platform-setup), the base store is available to use by default and does not need to be specified during graph compilation. To enable semantic search, however, you \\*\\*do\\*\\* need to configure the indexing settings in your `langgraph.json` file. For example: ```json theme={null} { ... \"store\": { \"index\": { \"embed\": \"openai:text-embeddings-3-small\", \"dims\": 1536, \"fields\": [\"$\"] } } } ``` See the [deployment guide](/langsmith/semantic-search) for more details and configuration options. ## Checkpointer libraries Under the hood, checkpointing is powered by checkpointer objects that conform to [`BaseCheckpointSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver) interface. LangGraph provides several checkpointer implementations, all implemented via standalone, installable libraries: \\* `langgraph-checkpoint`: The base interface for checkpointer savers ([`BaseCheckpointSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver)) and serialization/deserialization interface ([`SerializerProtocol`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.base.SerializerProtocol)). Includes in-memory checkpointer implementation ([`InMemorySaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.memory.InMemorySaver)) for experimentation. LangGraph comes with `langgraph-checkpoint` included. \\* `langgraph-checkpoint-sqlite`: An implementation of LangGraph checkpointer that uses SQLite database ([`SqliteSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.sqlite.SqliteSaver) / [`AsyncSqliteSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.sqlite.aio.AsyncSqliteSaver)). Ideal for experimentation and local workflows. Needs to be installed separately. \\* `langgraph-checkpoint-postgres`: An advanced checkpointer that uses Postgres database ([`PostgresSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.postgres.PostgresSaver) / [`AsyncPostgresSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.postgres.aio.AsyncPostgresSaver)), used in LangSmith. Ideal for using in production. Needs to be installed separately. ### Checkpointer interface Each checkpointer conforms to [`BaseCheckpointSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver) interface and implements the following methods: \\* `.put` - Store a checkpoint with its configuration and metadata. \\* `.put\\_writes` - Store intermediate writes linked to a checkpoint (i.e. [pending writes](#pending-writes)). \\* `.get\\_tuple` - Fetch a checkpoint tuple using for a given configuration (`thread\\_id` and `checkpoint\\_id`). This is used to populate `StateSnapshot` in `graph.get\\_state()`. \\* `.list` - List checkpoints that match a given configuration and filter criteria. This is used to populate state history in `graph.get\\_state\\_history()` If the checkpointer is used with asynchronous graph execution (i.e. executing the graph via `.ainvoke`, `.astream`, `.abatch`), asynchronous versions of the above methods will be used (`.aput`, `.aput\\_writes`, `.aget\\_tuple`, `.alist`).  For running your graph asynchronously, you can use [`InMemorySaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.memory.InMemorySaver), or async versions of Sqlite/Postgres checkpointers -- [`AsyncSqliteSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.sqlite.aio.AsyncSqliteSaver) / [`AsyncPostgresSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.postgres.aio.AsyncPostgresSaver) checkpointers.  ### Serializer When checkpointers save the graph state, they need to serialize the channel values in the state. This is done using serializer objects. `langgraph\\_checkpoint` defines [protocol](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.base.SerializerProtocol) for implementing serializers provides a default implementation ([`JsonPlusSerializer`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.jsonplus.JsonPlusSerializer)) that handles a wide variety of types, including LangChain and LangGraph primitives, datetimes, enums and more. #### Serialization with `pickle` The default serializer, [`JsonPlusSerializer`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.jsonplus.JsonPlusSerializer), uses ormsgpack and JSON under the hood, which is not suitable for all types of objects. If you want to fallback to pickle for objects not currently supported by our msgpack encoder (such as Pandas dataframes), you can use the `pickle\\_fallback` argument of the [`JsonPlusSerializer`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.jsonplus.JsonPlusSerializer): ```python theme={null} from langgraph.checkpoint.memory import InMemorySaver from langgraph.checkpoint.serde.jsonplus import JsonPlusSerializer # ... Define the graph ... graph.compile( checkpointer=InMemorySaver(serde=JsonPlusSerializer(pickle\\_fallback=True)) ) ``` #### Encryption Checkpointers can optionally encrypt all persisted state. To enable this, pass an instance of [`EncryptedSerializer`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.encrypted.EncryptedSerializer) to the `serde` argument of any [`BaseCheckpointSaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.base.BaseCheckpointSaver) implementation. The easiest way to create an encrypted serializer is via [`from\\_pycryptodome\\_aes`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.encrypted.EncryptedSerializer.from\\_pycryptodome\\_aes), which reads the AES key from the `LANGGRAPH\\_AES\\_KEY` environment variable (or accepts a `key` argument): ```python theme={null} import sqlite3 from langgraph.checkpoint.serde.encrypted import EncryptedSerializer from langgraph.checkpoint.sqlite import SqliteSaver serde = EncryptedSerializer.from\\_pycryptodome\\_aes() # reads LANGGRAPH\\_AES\\_KEY checkpointer = SqliteSaver(sqlite3.connect(\"checkpoint.db\"), serde=serde) ``` ```python theme={null} from langgraph.checkpoint.serde.encrypted import EncryptedSerializer from langgraph.checkpoint.postgres import PostgresSaver serde = EncryptedSerializer.from\\_pycryptodome\\_aes() checkpointer = PostgresSaver.from\\_conn\\_string(\"postgresql://...\", serde=serde) checkpointer.setup() ``` When running on LangSmith, encryption is automatically enabled whenever `LANGGRAPH\\_AES\\_KEY` is present, so you only need to provide the environment variable. Other encryption schemes can be used by implementing [`CipherProtocol`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.base.CipherProtocol) and supplying it to [`EncryptedSerializer`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.serde.encrypted.EncryptedSerializer). ## Capabilities ### Human-in-the-loop First, checkpointers facilitate [human-in-the-loop workflows](/oss/python/langgraph/interrupts) by allowing humans to inspect, interrupt, and approve graph steps. Checkpointers are needed for these workflows as the human has to be able to view the state of a graph at any point in time, and the graph has to be to resume execution after the human has made any updates to the state. See [the how-to guides](/oss/python/langgraph/interrupts) for examples. ### Memory Second, checkpointers allow for [\"memory\"](/oss/python/concepts/memory) between interactions. In the case of repeated human interactions (like conversations) any follow up messages can be sent to that thread, which will retain its memory of previous ones. See [Add memory](/oss/python/langgraph/add-memory) for information on how to add and manage conversation memory using checkpointers. ### Time Travel Third, checkpointers allow for [\"time travel\"](/oss/python/langgraph/use-time-travel), allowing users to replay prior graph executions to review and / or debug specific graph steps. In addition, checkpointers make it possible to fork the graph state at arbitrary checkpoints to explore alternative trajectories. ### Fault-tolerance Lastly, checkpointing also provides fault-tolerance and error recovery: if one or more nodes fail at a given superstep, you can restart your graph from the last successful step. Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don't re-run the successful nodes. #### Pending writes Additionally, when a graph node fails mid-execution at a given superstep, LangGraph stores pending checkpoint writes from any other nodes that completed successfully at that superstep, so that whenever we resume graph execution from that superstep we don't re-run the successful nodes. \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/persistence.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/pregel.md",
      "title": null,
      "raw_content": "# LangGraph runtime [`Pregel`](https://reference.langchain.com/python/langgraph/pregel/) implements LangGraph's runtime, managing the execution of LangGraph applications. Compiling a [StateGraph](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) or creating an [`@entrypoint`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.entrypoint) produces a [`Pregel`](https://reference.langchain.com/python/langgraph/pregel/) instance that can be invoked with input. This guide explains the runtime at a high level and provides instructions for directly implementing applications with Pregel. > \\*\\*Note:\\*\\* The [`Pregel`](https://reference.langchain.com/python/langgraph/pregel/) runtime is named after [Google's Pregel algorithm](https://research.google/pubs/pub37252/), which describes an efficient method for large-scale parallel computation using graphs. ## Overview In LangGraph, Pregel combines [\\*\\*actors\\*\\*](https://en.wikipedia.org/wiki/Actor\\_model) and \\*\\*channels\\*\\* into a single application. \\*\\*Actors\\*\\* read data from channels and write data to channels. Pregel organizes the execution of the application into multiple steps, following the \\*\\*Pregel Algorithm\\*\\*/\\*\\*Bulk Synchronous Parallel\\*\\* model. Each step consists of three phases: \\* \\*\\*Plan\\*\\*: Determine which \\*\\*actors\\*\\* to execute in this step. For example, in the first step, select the \\*\\*actors\\*\\* that subscribe to the special \\*\\*input\\*\\* channels; in subsequent steps, select the \\*\\*actors\\*\\* that subscribe to channels updated in the previous step. \\* \\*\\*Execution\\*\\*: Execute all selected \\*\\*actors\\*\\* in parallel, until all complete, or one fails, or a timeout is reached. During this phase, channel updates are invisible to actors until the next step. \\* \\*\\*Update\\*\\*: Update the channels with the values written by the \\*\\*actors\\*\\* in this step. Repeat until no \\*\\*actors\\*\\* are selected for execution, or a maximum number of steps is reached. ## Actors An \\*\\*actor\\*\\* is a `PregelNode`. It subscribes to channels, reads data from them, and writes data to them. It can be thought of as an \\*\\*actor\\*\\* in the Pregel algorithm. `PregelNodes` implement LangChain's Runnable interface. ## Channels Channels are used to communicate between actors (PregelNodes). Each channel has a value type, an update type, and an update function \u2013 which takes a sequence of updates and modifies the stored value. Channels can be used to send data from one chain to another, or to send data from a chain to itself in a future step. LangGraph provides a number of built-in channels: \\* [`LastValue`](https://reference.langchain.com/python/langgraph/channels/#langgraph.channels.LastValue): The default channel, stores the last value sent to the channel, useful for input and output values, or for sending data from one step to the next. \\* [`Topic`](https://reference.langchain.com/python/langgraph/channels/#langgraph.channels.Topic): A configurable PubSub Topic, useful for sending multiple values between \\*\\*actors\\*\\*, or for accumulating output. Can be configured to deduplicate values or to accumulate values over the course of multiple steps. \\* [`BinaryOperatorAggregate`](https://reference.langchain.com/python/langgraph/pregel/#langgraph.pregel.Pregel--advanced-channels-context-and-binaryoperatoraggregate): stores a persistent value, updated by applying a binary operator to the current value and each update sent to the channel, useful for computing aggregates over multiple steps; e.g.,`total = BinaryOperatorAggregate(int, operator.add)` ## Examples While most users will interact with Pregel through the [StateGraph](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) API or the [`@entrypoint`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.entrypoint) decorator, it is possible to interact with Pregel directly. Below are a few different examples to give you a sense of the Pregel API.   ```python theme={null} from langgraph.channels import EphemeralValue from langgraph.pregel import Pregel, NodeBuilder node1 = ( NodeBuilder().subscribe\\_only(\"a\") .do(lambda x: x + x) .write\\_to(\"b\") ) app = Pregel( nodes={\"node1\": node1}, channels={ \"a\": EphemeralValue(str), \"b\": EphemeralValue(str), }, input\\_channels=[\"a\"], output\\_channels=[\"b\"], ) app.invoke({\"a\": \"foo\"}) ``` ```con theme={null} {'b': 'foofoo'} ```   ```python theme={null} from langgraph.channels import LastValue, EphemeralValue from langgraph.pregel import Pregel, NodeBuilder node1 = ( NodeBuilder().subscribe\\_only(\"a\") .do(lambda x: x + x) .write\\_to(\"b\") ) node2 = ( NodeBuilder().subscribe\\_only(\"b\") .do(lambda x: x + x) .write\\_to(\"c\") ) app = Pregel( nodes={\"node1\": node1, \"node2\": node2}, channels={ \"a\": EphemeralValue(str), \"b\": LastValue(str), \"c\": EphemeralValue(str), }, input\\_channels=[\"a\"], output\\_channels=[\"b\", \"c\"], ) app.invoke({\"a\": \"foo\"}) ``` ```con theme={null} {'b': 'foofoo', 'c': 'foofoofoofoo'} ```   ```python theme={null} from langgraph.channels import EphemeralValue, Topic from langgraph.pregel import Pregel, NodeBuilder node1 = ( NodeBuilder().subscribe\\_only(\"a\") .do(lambda x: x + x) .write\\_to(\"b\", \"c\") ) node2 = ( NodeBuilder().subscribe\\_to(\"b\") .do(lambda x: x[\"b\"] + x[\"b\"]) .write\\_to(\"c\") ) app = Pregel( nodes={\"node1\": node1, \"node2\": node2}, channels={ \"a\": EphemeralValue(str), \"b\": EphemeralValue(str), \"c\": Topic(str, accumulate=True), }, input\\_channels=[\"a\"], output\\_channels=[\"c\"], ) app.invoke({\"a\": \"foo\"}) ``` ```pycon theme={null} {'c': ['foofoo', 'foofoofoofoo']} ```   This example demonstrates how to use the [`BinaryOperatorAggregate`](https://reference.langchain.com/python/langgraph/pregel/#langgraph.pregel.Pregel--advanced-channels-context-and-binaryoperatoraggregate) channel to implement a reducer. ```python theme={null} from langgraph.channels import EphemeralValue, BinaryOperatorAggregate from langgraph.pregel import Pregel, NodeBuilder node1 = ( NodeBuilder().subscribe\\_only(\"a\") .do(lambda x: x + x) .write\\_to(\"b\", \"c\") ) node2 = ( NodeBuilder().subscribe\\_only(\"b\") .do(lambda x: x + x) .write\\_to(\"c\") ) def reducer(current, update): if current: return current + \" | \" + update else: return update app = Pregel( nodes={\"node1\": node1, \"node2\": node2}, channels={ \"a\": EphemeralValue(str), \"b\": EphemeralValue(str), \"c\": BinaryOperatorAggregate(str, operator=reducer), }, input\\_channels=[\"a\"], output\\_channels=[\"c\"], ) app.invoke({\"a\": \"foo\"}) ```   This example demonstrates how to introduce a cycle in the graph, by having a chain write to a channel it subscribes to. Execution will continue until a `None` value is written to the channel. ```python theme={null} from langgraph.channels import EphemeralValue from langgraph.pregel import Pregel, NodeBuilder, ChannelWriteEntry example\\_node = ( NodeBuilder().subscribe\\_only(\"value\") .do(lambda x: x + x if len(x) < 10 else None) .write\\_to(ChannelWriteEntry(\"value\", skip\\_none=True)) ) app = Pregel( nodes={\"example\\_node\": example\\_node}, channels={ \"value\": EphemeralValue(str), }, input\\_channels=[\"value\"], output\\_channels=[\"value\"], ) app.invoke({\"value\": \"a\"}) ``` ```pycon theme={null} {'value': 'aaaaaaaaaaaaaaaa'} ```   ## High-level API LangGraph provides two high-level APIs for creating a Pregel application: the [StateGraph (Graph API)](/oss/python/langgraph/graph-api) and the [Functional API](/oss/python/langgraph/functional-api).   The [StateGraph (Graph API)](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) is a higher-level abstraction that simplifies the creation of Pregel applications. It allows you to define a graph of nodes and edges. When you compile the graph, the StateGraph API automatically creates the Pregel application for you. ```python theme={null} from typing import TypedDict from langgraph.constants import START from langgraph.graph import StateGraph class Essay(TypedDict): topic: str content: str | None score: float | None def write\\_essay(essay: Essay): return { \"content\": f\"Essay about {essay['topic']}\", } def score\\_essay(essay: Essay): return { \"score\": 10 } builder = StateGraph(Essay) builder.add\\_node(write\\_essay) builder.add\\_node(score\\_essay) builder.add\\_edge(START, \"write\\_essay\") builder.add\\_edge(\"write\\_essay\", \"score\\_essay\") # Compile the graph. # This will return a Pregel instance. graph = builder.compile() ``` The compiled Pregel instance will be associated with a list of nodes and channels. You can inspect the nodes and channels by printing them. ```python theme={null} print(graph.nodes) ``` You will see something like this: ```pycon theme={null} {'\\_\\_start\\_\\_': , 'write\\_essay': , 'score\\_essay': } ``` ```python theme={null} print(graph.channels) ``` You should see something like this ```pycon theme={null} {'topic': , 'content': , 'score': , '\\_\\_start\\_\\_': , 'write\\_essay': , 'score\\_essay': , 'branch:\\_\\_start\\_\\_:\\_\\_self\\_\\_:write\\_essay': , 'branch:\\_\\_start\\_\\_:\\_\\_self\\_\\_:score\\_essay': , 'branch:write\\_essay:\\_\\_self\\_\\_:write\\_essay': , 'branch:write\\_essay:\\_\\_self\\_\\_:score\\_essay': , 'branch:score\\_essay:\\_\\_self\\_\\_:write\\_essay': , 'branch:score\\_essay:\\_\\_self\\_\\_:score\\_essay': , 'start:write\\_essay': } ```   In the [Functional API](/oss/python/langgraph/functional-api), you can use an [`@entrypoint`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.entrypoint) to create a Pregel application. The `entrypoint` decorator allows you to define a function that takes input and returns output. ```python theme={null} from typing import TypedDict from langgraph.checkpoint.memory import InMemorySaver from langgraph.func import entrypoint class Essay(TypedDict): topic: str content: str | None score: float | None checkpointer = InMemorySaver() @entrypoint(checkpointer=checkpointer) def write\\_essay(essay: Essay): return { \"content\": f\"Essay about {essay['topic']}\", } print(\"Nodes: \") print(write\\_essay.nodes) print(\"Channels: \") print(write\\_essay.channels) ``` ```pycon theme={null} Nodes: {'write\\_essay': } Channels: {'\\_\\_start\\_\\_': , '\\_\\_end\\_\\_': , '\\_\\_previous\\_\\_': } ```   \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/pregel.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/quickstart.md",
      "title": null,
      "raw_content": "# Quickstart This quickstart demonstrates how to build a calculator agent using the LangGraph Graph API or the Functional API. \\* [Use the Graph API](#use-the-graph-api) if you prefer to define your agent as a graph of nodes and edges. \\* [Use the Functional API](#use-the-functional-api) if you prefer to define your agent as a single function. For conceptual information, see [Graph API overview](/oss/python/langgraph/graph-api) and [Functional API overview](/oss/python/langgraph/functional-api).  For this example, you will need to set up a [Claude (Anthropic)](https://www.anthropic.com/) account and get an API key. Then, set the `ANTHROPIC\\_API\\_KEY` environment variable in your terminal.    ## 1. Define tools and model In this example, we'll use the Claude Sonnet 4.5 model and define tools for addition, multiplication, and division. ```python theme={null} from langchain.tools import tool from langchain.chat\\_models import init\\_chat\\_model model = init\\_chat\\_model( \"claude-sonnet-4-5-20250929\", temperature=0 ) # Define tools @tool def multiply(a: int, b: int) -> int: \"\"\"Multiply `a` and `b`. Args: a: First int b: Second int \"\"\" return a \\* b @tool def add(a: int, b: int) -> int: \"\"\"Adds `a` and `b`. Args: a: First int b: Second int \"\"\" return a + b @tool def divide(a: int, b: int) -> float: \"\"\"Divide `a` and `b`. Args: a: First int b: Second int \"\"\" return a / b # Augment the LLM with tools tools = [add, multiply, divide] tools\\_by\\_name = {tool.name: tool for tool in tools} model\\_with\\_tools = model.bind\\_tools(tools) ``` ## 2. Define state The graph's state is used to store the messages and the number of LLM calls.  State in LangGraph persists throughout the agent's execution. The `Annotated` type with `operator.add` ensures that new messages are appended to the existing list rather than replacing it.  ```python theme={null} from langchain.messages import AnyMessage from typing\\_extensions import TypedDict, Annotated import operator class MessagesState(TypedDict): messages: Annotated[list[AnyMessage], operator.add] llm\\_calls: int ``` ## 3. Define model node The model node is used to call the LLM and decide whether to call a tool or not. ```python theme={null} from langchain.messages import SystemMessage def llm\\_call(state: dict): \"\"\"LLM decides whether to call a tool or not\"\"\" return { \"messages\": [ model\\_with\\_tools.invoke( [ SystemMessage( content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\" ) ] + state[\"messages\"] ) ], \"llm\\_calls\": state.get('llm\\_calls', 0) + 1 } ``` ## 4. Define tool node The tool node is used to call the tools and return the results. ```python theme={null} from langchain.messages import ToolMessage def tool\\_node(state: dict): \"\"\"Performs the tool call\"\"\" result = [] for tool\\_call in state[\"messages\"][-1].tool\\_calls: tool = tools\\_by\\_name[tool\\_call[\"name\"]] observation = tool.invoke(tool\\_call[\"args\"]) result.append(ToolMessage(content=observation, tool\\_call\\_id=tool\\_call[\"id\"])) return {\"messages\": result} ``` ## 5. Define end logic The conditional edge function is used to route to the tool node or end based upon whether the LLM made a tool call. ```python theme={null} from typing import Literal from langgraph.graph import StateGraph, START, END def should\\_continue(state: MessagesState) -> Literal[\"tool\\_node\", END]: \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\" messages = state[\"messages\"] last\\_message = messages[-1] # If the LLM makes a tool call, then perform an action if last\\_message.tool\\_calls: return \"tool\\_node\" # Otherwise, we stop (reply to the user) return END ``` ## 6. Build and compile the agent The agent is built using the [`StateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) class and compiled using the [`compile`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.compile) method. ```python theme={null} # Build workflow agent\\_builder = StateGraph(MessagesState) # Add nodes agent\\_builder.add\\_node(\"llm\\_call\", llm\\_call) agent\\_builder.add\\_node(\"tool\\_node\", tool\\_node) # Add edges to connect nodes agent\\_builder.add\\_edge(START, \"llm\\_call\") agent\\_builder.add\\_conditional\\_edges( \"llm\\_call\", should\\_continue, [\"tool\\_node\", END] ) agent\\_builder.add\\_edge(\"tool\\_node\", \"llm\\_call\") # Compile the agent agent = agent\\_builder.compile() # Show the agent from IPython.display import Image, display display(Image(agent.get\\_graph(xray=True).draw\\_mermaid\\_png())) # Invoke from langchain.messages import HumanMessage messages = [HumanMessage(content=\"Add 3 and 4.\")] messages = agent.invoke({\"messages\": messages}) for m in messages[\"messages\"]: m.pretty\\_print() ```  To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langgraph).  Congratulations! You've built your first agent using the LangGraph Graph API.  ```python theme={null} # Step 1: Define tools and model from langchain.tools import tool from langchain.chat\\_models import init\\_chat\\_model model = init\\_chat\\_model( \"claude-sonnet-4-5-20250929\", temperature=0 ) # Define tools @tool def multiply(a: int, b: int) -> int: \"\"\"Multiply `a` and `b`. Args: a: First int b: Second int \"\"\" return a \\* b @tool def add(a: int, b: int) -> int: \"\"\"Adds `a` and `b`. Args: a: First int b: Second int \"\"\" return a + b @tool def divide(a: int, b: int) -> float: \"\"\"Divide `a` and `b`. Args: a: First int b: Second int \"\"\" return a / b # Augment the LLM with tools tools = [add, multiply, divide] tools\\_by\\_name = {tool.name: tool for tool in tools} model\\_with\\_tools = model.bind\\_tools(tools) # Step 2: Define state from langchain.messages import AnyMessage from typing\\_extensions import TypedDict, Annotated import operator class MessagesState(TypedDict): messages: Annotated[list[AnyMessage], operator.add] llm\\_calls: int # Step 3: Define model node from langchain.messages import SystemMessage def llm\\_call(state: dict): \"\"\"LLM decides whether to call a tool or not\"\"\" return { \"messages\": [ model\\_with\\_tools.invoke( [ SystemMessage( content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\" ) ] + state[\"messages\"] ) ], \"llm\\_calls\": state.get('llm\\_calls', 0) + 1 } # Step 4: Define tool node from langchain.messages import ToolMessage def tool\\_node(state: dict): \"\"\"Performs the tool call\"\"\" result = [] for tool\\_call in state[\"messages\"][-1].tool\\_calls: tool = tools\\_by\\_name[tool\\_call[\"name\"]] observation = tool.invoke(tool\\_call[\"args\"]) result.append(ToolMessage(content=observation, tool\\_call\\_id=tool\\_call[\"id\"])) return {\"messages\": result} # Step 5: Define logic to determine whether to end from typing import Literal from langgraph.graph import StateGraph, START, END # Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call def should\\_continue(state: MessagesState) -> Literal[\"tool\\_node\", END]: \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\" messages = state[\"messages\"] last\\_message = messages[-1] # If the LLM makes a tool call, then perform an action if last\\_message.tool\\_calls: return \"tool\\_node\" # Otherwise, we stop (reply to the user) return END # Step 6: Build agent # Build workflow agent\\_builder = StateGraph(MessagesState) # Add nodes agent\\_builder.add\\_node(\"llm\\_call\", llm\\_call) agent\\_builder.add\\_node(\"tool\\_node\", tool\\_node) # Add edges to connect nodes agent\\_builder.add\\_edge(START, \"llm\\_call\") agent\\_builder.add\\_conditional\\_edges( \"llm\\_call\", should\\_continue, [\"tool\\_node\", END] ) agent\\_builder.add\\_edge(\"tool\\_node\", \"llm\\_call\") # Compile the agent agent = agent\\_builder.compile() from IPython.display import Image, display # Show the agent display(Image(agent.get\\_graph(xray=True).draw\\_mermaid\\_png())) # Invoke from langchain.messages import HumanMessage messages = [HumanMessage(content=\"Add 3 and 4.\")] messages = agent.invoke({\"messages\": messages}) for m in messages[\"messages\"]: m.pretty\\_print() ```    ## 1. Define tools and model In this example, we'll use the Claude Sonnet 4.5 model and define tools for addition, multiplication, and division. ```python theme={null} from langchain.tools import tool from langchain.chat\\_models import init\\_chat\\_model model = init\\_chat\\_model( \"claude-sonnet-4-5-20250929\", temperature=0 ) # Define tools @tool def multiply(a: int, b: int) -> int: \"\"\"Multiply `a` and `b`. Args: a: First int b: Second int \"\"\" return a \\* b @tool def add(a: int, b: int) -> int: \"\"\"Adds `a` and `b`. Args: a: First int b: Second int \"\"\" return a + b @tool def divide(a: int, b: int) -> float: \"\"\"Divide `a` and `b`. Args: a: First int b: Second int \"\"\" return a / b # Augment the LLM with tools tools = [add, multiply, divide] tools\\_by\\_name = {tool.name: tool for tool in tools} model\\_with\\_tools = model.bind\\_tools(tools) from langgraph.graph import add\\_messages from langchain.messages import ( SystemMessage, HumanMessage, ToolCall, ) from langchain\\_core.messages import BaseMessage from langgraph.func import entrypoint, task ``` ## 2. Define model node The model node is used to call the LLM and decide whether to call a tool or not.  The [`@task`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.task) decorator marks a function as a task that can be executed as part of the agent. Tasks can be called synchronously or asynchronously within your entrypoint function.  ```python theme={null} @task def call\\_llm(messages: list[BaseMessage]): \"\"\"LLM decides whether to call a tool or not\"\"\" return model\\_with\\_tools.invoke( [ SystemMessage( content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\" ) ] + messages ) ``` ## 3. Define tool node The tool node is used to call the tools and return the results. ```python theme={null} @task def call\\_tool(tool\\_call: ToolCall): \"\"\"Performs the tool call\"\"\" tool = tools\\_by\\_name[tool\\_call[\"name\"]] return tool.invoke(tool\\_call) ``` ## 4. Define agent The agent is built using the [`@entrypoint`](https://reference.langchain.com/python/langgraph/func/#langgraph.func.entrypoint) function.  In the Functional API, instead of defining nodes and edges explicitly, you write standard control flow logic (loops, conditionals) within a single function.  ```python theme={null} @entrypoint() def agent(messages: list[BaseMessage]): model\\_response = call\\_llm(messages).result() while True: if not model\\_response.tool\\_calls: break # Execute tools tool\\_result\\_futures = [ call\\_tool(tool\\_call) for tool\\_call in model\\_response.tool\\_calls ] tool\\_results = [fut.result() for fut in tool\\_result\\_futures] messages = add\\_messages(messages, [model\\_response, \\*tool\\_results]) model\\_response = call\\_llm(messages).result() messages = add\\_messages(messages, model\\_response) return messages # Invoke messages = [HumanMessage(content=\"Add 3 and 4.\")] for chunk in agent.stream(messages, stream\\_mode=\"updates\"): print(chunk) print(\"\\n\") ```  To learn how to trace your agent with LangSmith, see the [LangSmith documentation](/langsmith/trace-with-langgraph).  Congratulations! You've built your first agent using the LangGraph Functional API.  ```python theme={null} # Step 1: Define tools and model from langchain.tools import tool from langchain.chat\\_models import init\\_chat\\_model model = init\\_chat\\_model( \"claude-sonnet-4-5-20250929\", temperature=0 ) # Define tools @tool def multiply(a: int, b: int) -> int: \"\"\"Multiply `a` and `b`. Args: a: First int b: Second int \"\"\" return a \\* b @tool def add(a: int, b: int) -> int: \"\"\"Adds `a` and `b`. Args: a: First int b: Second int \"\"\" return a + b @tool def divide(a: int, b: int) -> float: \"\"\"Divide `a` and `b`. Args: a: First int b: Second int \"\"\" return a / b # Augment the LLM with tools tools = [add, multiply, divide] tools\\_by\\_name = {tool.name: tool for tool in tools} model\\_with\\_tools = model.bind\\_tools(tools) from langgraph.graph import add\\_messages from langchain.messages import ( SystemMessage, HumanMessage, ToolCall, ) from langchain\\_core.messages import BaseMessage from langgraph.func import entrypoint, task # Step 2: Define model node @task def call\\_llm(messages: list[BaseMessage]): \"\"\"LLM decides whether to call a tool or not\"\"\" return model\\_with\\_tools.invoke( [ SystemMessage( content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\" ) ] + messages ) # Step 3: Define tool node @task def call\\_tool(tool\\_call: ToolCall): \"\"\"Performs the tool call\"\"\" tool = tools\\_by\\_name[tool\\_call[\"name\"]] return tool.invoke(tool\\_call) # Step 4: Define agent @entrypoint() def agent(messages: list[BaseMessage]): model\\_response = call\\_llm(messages).result() while True: if not model\\_response.tool\\_calls: break # Execute tools tool\\_result\\_futures = [ call\\_tool(tool\\_call) for tool\\_call in model\\_response.tool\\_calls ] tool\\_results = [fut.result() for fut in tool\\_result\\_futures] messages = add\\_messages(messages, [model\\_response, \\*tool\\_results]) model\\_response = call\\_llm(messages).result() messages = add\\_messages(messages, model\\_response) return messages # Invoke messages = [HumanMessage(content=\"Add 3 and 4.\")] for chunk in agent.stream(messages, stream\\_mode=\"updates\"): print(chunk) print(\"\\n\") ```    \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/quickstart.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/sql-agent.md",
      "title": null,
      "raw_content": "# Build a custom SQL agent In this tutorial we will build a custom agent that can answer questions about a SQL database using LangGraph. LangChain offers built-in [agent](/oss/python/langchain/agents) implementations, implemented using [LangGraph](/oss/python/langgraph/overview) primitives. If deeper customization is required, agents can be implemented directly in LangGraph. This guide demonstrates an example implementation of a SQL agent. You can find a tutorial building a SQL agent using higher-level LangChain abstractions [here](/oss/python/langchain/sql-agent).  Building Q\\&A systems of SQL databases requires executing model-generated SQL queries. There are inherent risks in doing this. Make sure that your database connection permissions are always scoped as narrowly as possible for your agent's needs. This will mitigate, though not eliminate, the risks of building a model-driven system.  The [prebuilt agent](/oss/python/langchain/sql-agent) lets us get started quickly, but we relied on the system prompt to constrain its behavior\u2014 for example, we instructed the agent to always start with the \"list tables\" tool, and to always run a query-checker tool before executing the query. We can enforce a higher degree of control in LangGraph by customizing the agent. Here, we implement a simple ReAct-agent setup, with dedicated nodes for specific tool-calls. We will use the same \\[state] as the pre-built agent. ### Concepts We will cover the following concepts: \\* [Tools](/oss/python/langchain/tools) for reading from SQL databases \\* The LangGraph [Graph API](/oss/python/langgraph/graph-api), including state, nodes, edges, and conditional edges. \\* [Human-in-the-loop](/oss/python/langgraph/interrupts) processes ## Setup ### Installation  ```bash pip theme={null} pip install langchain langgraph langchain-community ```  ### LangSmith Set up [LangSmith](https://smith.langchain.com) to inspect what is happening inside your chain or agent. Then set the following environment variables: ```shell theme={null} export LANGSMITH\\_TRACING=\"true\" export LANGSMITH\\_API\\_KEY=\"...\" ``` ## 1. Select an LLM Select a model that supports [tool-calling](/oss/python/integrations/providers/overview):   \ud83d\udc49 Read the [OpenAI chat model integration docs](/oss/python/integrations/chat/openai/) ```shell theme={null} pip install -U \"langchain[openai]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"OPENAI\\_API\\_KEY\"] = \"sk-...\" model = init\\_chat\\_model(\"gpt-4.1\") ``` ```python Model Class theme={null} import os from langchain\\_openai import ChatOpenAI os.environ[\"OPENAI\\_API\\_KEY\"] = \"sk-...\" model = ChatOpenAI(model=\"gpt-4.1\") ```    \ud83d\udc49 Read the [Anthropic chat model integration docs](/oss/python/integrations/chat/anthropic/) ```shell theme={null} pip install -U \"langchain[anthropic]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"ANTHROPIC\\_API\\_KEY\"] = \"sk-...\" model = init\\_chat\\_model(\"claude-sonnet-4-5-20250929\") ``` ```python Model Class theme={null} import os from langchain\\_anthropic import ChatAnthropic os.environ[\"ANTHROPIC\\_API\\_KEY\"] = \"sk-...\" model = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\") ```    \ud83d\udc49 Read the [Azure chat model integration docs](/oss/python/integrations/chat/azure\\_chat\\_openai/) ```shell theme={null} pip install -U \"langchain[openai]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"AZURE\\_OPENAI\\_API\\_KEY\"] = \"...\" os.environ[\"AZURE\\_OPENAI\\_ENDPOINT\"] = \"...\" os.environ[\"OPENAI\\_API\\_VERSION\"] = \"2025-03-01-preview\" model = init\\_chat\\_model( \"azure\\_openai:gpt-4.1\", azure\\_deployment=os.environ[\"AZURE\\_OPENAI\\_DEPLOYMENT\\_NAME\"], ) ``` ```python Model Class theme={null} import os from langchain\\_openai import AzureChatOpenAI os.environ[\"AZURE\\_OPENAI\\_API\\_KEY\"] = \"...\" os.environ[\"AZURE\\_OPENAI\\_ENDPOINT\"] = \"...\" os.environ[\"OPENAI\\_API\\_VERSION\"] = \"2025-03-01-preview\" model = AzureChatOpenAI( model=\"gpt-4.1\", azure\\_deployment=os.environ[\"AZURE\\_OPENAI\\_DEPLOYMENT\\_NAME\"] ) ```    \ud83d\udc49 Read the [Google GenAI chat model integration docs](/oss/python/integrations/chat/google\\_generative\\_ai/) ```shell theme={null} pip install -U \"langchain[google-genai]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"GOOGLE\\_API\\_KEY\"] = \"...\" model = init\\_chat\\_model(\"google\\_genai:gemini-2.5-flash-lite\") ``` ```python Model Class theme={null} import os from langchain\\_google\\_genai import ChatGoogleGenerativeAI os.environ[\"GOOGLE\\_API\\_KEY\"] = \"...\" model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\") ```    \ud83d\udc49 Read the [AWS Bedrock chat model integration docs](/oss/python/integrations/chat/bedrock/) ```shell theme={null} pip install -U \"langchain[aws]\" ```  ```python init\\_chat\\_model theme={null} from langchain.chat\\_models import init\\_chat\\_model # Follow the steps here to configure your credentials: # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html model = init\\_chat\\_model( \"anthropic.claude-3-5-sonnet-20240620-v1:0\", model\\_provider=\"bedrock\\_converse\", ) ``` ```python Model Class theme={null} from langchain\\_aws import ChatBedrock model = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\") ```   \ud83d\udc49 Read the [HuggingFace chat model integration docs](/oss/python/integrations/chat/huggingface/) ```shell theme={null} pip install -U \"langchain[huggingface]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"HUGGINGFACEHUB\\_API\\_TOKEN\"] = \"hf\\_...\" model = init\\_chat\\_model( \"microsoft/Phi-3-mini-4k-instruct\", model\\_provider=\"huggingface\", temperature=0.7, max\\_tokens=1024, ) ``` ```python Model Class theme={null} import os from langchain\\_huggingface import ChatHuggingFace, HuggingFaceEndpoint os.environ[\"HUGGINGFACEHUB\\_API\\_TOKEN\"] = \"hf\\_...\" llm = HuggingFaceEndpoint( repo\\_id=\"microsoft/Phi-3-mini-4k-instruct\", temperature=0.7, max\\_length=1024, ) model = ChatHuggingFace(llm=llm) ```     The output shown in the examples below used OpenAI. ## 2. Configure the database You will be creating a [SQLite database](https://www.sqlitetutorial.net/sqlite-sample-database/) for this tutorial. SQLite is a lightweight database that is easy to set up and use. We will be loading the `chinook` database, which is a sample database that represents a digital media store. For convenience, we have hosted the database (`Chinook.db`) on a public GCS bucket. ```python theme={null} import requests, pathlib url = \"https://storage.googleapis.com/benchmarks-artifacts/chinook/Chinook.db\" local\\_path = pathlib.Path(\"Chinook.db\") if local\\_path.exists(): print(f\"{local\\_path} already exists, skipping download.\") else: response = requests.get(url) if response.status\\_code == 200: local\\_path.write\\_bytes(response.content) print(f\"File downloaded and saved as {local\\_path}\") else: print(f\"Failed to download the file. Status code: {response.status\\_code}\") ``` We will use a handy SQL database wrapper available in the `langchain\\_community` package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results: ```python theme={null} from langchain\\_community.utilities import SQLDatabase db = SQLDatabase.from\\_uri(\"sqlite:///Chinook.db\") print(f\"Dialect: {db.dialect}\") print(f\"Available tables: {db.get\\_usable\\_table\\_names()}\") print(f'Sample output: {db.run(\"SELECT \\* FROM Artist LIMIT 5;\")}') ``` ``` Dialect: sqlite Available tables: ['Album', 'Artist', 'Customer', 'Employee', 'Genre', 'Invoice', 'InvoiceLine', 'MediaType', 'Playlist', 'PlaylistTrack', 'Track'] Sample output: [(1, 'AC/DC'), (2, 'Accept'), (3, 'Aerosmith'), (4, 'Alanis Morissette'), (5, 'Alice In Chains')] ``` ## 3. Add tools for database interactions Use the `SQLDatabase` wrapper available in the `langchain\\_community` package to interact with the database. The wrapper provides a simple interface to execute SQL queries and fetch results: ```python theme={null} from langchain\\_community.agent\\_toolkits import SQLDatabaseToolkit toolkit = SQLDatabaseToolkit(db=db, llm=model) tools = toolkit.get\\_tools() for tool in tools: print(f\"{tool.name}: {tool.description}\\n\") ``` ``` sql\\_db\\_query: Input to this tool is a detailed and correct SQL query, output is a result from the database. If the query is not correct, an error message will be returned. If an error is returned, rewrite the query, check the query, and try again. If you encounter an issue with Unknown column 'xxxx' in 'field list', use sql\\_db\\_schema to query the correct table fields. sql\\_db\\_schema: Input to this tool is a comma-separated list of tables, output is the schema and sample rows for those tables. Be sure that the tables actually exist by calling sql\\_db\\_list\\_tables first! Example Input: table1, table2, table3 sql\\_db\\_list\\_tables: Input is an empty string, output is a comma-separated list of tables in the database. sql\\_db\\_query\\_checker: Use this tool to double check if your query is correct before executing it. Always use this tool before executing a query with sql\\_db\\_query! ``` ## 4. Define application steps We construct dedicated nodes for the following steps: \\* Listing DB tables \\* Calling the \"get schema\" tool \\* Generating a query \\* Checking the query Putting these steps in dedicated nodes lets us (1) force tool-calls when needed, and (2) customize the prompts associated with each step. ```python theme={null} from typing import Literal from langchain.messages import AIMessage from langchain\\_core.runnables import RunnableConfig from langgraph.graph import END, START, MessagesState, StateGraph from langgraph.prebuilt import ToolNode get\\_schema\\_tool = next(tool for tool in tools if tool.name == \"sql\\_db\\_schema\") get\\_schema\\_node = ToolNode([get\\_schema\\_tool], name=\"get\\_schema\") run\\_query\\_tool = next(tool for tool in tools if tool.name == \"sql\\_db\\_query\") run\\_query\\_node = ToolNode([run\\_query\\_tool], name=\"run\\_query\") # Example: create a predetermined tool call def list\\_tables(state: MessagesState): tool\\_call = { \"name\": \"sql\\_db\\_list\\_tables\", \"args\": {}, \"id\": \"abc123\", \"type\": \"tool\\_call\", } tool\\_call\\_message = AIMessage(content=\"\", tool\\_calls=[tool\\_call]) list\\_tables\\_tool = next(tool for tool in tools if tool.name == \"sql\\_db\\_list\\_tables\") tool\\_message = list\\_tables\\_tool.invoke(tool\\_call) response = AIMessage(f\"Available tables: {tool\\_message.content}\") return {\"messages\": [tool\\_call\\_message, tool\\_message, response]} # Example: force a model to create a tool call def call\\_get\\_schema(state: MessagesState): # Note that LangChain enforces that all models accept `tool\\_choice=\"any\"` # as well as `tool\\_choice=`. llm\\_with\\_tools = model.bind\\_tools([get\\_schema\\_tool], tool\\_choice=\"any\") response = llm\\_with\\_tools.invoke(state[\"messages\"]) return {\"messages\": [response]} generate\\_query\\_system\\_prompt = \"\"\" You are an agent designed to interact with a SQL database. Given an input question, create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer. Unless the user specifies a specific number of examples they wish to obtain, always limit your query to at most {top\\_k} results. You can order the results by a relevant column to return the most interesting examples in the database. Never query for all the columns from a specific table, only ask for the relevant columns given the question. DO NOT make any DML statements (INSERT, UPDATE, DELETE, DROP etc.) to the database. \"\"\".format( dialect=db.dialect, top\\_k=5, ) def generate\\_query(state: MessagesState): system\\_message = { \"role\": \"system\", \"content\": generate\\_query\\_system\\_prompt, } # We do not force a tool call here, to allow the model to # respond naturally when it obtains the solution. llm\\_with\\_tools = model.bind\\_tools([run\\_query\\_tool]) response = llm\\_with\\_tools.invoke([system\\_message] + state[\"messages\"]) return {\"messages\": [response]} check\\_query\\_system\\_prompt = \"\"\" You are a SQL expert with a strong attention to detail. Double check the {dialect} query for common mistakes, including: - Using NOT IN with NULL values - Using UNION when UNION ALL should have been used - Using BETWEEN for exclusive ranges - Data type mismatch in predicates - Properly quoting identifiers - Using the correct number of arguments for functions - Casting to the correct data type - Using the proper columns for joins If there are any of the above mistakes, rewrite the query. If there are no mistakes, just reproduce the original query. You will call the appropriate tool to execute the query after running this check. \"\"\".format(dialect=db.dialect) def check\\_query(state: MessagesState): system\\_message = { \"role\": \"system\", \"content\": check\\_query\\_system\\_prompt, } # Generate an artificial user message to check tool\\_call = state[\"messages\"][-1].tool\\_calls[0] user\\_message = {\"role\": \"user\", \"content\": tool\\_call[\"args\"][\"query\"]} llm\\_with\\_tools = model.bind\\_tools([run\\_query\\_tool], tool\\_choice=\"any\") response = llm\\_with\\_tools.invoke([system\\_message, user\\_message]) response.id = state[\"messages\"][-1].id return {\"messages\": [response]} ``` ## 5. Implement the agent We can now assemble these steps into a workflow using the [Graph API](/oss/python/langgraph/graph-api). We define a [conditional edge](/oss/python/langgraph/graph-api#conditional-edges) at the query generation step that will route to the query checker if a query is generated, or end if there are no tool calls present, such that the LLM has delivered a response to the query. ```python theme={null} def should\\_continue(state: MessagesState) -> Literal[END, \"check\\_query\"]: messages = state[\"messages\"] last\\_message = messages[-1] if not last\\_message.tool\\_calls: return END else: return \"check\\_query\" builder = StateGraph(MessagesState) builder.add\\_node(list\\_tables) builder.add\\_node(call\\_get\\_schema) builder.add\\_node(get\\_schema\\_node, \"get\\_schema\") builder.add\\_node(generate\\_query) builder.add\\_node(check\\_query) builder.add\\_node(run\\_query\\_node, \"run\\_query\") builder.add\\_edge(START, \"list\\_tables\") builder.add\\_edge(\"list\\_tables\", \"call\\_get\\_schema\") builder.add\\_edge(\"call\\_get\\_schema\", \"get\\_schema\") builder.add\\_edge(\"get\\_schema\", \"generate\\_query\") builder.add\\_conditional\\_edges( \"generate\\_query\", should\\_continue, ) builder.add\\_edge(\"check\\_query\", \"run\\_query\") builder.add\\_edge(\"run\\_query\", \"generate\\_query\") agent = builder.compile() ``` We visualize the application below: ```python theme={null} from IPython.display import Image, display from langchain\\_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles display(Image(agent.get\\_graph().draw\\_mermaid\\_png())) ``` We can now invoke the graph: ```python theme={null} question = \"Which genre on average has the longest tracks?\" for step in agent.stream( {\"messages\": [{\"role\": \"user\", \"content\": question}]}, stream\\_mode=\"values\", ): step[\"messages\"][-1].pretty\\_print() ``` ``` ================================ Human Message ================================= Which genre on average has the longest tracks? ================================== Ai Message ================================== Available tables: Album, Artist, Customer, Employee, Genre, Invoice, InvoiceLine, MediaType, Playlist, PlaylistTrack, Track ================================== Ai Message ================================== Tool Calls: sql\\_db\\_schema (call\\_yzje0tj7JK3TEzDx4QnRR3lL) Call ID: call\\_yzje0tj7JK3TEzDx4QnRR3lL Args: table\\_names: Genre, Track ================================= Tool Message ================================= Name: sql\\_db\\_schema CREATE TABLE \"Genre\" ( \"GenreId\" INTEGER NOT NULL, \"Name\" NVARCHAR(120), PRIMARY KEY (\"GenreId\") ) /\\* 3 rows from Genre table: GenreId Name 1 Rock 2 Jazz 3 Metal \\*/ CREATE TABLE \"Track\" ( \"TrackId\" INTEGER NOT NULL, \"Name\" NVARCHAR(200) NOT NULL, \"AlbumId\" INTEGER, \"MediaTypeId\" INTEGER NOT NULL, \"GenreId\" INTEGER, \"Composer\" NVARCHAR(220), \"Milliseconds\" INTEGER NOT NULL, \"Bytes\" INTEGER, \"UnitPrice\" NUMERIC(10, 2) NOT NULL, PRIMARY KEY (\"TrackId\"), FOREIGN KEY(\"MediaTypeId\") REFERENCES \"MediaType\" (\"MediaTypeId\"), FOREIGN KEY(\"GenreId\") REFERENCES \"Genre\" (\"GenreId\"), FOREIGN KEY(\"AlbumId\") REFERENCES \"Album\" (\"AlbumId\") ) /\\* 3 rows from Track table: TrackId Name AlbumId MediaTypeId GenreId Composer Milliseconds Bytes UnitPrice 1 For Those About To Rock (We Salute You) 1 1 1 Angus Young, Malcolm Young, Brian Johnson 343719 11170334 0.99 2 Balls to the Wall 2 2 1 U. Dirkschneider, W. Hoffmann, H. Frank, P. Baltes, S. Kaufmann, G. Hoffmann 342562 5510424 0.99 3 Fast As a Shark 3 2 1 F. Baltes, S. Kaufman, U. Dirkscneider & W. Hoffman 230619 3990994 0.99 \\*/ ================================== Ai Message ================================== Tool Calls: sql\\_db\\_query (call\\_cb9ApLfZLSq7CWg6jd0im90b) Call ID: call\\_cb9ApLfZLSq7CWg6jd0im90b Args: query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgMilliseconds FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.GenreId ORDER BY AvgMilliseconds DESC LIMIT 5; ================================== Ai Message ================================== Tool Calls: sql\\_db\\_query (call\\_DMVALfnQ4kJsuF3Yl6jxbeAU) Call ID: call\\_DMVALfnQ4kJsuF3Yl6jxbeAU Args: query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgMilliseconds FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.GenreId ORDER BY AvgMilliseconds DESC LIMIT 5; ================================= Tool Message ================================= Name: sql\\_db\\_query [('Sci Fi & Fantasy', 2911783.0384615385), ('Science Fiction', 2625549.076923077), ('Drama', 2575283.78125), ('TV Shows', 2145041.0215053763), ('Comedy', 1585263.705882353)] ================================== Ai Message ================================== The genre with the longest tracks on average is \"Sci Fi & Fantasy,\" with an average track length of approximately 2,911,783 milliseconds. Other genres with relatively long tracks include \"Science Fiction,\" \"Drama,\" \"TV Shows,\" and \"Comedy.\" ```  See [LangSmith trace](https://smith.langchain.com/public/94b8c9ac-12f7-4692-8706-836a1f30f1ea/r) for the above run.  ## 6. Implement human-in-the-loop review It can be prudent to check the agent's SQL queries before they are executed for any unintended actions or inefficiencies. Here we leverage LangGraph's [human-in-the-loop](/oss/python/langgraph/interrupts) features to pause the run before executing a SQL query and wait for human review. Using LangGraph's [persistence layer](/oss/python/langgraph/persistence), we can pause the run indefinitely (or at least as long as the persistence layer is alive). Let's wrap the `sql\\_db\\_query` tool in a node that receives human input. We can implement this using the [interrupt](/oss/python/langgraph/interrupts) function. Below, we allow for input to approve the tool call, edit its arguments, or provide user feedback. ```python theme={null} from langchain\\_core.runnables import RunnableConfig from langchain.tools import tool from langgraph.types import interrupt @tool( run\\_query\\_tool.name, description=run\\_query\\_tool.description, args\\_schema=run\\_query\\_tool.args\\_schema ) def run\\_query\\_tool\\_with\\_interrupt(config: RunnableConfig, \\*\\*tool\\_input): request = { \"action\": run\\_query\\_tool.name, \"args\": tool\\_input, \"description\": \"Please review the tool call\" } response = interrupt([request]) # [!code highlight] # approve the tool call if response[\"type\"] == \"accept\": tool\\_response = run\\_query\\_tool.invoke(tool\\_input, config) # update tool call args elif response[\"type\"] == \"edit\": tool\\_input = response[\"args\"][\"args\"] tool\\_response = run\\_query\\_tool.invoke(tool\\_input, config) # respond to the LLM with user feedback elif response[\"type\"] == \"response\": user\\_feedback = response[\"args\"] tool\\_response = user\\_feedback else: raise ValueError(f\"Unsupported interrupt response type: {response['type']}\") return tool\\_response # Redefine the tool node to use the interrupt version run\\_query\\_node = ToolNode([run\\_query\\_tool\\_with\\_interrupt], name=\"run\\_query\") # [!code highlight] ```  The above implementation follows the [tool interrupt example](/oss/python/langgraph/interrupts#configuring-interrupts) in the broader [human-in-the-loop](/oss/python/langgraph/interrupts) guide. Refer to that guide for details and alternatives.  Let's now re-assemble our graph. We will replace the programmatic check with human review. Note that we now include a [checkpointer](/oss/python/langgraph/persistence); this is required to pause and resume the run. ```python theme={null} from langgraph.checkpoint.memory import InMemorySaver def should\\_continue(state: MessagesState) -> Literal[END, \"run\\_query\"]: messages = state[\"messages\"] last\\_message = messages[-1] if not last\\_message.tool\\_calls: return END else: return \"run\\_query\" builder = StateGraph(MessagesState) builder.add\\_node(list\\_tables) builder.add\\_node(call\\_get\\_schema) builder.add\\_node(get\\_schema\\_node, \"get\\_schema\") builder.add\\_node(generate\\_query) builder.add\\_node(run\\_query\\_node, \"run\\_query\") builder.add\\_edge(START, \"list\\_tables\") builder.add\\_edge(\"list\\_tables\", \"call\\_get\\_schema\") builder.add\\_edge(\"call\\_get\\_schema\", \"get\\_schema\") builder.add\\_edge(\"get\\_schema\", \"generate\\_query\") builder.add\\_conditional\\_edges( \"generate\\_query\", should\\_continue, ) builder.add\\_edge(\"run\\_query\", \"generate\\_query\") checkpointer = InMemorySaver() # [!code highlight] agent = builder.compile(checkpointer=checkpointer) # [!code highlight] ``` We can invoke the graph as before. This time, execution is interrupted: ```python theme={null} import json config = {\"configurable\": {\"thread\\_id\": \"1\"}} question = \"Which genre on average has the longest tracks?\" for step in agent.stream( {\"messages\": [{\"role\": \"user\", \"content\": question}]}, config, stream\\_mode=\"values\", ): if \"messages\" in step: step[\"messages\"][-1].pretty\\_print() elif \"\\_\\_interrupt\\_\\_\" in step: action = step[\"\\_\\_interrupt\\_\\_\"][0] print(\"INTERRUPTED:\") for request in action.value: print(json.dumps(request, indent=2)) else: pass ``` ``` ... INTERRUPTED: { \"action\": \"sql\\_db\\_query\", \"args\": { \"query\": \"SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgLength FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AvgLength DESC LIMIT 5;\" }, \"description\": \"Please review the tool call\" } ``` We can accept or edit the tool call using [Command](/oss/python/langgraph/use-graph-api#combine-control-flow-and-state-updates-with-command): ```python theme={null} from langgraph.types import Command for step in agent.stream( Command(resume={\"type\": \"accept\"}), # Command(resume={\"type\": \"edit\", \"args\": {\"query\": \"...\"}}), config, stream\\_mode=\"values\", ): if \"messages\" in step: step[\"messages\"][-1].pretty\\_print() elif \"\\_\\_interrupt\\_\\_\" in step: action = step[\"\\_\\_interrupt\\_\\_\"][0] print(\"INTERRUPTED:\") for request in action.value: print(json.dumps(request, indent=2)) else: pass ``` ``` ================================== Ai Message ================================== Tool Calls: sql\\_db\\_query (call\\_t4yXkD6shwdTPuelXEmY3sAY) Call ID: call\\_t4yXkD6shwdTPuelXEmY3sAY Args: query: SELECT Genre.Name, AVG(Track.Milliseconds) AS AvgLength FROM Track JOIN Genre ON Track.GenreId = Genre.GenreId GROUP BY Genre.Name ORDER BY AvgLength DESC LIMIT 5; ================================= Tool Message ================================= Name: sql\\_db\\_query [('Sci Fi & Fantasy', 2911783.0384615385), ('Science Fiction', 2625549.076923077), ('Drama', 2575283.78125), ('TV Shows', 2145041.0215053763), ('Comedy', 1585263.705882353)] ================================== Ai Message ================================== The genre with the longest average track length is \"Sci Fi & Fantasy\" with an average length of about 2,911,783 milliseconds. Other genres with long average track lengths include \"Science Fiction,\" \"Drama,\" \"TV Shows,\" and \"Comedy.\" ``` Refer to the [human-in-the-loop guide](/oss/python/langgraph/interrupts) for details. ## Next steps Check out the [Evaluate a graph](/langsmith/evaluate-graph) guide for evaluating LangGraph applications, including SQL agents like this one, using LangSmith. \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/sql-agent.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/streaming.md",
      "title": null,
      "raw_content": "# Streaming LangGraph implements a streaming system to surface real-time updates. Streaming is crucial for enhancing the responsiveness of applications built on LLMs. By displaying output progressively, even before a complete response is ready, streaming significantly improves user experience (UX), particularly when dealing with the latency of LLMs. What's possible with LangGraph streaming: \\* [\\*\\*Stream graph state\\*\\*](#stream-graph-state) \u2014 get state updates / values with `updates` and `values` modes. \\* [\\*\\*Stream subgraph outputs\\*\\*](#stream-subgraph-outputs) \u2014 include outputs from both the parent graph and any nested subgraphs. \\* [\\*\\*Stream LLM tokens\\*\\*](#messages) \u2014 capture token streams from anywhere: inside nodes, subgraphs, or tools. \\* [\\*\\*Stream custom data\\*\\*](#stream-custom-data) \u2014 send custom updates or progress signals directly from tool functions. \\* [\\*\\*Use multiple streaming modes\\*\\*](#stream-multiple-modes) \u2014 choose from `values` (full state), `updates` (state deltas), `messages` (LLM tokens + metadata), `custom` (arbitrary user data), or `debug` (detailed traces). ## Supported stream modes Pass one or more of the following stream modes as a list to the [`stream`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.stream) or [`astream`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.astream) methods: | Mode | Description | | ---------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | | `values` | Streams the full value of the state after each step of the graph. | | `updates` | Streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately. | | `custom` | Streams custom data from inside your graph nodes. | | `messages` | Streams 2-tuples (LLM token, metadata) from any graph nodes where an LLM is invoked. | | `debug` | Streams as much information as possible throughout the execution of the graph. | ## Basic usage example LangGraph graphs expose the [`stream`](https://reference.langchain.com/python/langgraph/pregel/#langgraph.pregel.Pregel.stream) (sync) and [`astream`](https://reference.langchain.com/python/langgraph/pregel/#langgraph.pregel.Pregel.astream) (async) methods to yield streamed outputs as iterators. ```python theme={null} for chunk in graph.stream(inputs, stream\\_mode=\"updates\"): print(chunk) ```  ```python theme={null} from typing import TypedDict from langgraph.graph import StateGraph, START, END class State(TypedDict): topic: str joke: str def refine\\_topic(state: State): return {\"topic\": state[\"topic\"] + \" and cats\"} def generate\\_joke(state: State): return {\"joke\": f\"This is a joke about {state['topic']}\"} graph = ( StateGraph(State) .add\\_node(refine\\_topic) .add\\_node(generate\\_joke) .add\\_edge(START, \"refine\\_topic\") .add\\_edge(\"refine\\_topic\", \"generate\\_joke\") .add\\_edge(\"generate\\_joke\", END) .compile() ) # The stream() method returns an iterator that yields streamed outputs for chunk in graph.stream( # [!code highlight] {\"topic\": \"ice cream\"}, # Set stream\\_mode=\"updates\" to stream only the updates to the graph state after each node # Other stream modes are also available. See supported stream modes for details stream\\_mode=\"updates\", # [!code highlight] ): print(chunk) ``` ```output theme={null} {'refineTopic': {'topic': 'ice cream and cats'}} {'generateJoke': {'joke': 'This is a joke about ice cream and cats'}} ```  ## Stream multiple modes You can pass a list as the `stream\\_mode` parameter to stream multiple modes at once. The streamed outputs will be tuples of `(mode, chunk)` where `mode` is the name of the stream mode and `chunk` is the data streamed by that mode. ```python theme={null} for mode, chunk in graph.stream(inputs, stream\\_mode=[\"updates\", \"custom\"]): print(chunk) ``` ## Stream graph state Use the stream modes `updates` and `values` to stream the state of the graph as it executes. \\* `updates` streams the \\*\\*updates\\*\\* to the state after each step of the graph. \\* `values` streams the \\*\\*full value\\*\\* of the state after each step of the graph. ```python theme={null} from typing import TypedDict from langgraph.graph import StateGraph, START, END class State(TypedDict): topic: str joke: str def refine\\_topic(state: State): return {\"topic\": state[\"topic\"] + \" and cats\"} def generate\\_joke(state: State): return {\"joke\": f\"This is a joke about {state['topic']}\"} graph = ( StateGraph(State) .add\\_node(refine\\_topic) .add\\_node(generate\\_joke) .add\\_edge(START, \"refine\\_topic\") .add\\_edge(\"refine\\_topic\", \"generate\\_joke\") .add\\_edge(\"generate\\_joke\", END) .compile() ) ```   Use this to stream only the \\*\\*state updates\\*\\* returned by the nodes after each step. The streamed outputs include the name of the node as well as the update. ```python theme={null} for chunk in graph.stream( {\"topic\": \"ice cream\"}, stream\\_mode=\"updates\", # [!code highlight] ): print(chunk) ```   Use this to stream the \\*\\*full state\\*\\* of the graph after each step. ```python theme={null} for chunk in graph.stream( {\"topic\": \"ice cream\"}, stream\\_mode=\"values\", # [!code highlight] ): print(chunk) ```   ## Stream subgraph outputs To include outputs from [subgraphs](/oss/python/langgraph/use-subgraphs) in the streamed outputs, you can set `subgraphs=True` in the `.stream()` method of the parent graph. This will stream outputs from both the parent graph and any subgraphs. The outputs will be streamed as tuples `(namespace, data)`, where `namespace` is a tuple with the path to the node where a subgraph is invoked, e.g. `(\"parent\\_node:\", \"child\\_node:\")`. ```python theme={null} for chunk in graph.stream( {\"foo\": \"foo\"}, # Set subgraphs=True to stream outputs from subgraphs subgraphs=True, # [!code highlight] stream\\_mode=\"updates\", ): print(chunk) ```  ```python theme={null} from langgraph.graph import START, StateGraph from typing import TypedDict # Define subgraph class SubgraphState(TypedDict): foo: str # note that this key is shared with the parent graph state bar: str def subgraph\\_node\\_1(state: SubgraphState): return {\"bar\": \"bar\"} def subgraph\\_node\\_2(state: SubgraphState): return {\"foo\": state[\"foo\"] + state[\"bar\"]} subgraph\\_builder = StateGraph(SubgraphState) subgraph\\_builder.add\\_node(subgraph\\_node\\_1) subgraph\\_builder.add\\_node(subgraph\\_node\\_2) subgraph\\_builder.add\\_edge(START, \"subgraph\\_node\\_1\") subgraph\\_builder.add\\_edge(\"subgraph\\_node\\_1\", \"subgraph\\_node\\_2\") subgraph = subgraph\\_builder.compile() # Define parent graph class ParentState(TypedDict): foo: str def node\\_1(state: ParentState): return {\"foo\": \"hi! \" + state[\"foo\"]} builder = StateGraph(ParentState) builder.add\\_node(\"node\\_1\", node\\_1) builder.add\\_node(\"node\\_2\", subgraph) builder.add\\_edge(START, \"node\\_1\") builder.add\\_edge(\"node\\_1\", \"node\\_2\") graph = builder.compile() for chunk in graph.stream( {\"foo\": \"foo\"}, stream\\_mode=\"updates\", # Set subgraphs=True to stream outputs from subgraphs subgraphs=True, # [!code highlight] ): print(chunk) ``` ``` ((), {'node\\_1': {'foo': 'hi! foo'}}) (('node\\_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2',), {'subgraph\\_node\\_1': {'bar': 'bar'}}) (('node\\_2:dfddc4ba-c3c5-6887-5012-a243b5b377c2',), {'subgraph\\_node\\_2': {'foo': 'hi! foobar'}}) ((), {'node\\_2': {'foo': 'hi! foobar'}}) ``` \\*\\*Note\\*\\* that we are receiving not just the node updates, but we also the namespaces which tell us what graph (or subgraph) we are streaming from.   ### Debugging Use the `debug` streaming mode to stream as much information as possible throughout the execution of the graph. The streamed outputs include the name of the node as well as the full state. ```python theme={null} for chunk in graph.stream( {\"topic\": \"ice cream\"}, stream\\_mode=\"debug\", # [!code highlight] ): print(chunk) ```  ## LLM tokens Use the `messages` streaming mode to stream Large Language Model (LLM) outputs \\*\\*token by token\\*\\* from any part of your graph, including nodes, tools, subgraphs, or tasks. The streamed output from [`messages` mode](#supported-stream-modes) is a tuple `(message\\_chunk, metadata)` where: \\* `message\\_chunk`: the token or message segment from the LLM. \\* `metadata`: a dictionary containing details about the graph node and LLM invocation. > If your LLM is not available as a LangChain integration, you can stream its outputs using `custom` mode instead. See [use with any LLM](#use-with-any-llm) for details.  \\*\\*Manual config required for async in Python \\< 3.11\\*\\* When using Python \\< 3.11 with async code, you must explicitly pass [`RunnableConfig`](https://reference.langchain.com/python/langchain\\_core/runnables/#langchain\\_core.runnables.RunnableConfig) to `ainvoke()` to enable proper streaming. See [Async with Python \\< 3.11](#async) for details or upgrade to Python 3.11+.  ```python theme={null} from dataclasses import dataclass from langchain.chat\\_models import init\\_chat\\_model from langgraph.graph import StateGraph, START @dataclass class MyState: topic: str joke: str = \"\" model = init\\_chat\\_model(model=\"gpt-4o-mini\") def call\\_model(state: MyState): \"\"\"Call the LLM to generate a joke about a topic\"\"\" # Note that message events are emitted even when the LLM is run using .invoke rather than .stream model\\_response = model.invoke( # [!code highlight] [ {\"role\": \"user\", \"content\": f\"Generate a joke about {state.topic}\"} ] ) return {\"joke\": model\\_response.content} graph = ( StateGraph(MyState) .add\\_node(call\\_model) .add\\_edge(START, \"call\\_model\") .compile() ) # The \"messages\" stream mode returns an iterator of tuples (message\\_chunk, metadata) # where message\\_chunk is the token streamed by the LLM and metadata is a dictionary # with information about the graph node where the LLM was called and other information for message\\_chunk, metadata in graph.stream( {\"topic\": \"ice cream\"}, stream\\_mode=\"messages\", # [!code highlight] ): if message\\_chunk.content: print(message\\_chunk.content, end=\"|\", flush=True) ``` #### Filter by LLM invocation You can associate `tags` with LLM invocations to filter the streamed tokens by LLM invocation. ```python theme={null} from langchain.chat\\_models import init\\_chat\\_model # model\\_1 is tagged with \"joke\" model\\_1 = init\\_chat\\_model(model=\"gpt-4o-mini\", tags=['joke']) # model\\_2 is tagged with \"poem\" model\\_2 = init\\_chat\\_model(model=\"gpt-4o-mini\", tags=['poem']) graph = ... # define a graph that uses these LLMs # The stream\\_mode is set to \"messages\" to stream LLM tokens # The metadata contains information about the LLM invocation, including the tags async for msg, metadata in graph.astream( {\"topic\": \"cats\"}, stream\\_mode=\"messages\", # [!code highlight] ): # Filter the streamed tokens by the tags field in the metadata to only include # the tokens from the LLM invocation with the \"joke\" tag if metadata[\"tags\"] == [\"joke\"]: print(msg.content, end=\"|\", flush=True) ```  ```python theme={null} from typing import TypedDict from langchain.chat\\_models import init\\_chat\\_model from langgraph.graph import START, StateGraph # The joke\\_model is tagged with \"joke\" joke\\_model = init\\_chat\\_model(model=\"gpt-4o-mini\", tags=[\"joke\"]) # The poem\\_model is tagged with \"poem\" poem\\_model = init\\_chat\\_model(model=\"gpt-4o-mini\", tags=[\"poem\"]) class State(TypedDict): topic: str joke: str poem: str async def call\\_model(state, config): topic = state[\"topic\"] print(\"Writing joke...\") # Note: Passing the config through explicitly is required for python < 3.11 # Since context var support wasn't added before then: https://docs.python.org/3/library/asyncio-task.html#creating-tasks # The config is passed through explicitly to ensure the context vars are propagated correctly # This is required for Python < 3.11 when using async code. Please see the async section for more details joke\\_response = await joke\\_model.ainvoke( [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}], config, ) print(\"\\n\\nWriting poem...\") poem\\_response = await poem\\_model.ainvoke( [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}], config, ) return {\"joke\": joke\\_response.content, \"poem\": poem\\_response.content} graph = ( StateGraph(State) .add\\_node(call\\_model) .add\\_edge(START, \"call\\_model\") .compile() ) # The stream\\_mode is set to \"messages\" to stream LLM tokens # The metadata contains information about the LLM invocation, including the tags async for msg, metadata in graph.astream( {\"topic\": \"cats\"}, stream\\_mode=\"messages\", ): if metadata[\"tags\"] == [\"joke\"]: print(msg.content, end=\"|\", flush=True) ```  #### Filter by node To stream tokens only from specific nodes, use `stream\\_mode=\"messages\"` and filter the outputs by the `langgraph\\_node` field in the streamed metadata: ```python theme={null} # The \"messages\" stream mode returns a tuple of (message\\_chunk, metadata) # where message\\_chunk is the token streamed by the LLM and metadata is a dictionary # with information about the graph node where the LLM was called and other information for msg, metadata in graph.stream( inputs, stream\\_mode=\"messages\", # [!code highlight] ): # Filter the streamed tokens by the langgraph\\_node field in the metadata # to only include the tokens from the specified node if msg.content and metadata[\"langgraph\\_node\"] == \"some\\_node\\_name\": ... ```  ```python theme={null} from typing import TypedDict from langgraph.graph import START, StateGraph from langchain\\_openai import ChatOpenAI model = ChatOpenAI(model=\"gpt-4o-mini\") class State(TypedDict): topic: str joke: str poem: str def write\\_joke(state: State): topic = state[\"topic\"] joke\\_response = model.invoke( [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}] ) return {\"joke\": joke\\_response.content} def write\\_poem(state: State): topic = state[\"topic\"] poem\\_response = model.invoke( [{\"role\": \"user\", \"content\": f\"Write a short poem about {topic}\"}] ) return {\"poem\": poem\\_response.content} graph = ( StateGraph(State) .add\\_node(write\\_joke) .add\\_node(write\\_poem) # write both the joke and the poem concurrently .add\\_edge(START, \"write\\_joke\") .add\\_edge(START, \"write\\_poem\") .compile() ) # The \"messages\" stream mode returns a tuple of (message\\_chunk, metadata) # where message\\_chunk is the token streamed by the LLM and metadata is a dictionary # with information about the graph node where the LLM was called and other information for msg, metadata in graph.stream( {\"topic\": \"cats\"}, stream\\_mode=\"messages\", # [!code highlight] ): # Filter the streamed tokens by the langgraph\\_node field in the metadata # to only include the tokens from the write\\_poem node if msg.content and metadata[\"langgraph\\_node\"] == \"write\\_poem\": print(msg.content, end=\"|\", flush=True) ```  ## Stream custom data To send \\*\\*custom user-defined data\\*\\* from inside a LangGraph node or tool, follow these steps: 1. Use [`get\\_stream\\_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get\\_stream\\_writer) to access the stream writer and emit custom data. 2. Set `stream\\_mode=\"custom\"` when calling `.stream()` or `.astream()` to get the custom data in the stream. You can combine multiple modes (e.g., `[\"updates\", \"custom\"]`), but at least one must be `\"custom\"`.  \\*\\*No [`get\\_stream\\_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get\\_stream\\_writer) in async for Python \\< 3.11\\*\\* In async code running on Python \\< 3.11, [`get\\_stream\\_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get\\_stream\\_writer) will not work. Instead, add a `writer` parameter to your node or tool and pass it manually. See [Async with Python \\< 3.11](#async) for usage examples.    ```python theme={null} from typing import TypedDict from langgraph.config import get\\_stream\\_writer from langgraph.graph import StateGraph, START class State(TypedDict): query: str answer: str def node(state: State): # Get the stream writer to send custom data writer = get\\_stream\\_writer() # Emit a custom key-value pair (e.g., progress update) writer({\"custom\\_key\": \"Generating custom data inside node\"}) return {\"answer\": \"some data\"} graph = ( StateGraph(State) .add\\_node(node) .add\\_edge(START, \"node\") .compile() ) inputs = {\"query\": \"example\"} # Set stream\\_mode=\"custom\" to receive the custom data in the stream for chunk in graph.stream(inputs, stream\\_mode=\"custom\"): print(chunk) ```   ```python theme={null} from langchain.tools import tool from langgraph.config import get\\_stream\\_writer @tool def query\\_database(query: str) -> str: \"\"\"Query the database.\"\"\" # Access the stream writer to send custom data writer = get\\_stream\\_writer() # [!code highlight] # Emit a custom key-value pair (e.g., progress update) writer({\"data\": \"Retrieved 0/100 records\", \"type\": \"progress\"}) # [!code highlight] # perform query # Emit another custom key-value pair writer({\"data\": \"Retrieved 100/100 records\", \"type\": \"progress\"}) return \"some-answer\" graph = ... # define a graph that uses this tool # Set stream\\_mode=\"custom\" to receive the custom data in the stream for chunk in graph.stream(inputs, stream\\_mode=\"custom\"): print(chunk) ```   ## Use with any LLM You can use `stream\\_mode=\"custom\"` to stream data from \\*\\*any LLM API\\*\\* \u2014 even if that API does \\*\\*not\\*\\* implement the LangChain chat model interface. This lets you integrate raw LLM clients or external services that provide their own streaming interfaces, making LangGraph highly flexible for custom setups. ```python theme={null} from langgraph.config import get\\_stream\\_writer def call\\_arbitrary\\_model(state): \"\"\"Example node that calls an arbitrary model and streams the output\"\"\" # Get the stream writer to send custom data writer = get\\_stream\\_writer() # [!code highlight] # Assume you have a streaming client that yields chunks # Generate LLM tokens using your custom streaming client for chunk in your\\_custom\\_streaming\\_client(state[\"topic\"]): # Use the writer to send custom data to the stream writer({\"custom\\_llm\\_chunk\": chunk}) # [!code highlight] return {\"result\": \"completed\"} graph = ( StateGraph(State) .add\\_node(call\\_arbitrary\\_model) # Add other nodes and edges as needed .compile() ) # Set stream\\_mode=\"custom\" to receive the custom data in the stream for chunk in graph.stream( {\"topic\": \"cats\"}, stream\\_mode=\"custom\", # [!code highlight] ): # The chunk will contain the custom data streamed from the llm print(chunk) ```  ```python theme={null} import operator import json from typing import TypedDict from typing\\_extensions import Annotated from langgraph.graph import StateGraph, START from openai import AsyncOpenAI openai\\_client = AsyncOpenAI() model\\_name = \"gpt-4o-mini\" async def stream\\_tokens(model\\_name: str, messages: list[dict]): response = await openai\\_client.chat.completions.create( messages=messages, model=model\\_name, stream=True ) role = None async for chunk in response: delta = chunk.choices[0].delta if delta.role is not None: role = delta.role if delta.content: yield {\"role\": role, \"content\": delta.content} # this is our tool async def get\\_items(place: str) -> str: \"\"\"Use this tool to list items one might find in a place you're asked about.\"\"\" writer = get\\_stream\\_writer() response = \"\" async for msg\\_chunk in stream\\_tokens( model\\_name, [ { \"role\": \"user\", \"content\": ( \"Can you tell me what kind of items \" f\"i might find in the following place: '{place}'. \" \"List at least 3 such items separating them by a comma. \" \"And include a brief description of each item.\" ), } ], ): response += msg\\_chunk[\"content\"] writer(msg\\_chunk) return response class State(TypedDict): messages: Annotated[list[dict], operator.add] # this is the tool-calling graph node async def call\\_tool(state: State): ai\\_message = state[\"messages\"][-1] tool\\_call = ai\\_message[\"tool\\_calls\"][-1] function\\_name = tool\\_call[\"function\"][\"name\"] if function\\_name != \"get\\_items\": raise ValueError(f\"Tool {function\\_name} not supported\") function\\_arguments = tool\\_call[\"function\"][\"arguments\"] arguments = json.loads(function\\_arguments) function\\_response = await get\\_items(\\*\\*arguments) tool\\_message = { \"tool\\_call\\_id\": tool\\_call[\"id\"], \"role\": \"tool\", \"name\": function\\_name, \"content\": function\\_response, } return {\"messages\": [tool\\_message]} graph = ( StateGraph(State) .add\\_node(call\\_tool) .add\\_edge(START, \"call\\_tool\") .compile() ) ``` Let's invoke the graph with an [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) that includes a tool call: ```python theme={null} inputs = { \"messages\": [ { \"content\": None, \"role\": \"assistant\", \"tool\\_calls\": [ { \"id\": \"1\", \"function\": { \"arguments\": '{\"place\":\"bedroom\"}', \"name\": \"get\\_items\", }, \"type\": \"function\", } ], } ] } async for chunk in graph.astream( inputs, stream\\_mode=\"custom\", ): print(chunk[\"content\"], end=\"|\", flush=True) ```  ## Disable streaming for specific chat models If your application mixes models that support streaming with those that do not, you may need to explicitly disable streaming for models that do not support it. Set `disable\\_streaming=True` when initializing the model.   ```python theme={null} from langchain.chat\\_models import init\\_chat\\_model model = init\\_chat\\_model( \"claude-sonnet-4-5-20250929\", # Set disable\\_streaming=True to disable streaming for the chat model disable\\_streaming=True # [!code highlight] ) ```   ```python theme={null} from langchain\\_openai import ChatOpenAI # Set disable\\_streaming=True to disable streaming for the chat model model = ChatOpenAI(model=\"o1-preview\", disable\\_streaming=True) ```    ### Async with Python \\< 3.11 In Python versions \\< 3.11, [asyncio tasks](https://docs.python.org/3/library/asyncio-task.html#asyncio.create\\_task) do not support the `context` parameter. This limits LangGraph ability to automatically propagate context, and affects LangGraph's streaming mechanisms in two key ways: 1. You \\*\\*must\\*\\* explicitly pass [`RunnableConfig`](https://python.langchain.com/docs/concepts/runnables/#runnableconfig) into async LLM calls (e.g., `ainvoke()`), as callbacks are not automatically propagated. 2. You \\*\\*cannot\\*\\* use [`get\\_stream\\_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get\\_stream\\_writer) in async nodes or tools \u2014 you must pass a `writer` argument directly.  ```python theme={null} from typing import TypedDict from langgraph.graph import START, StateGraph from langchain.chat\\_models import init\\_chat\\_model model = init\\_chat\\_model(model=\"gpt-4o-mini\") class State(TypedDict): topic: str joke: str # Accept config as an argument in the async node function async def call\\_model(state, config): topic = state[\"topic\"] print(\"Generating joke...\") # Pass config to model.ainvoke() to ensure proper context propagation joke\\_response = await model.ainvoke( # [!code highlight] [{\"role\": \"user\", \"content\": f\"Write a joke about {topic}\"}], config, ) return {\"joke\": joke\\_response.content} graph = ( StateGraph(State) .add\\_node(call\\_model) .add\\_edge(START, \"call\\_model\") .compile() ) # Set stream\\_mode=\"messages\" to stream LLM tokens async for chunk, metadata in graph.astream( {\"topic\": \"ice cream\"}, stream\\_mode=\"messages\", # [!code highlight] ): if chunk.content: print(chunk.content, end=\"|\", flush=True) ```   ```python theme={null} from typing import TypedDict from langgraph.types import StreamWriter class State(TypedDict): topic: str joke: str # Add writer as an argument in the function signature of the async node or tool # LangGraph will automatically pass the stream writer to the function async def generate\\_joke(state: State, writer: StreamWriter): # [!code highlight] writer({\"custom\\_key\": \"Streaming custom data while generating a joke\"}) return {\"joke\": f\"This is a joke about {state['topic']}\"} graph = ( StateGraph(State) .add\\_node(generate\\_joke) .add\\_edge(START, \"generate\\_joke\") .compile() ) # Set stream\\_mode=\"custom\" to receive the custom data in the stream # [!code highlight] async for chunk in graph.astream( {\"topic\": \"ice cream\"}, stream\\_mode=\"custom\", ): print(chunk) ```  \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/streaming.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/studio.md",
      "title": null,
      "raw_content": "# LangSmith Studio When building agents with LangChain locally, it's helpful to visualize what's happening inside your agent, interact with it in real-time, and debug issues as they occur. \\*\\*LangSmith Studio\\*\\* is a free visual interface for developing and testing your LangChain agents from your local machine. Studio connects to your locally running agent to show you each step your agent takes: the prompts sent to the model, tool calls and their results, and the final output. You can test different inputs, inspect intermediate states, and iterate on your agent's behavior without additional code or deployment. This pages describes how to set up Studio with your local LangChain agent. ## Prerequisites Before you begin, ensure you have the following: \\* \\*\\*A LangSmith account\\*\\*: Sign up (for free) or log in at [smith.langchain.com](https://smith.langchain.com). \\* \\*\\*A LangSmith API key\\*\\*: Follow the [Create an API key](/langsmith/create-account-api-key#create-an-api-key) guide. \\* If you don't want data [traced](/langsmith/observability-concepts#traces) to LangSmith, set `LANGSMITH\\_TRACING=false` in your application's `.env` file. With tracing disabled, no data leaves your local server. ## Set up local Agent server ### 1. Install the LangGraph CLI The [LangGraph CLI](/langsmith/cli) provides a local development server (also called [Agent Server](/langsmith/agent-server)) that connects your agent to Studio. ```shell theme={null} # Python >= 3.11 is required. pip install --upgrade \"langgraph-cli[inmem]\" ``` ### 2. Prepare your agent If you already have a LangChain agent, you can use it directly. This example uses a simple email agent: ```python title=\"agent.py\" theme={null} from langchain.agents import create\\_agent def send\\_email(to: str, subject: str, body: str): \"\"\"Send an email\"\"\" email = { \"to\": to, \"subject\": subject, \"body\": body } # ... email sending logic return f\"Email sent to {to}\" agent = create\\_agent( \"gpt-4o\", tools=[send\\_email], system\\_prompt=\"You are an email assistant. Always use the send\\_email tool.\", ) ``` ### 3. Environment variables Studio requires a LangSmith API key to connect your local agent. Create a `.env` file in the root of your project and add your API key from [LangSmith](https://smith.langchain.com/settings).  Ensure your `.env` file is not committed to version control, such as Git.  ```bash .env theme={null} LANGSMITH\\_API\\_KEY=lsv2... ``` ### 4. Create a LangGraph config file The LangGraph CLI uses a configuration file to locate your agent and manage dependencies. Create a `langgraph.json` file in your app's directory: ```json title=\"langgraph.json\" theme={null} { \"dependencies\": [\".\"], \"graphs\": { \"agent\": \"./src/agent.py:agent\" }, \"env\": \".env\" } ``` The [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent) function automatically returns a compiled LangGraph graph, which is what the `graphs` key expects in the configuration file.  For detailed explanations of each key in the JSON object of the configuration file, refer to the [LangGraph configuration file reference](/langsmith/cli#configuration-file).  At this point, the project structure will look like this: ```bash theme={null} my-app/ \u251c\u2500\u2500 src \u2502 \u2514\u2500\u2500 agent.py \u251c\u2500\u2500 .env \u2514\u2500\u2500 langgraph.json ``` ### 5. Install dependencies Install your project dependencies from the root directory:  ```shell pip theme={null} pip install -e . ``` ```shell uv theme={null} uv sync ```  ### 6. View your agent in Studio Start the development server to connect your agent to Studio: ```shell theme={null} langgraph dev ```  Safari blocks `localhost` connections to Studio. To work around this, run the above command with `--tunnel` to access Studio via a secure tunnel.  Once the server is running, your agent is accessible both via API at `http://127.0.0.1:2024` and through the Studio UI at `https://smith.langchain.com/studio/?baseUrl=http://127.0.0.1:2024`: With Studio connected to your local agent, you can iterate quickly on your agent's behavior. Run a test input, inspect the full execution trace including prompts, tool arguments, return values, and token/latency metrics. When something goes wrong, Studio captures exceptions with the surrounding state to help you understand what happened. The development server supports hot-reloading\u2014make changes to prompts or tool signatures in your code, and Studio reflects them immediately. Re-run conversation threads from any step to test your changes without starting over. This workflow scales from simple single-tool agents to complex multi-node graphs. For more information on how to run Studio, refer to the following guides in the [LangSmith docs](/langsmith/home): \\* [Run application](/langsmith/use-studio#run-application) \\* [Manage assistants](/langsmith/use-studio#manage-assistants) \\* [Manage threads](/langsmith/use-studio#manage-threads) \\* [Iterate on prompts](/langsmith/observability-studio) \\* [Debug LangSmith traces](/langsmith/observability-studio#debug-langsmith-traces) \\* [Add node to dataset](/langsmith/observability-studio#add-node-to-dataset) ## Video guide \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/studio.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/test.md",
      "title": null,
      "raw_content": "# Test After you've prototyped your LangGraph agent, a natural next step is to add tests. This guide covers some useful patterns you can use when writing unit tests. Note that this guide is LangGraph-specific and covers scenarios around graphs with custom structures - if you are just getting started, check out [this section](/oss/python/langchain/test/) that uses LangChain's built-in [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent) instead. ## Prerequisites First, make sure you have [`pytest`](https://docs.pytest.org/) installed: ```bash theme={null} $ pip install -U pytest ``` ## Getting started Because many LangGraph agents depend on state, a useful pattern is to create your graph before each test where you use it, then compile it within tests with a new checkpointer instance. The below example shows how this works with a simple, linear graph that progresses through `node1` and `node2`. Each node updates the single state key `my\\_key`: ```python theme={null} import pytest from typing\\_extensions import TypedDict from langgraph.graph import StateGraph, START, END from langgraph.checkpoint.memory import MemorySaver def create\\_graph() -> StateGraph: class MyState(TypedDict): my\\_key: str graph = StateGraph(MyState) graph.add\\_node(\"node1\", lambda state: {\"my\\_key\": \"hello from node1\"}) graph.add\\_node(\"node2\", lambda state: {\"my\\_key\": \"hello from node2\"}) graph.add\\_edge(START, \"node1\") graph.add\\_edge(\"node1\", \"node2\") graph.add\\_edge(\"node2\", END) return graph def test\\_basic\\_agent\\_execution() -> None: checkpointer = MemorySaver() graph = create\\_graph() compiled\\_graph = graph.compile(checkpointer=checkpointer) result = compiled\\_graph.invoke( {\"my\\_key\": \"initial\\_value\"}, config={\"configurable\": {\"thread\\_id\": \"1\"}} ) assert result[\"my\\_key\"] == \"hello from node2\" ``` ## Testing individual nodes and edges Compiled LangGraph agents expose references to each individual node as `graph.nodes`. You can take advantage of this to test individual nodes within your agent. Note that this will bypass any checkpointers passed when compiling the graph: ```python theme={null} import pytest from typing\\_extensions import TypedDict from langgraph.graph import StateGraph, START, END from langgraph.checkpoint.memory import MemorySaver def create\\_graph() -> StateGraph: class MyState(TypedDict): my\\_key: str graph = StateGraph(MyState) graph.add\\_node(\"node1\", lambda state: {\"my\\_key\": \"hello from node1\"}) graph.add\\_node(\"node2\", lambda state: {\"my\\_key\": \"hello from node2\"}) graph.add\\_edge(START, \"node1\") graph.add\\_edge(\"node1\", \"node2\") graph.add\\_edge(\"node2\", END) return graph def test\\_individual\\_node\\_execution() -> None: # Will be ignored in this example checkpointer = MemorySaver() graph = create\\_graph() compiled\\_graph = graph.compile(checkpointer=checkpointer) # Only invoke node 1 result = compiled\\_graph.nodes[\"node1\"].invoke( {\"my\\_key\": \"initial\\_value\"}, ) assert result[\"my\\_key\"] == \"hello from node1\" ``` ## Partial execution For agents made up of larger graphs, you may wish to test partial execution paths within your agent rather than the entire flow end-to-end. In some cases, it may make semantic sense to [restructure these sections as subgraphs](/oss/python/langgraph/use-subgraphs), which you can invoke in isolation as normal. However, if you do not wish to make changes to your agent graph's overall structure, you can use LangGraph's persistence mechanisms to simulate a state where your agent is paused right before the beginning of the desired section, and will pause again at the end of the desired section. The steps are as follows: 1. Compile your agent with a checkpointer (the in-memory checkpointer [`InMemorySaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.memory.InMemorySaver) will suffice for testing). 2. Call your agent's [`update\\_state`](/oss/python/langgraph/use-time-travel) method with an [`as\\_node`](/oss/python/langgraph/persistence#as-node) parameter set to the name of the node \\*before\\* the one you want to start your test. 3. Invoke your agent with the same `thread\\_id` you used to update the state and an `interrupt\\_after` parameter set to the name of the node you want to stop at. Here's an example that executes only the second and third nodes in a linear graph: ```python theme={null} import pytest from typing\\_extensions import TypedDict from langgraph.graph import StateGraph, START, END from langgraph.checkpoint.memory import MemorySaver def create\\_graph() -> StateGraph: class MyState(TypedDict): my\\_key: str graph = StateGraph(MyState) graph.add\\_node(\"node1\", lambda state: {\"my\\_key\": \"hello from node1\"}) graph.add\\_node(\"node2\", lambda state: {\"my\\_key\": \"hello from node2\"}) graph.add\\_node(\"node3\", lambda state: {\"my\\_key\": \"hello from node3\"}) graph.add\\_node(\"node4\", lambda state: {\"my\\_key\": \"hello from node4\"}) graph.add\\_edge(START, \"node1\") graph.add\\_edge(\"node1\", \"node2\") graph.add\\_edge(\"node2\", \"node3\") graph.add\\_edge(\"node3\", \"node4\") graph.add\\_edge(\"node4\", END) return graph def test\\_partial\\_execution\\_from\\_node2\\_to\\_node3() -> None: checkpointer = MemorySaver() graph = create\\_graph() compiled\\_graph = graph.compile(checkpointer=checkpointer) compiled\\_graph.update\\_state( config={ \"configurable\": { \"thread\\_id\": \"1\" } }, # The state passed into node 2 - simulating the state at # the end of node 1 values={\"my\\_key\": \"initial\\_value\"}, # Update saved state as if it came from node 1 # Execution will resume at node 2 as\\_node=\"node1\", ) result = compiled\\_graph.invoke( # Resume execution by passing None None, config={\"configurable\": {\"thread\\_id\": \"1\"}}, # Stop after node 3 so that node 4 doesn't run interrupt\\_after=\"node3\", ) assert result[\"my\\_key\"] == \"hello from node3\" ``` \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/test.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/thinking-in-langgraph.md",
      "title": null,
      "raw_content": "# Thinking in LangGraph > Learn how to think about building agents with LangGraph When you build an agent with LangGraph, you will first break it apart into discrete steps called \\*\\*nodes\\*\\*. Then, you will describe the different decisions and transitions from each of your nodes. Finally, you connect nodes together through a shared \\*\\*state\\*\\* that each node can read from and write to. In this walkthrough, we'll guide you through the thought process of building a customer support email agent with LangGraph. ## Start with the process you want to automate Imagine that you need to build an AI agent that handles customer support emails. Your product team has given you these requirements: ```txt theme={null} The agent should: - Read incoming customer emails - Classify them by urgency and topic - Search relevant documentation to answer questions - Draft appropriate responses - Escalate complex issues to human agents - Schedule follow-ups when needed Example scenarios to handle: 1. Simple product question: \"How do I reset my password?\" 2. Bug report: \"The export feature crashes when I select PDF format\" 3. Urgent billing issue: \"I was charged twice for my subscription!\" 4. Feature request: \"Can you add dark mode to the mobile app?\" 5. Complex technical issue: \"Our API integration fails intermittently with 504 errors\" ``` To implement an agent in LangGraph, you will usually follow the same five steps. ## Step 1: Map out your workflow as discrete steps Start by identifying the distinct steps in your process. Each step will become a \\*\\*node\\*\\* (a function that does one specific thing). Then, sketch how these steps connect to each other. ```mermaid theme={null} flowchart TD A[START] --> B[Read Email] B --> C[Classify Intent] C -.-> D[Doc Search] C -.-> E[Bug Track] C -.-> F[Human Review] D --> G[Draft Reply] E --> G F --> G G -.-> H[Human Review] G -.-> I[Send Reply] H --> J[END] I --> J[END] ``` The arrows in this diagram show possible paths, but the actual decision of which path to take happens inside each node. Now that we've identified the components in our workflow, let's understand what each node needs to do: \\* `Read Email`: Extract and parse the email content \\* `Classify Intent`: Use an LLM to categorize urgency and topic, then route to appropriate action \\* `Doc Search`: Query your knowledge base for relevant information \\* `Bug Track`: Create or update issue in tracking system \\* `Draft Reply`: Generate an appropriate response \\* `Human Review`: Escalate to human agent for approval or handling \\* `Send Reply`: Dispatch the email response  Notice that some nodes make decisions about where to go next (`Classify Intent`, `Draft Reply`, `Human Review`), while others always proceed to the same next step (`Read Email` always goes to `Classify Intent`, `Doc Search` always goes to `Draft Reply`).  ## Step 2: Identify what each step needs to do For each node in your graph, determine what type of operation it represents and what context it needs to work properly.   Use when you need to understand, analyze, generate text, or make reasoning decisions   Use when you need to retrieve information from external sources   Use when you need to perform external actions   Use when you need human intervention   ### LLM steps When a step needs to understand, analyze, generate text, or make reasoning decisions:   \\* Static context (prompt): Classification categories, urgency definitions, response format \\* Dynamic context (from state): Email content, sender information \\* Desired outcome: Structured classification that determines routing   \\* Static context (prompt): Tone guidelines, company policies, response templates \\* Dynamic context (from state): Classification results, search results, customer history \\* Desired outcome: Professional email response ready for review   ### Data steps When a step needs to retrieve information from external sources:   \\* Parameters: Query built from intent and topic \\* Retry strategy: Yes, with exponential backoff for transient failures \\* Caching: Could cache common queries to reduce API calls   \\* Parameters: Customer email or ID from state \\* Retry strategy: Yes, but with fallback to basic info if unavailable \\* Caching: Yes, with time-to-live to balance freshness and performance   ### Action steps When a step needs to perform an external action:   \\* When to execute node: After approval (human or automated) \\* Retry strategy: Yes, with exponential backoff for network issues \\* Should not cache: Each send is a unique action   \\* When to execute node: Always when intent is \"bug\" \\* Retry strategy: Yes, critical to not lose bug reports \\* Returns: Ticket ID to include in response   ### User input steps When a step needs human intervention:   \\* Context for decision: Original email, draft response, urgency, classification \\* Expected input format: Approval boolean plus optional edited response \\* When triggered: High urgency, complex issues, or quality concerns   ## Step 3: Design your state State is the shared [memory](/oss/python/concepts/memory) accessible to all nodes in your agent. Think of it as the notebook your agent uses to keep track of everything it learns and decides as it works through the process. ### What belongs in state? Ask yourself these questions about each piece of data:   Does it need to persist across steps? If yes, it goes in state.   Can you derive it from other data? If yes, compute it when needed instead of storing it in state.   For our email agent, we need to track: \\* The original email and sender info (can't reconstruct these later) \\* Classification results (needed by multiple later/downstream nodes) \\* Search results and customer data (expensive to re-fetch) \\* The draft response (needs to persist through review) \\* Execution metadata (for debugging and recovery) ### Keep state raw, format prompts on-demand  A key principle: your state should store raw data, not formatted text. Format prompts inside nodes when you need them.  This separation means: \\* Different nodes can format the same data differently for their needs \\* You can change prompt templates without modifying your state schema \\* Debugging is clearer \u2013 you see exactly what data each node received \\* Your agent can evolve without breaking existing state Let's define our state: ```python theme={null} from typing import TypedDict, Literal # Define the structure for email classification class EmailClassification(TypedDict): intent: Literal[\"question\", \"bug\", \"billing\", \"feature\", \"complex\"] urgency: Literal[\"low\", \"medium\", \"high\", \"critical\"] topic: str summary: str class EmailAgentState(TypedDict): # Raw email data email\\_content: str sender\\_email: str email\\_id: str # Classification result classification: EmailClassification | None # Raw search/API results search\\_results: list[str] | None # List of raw document chunks customer\\_history: dict | None # Raw customer data from CRM # Generated content draft\\_response: str | None messages: list[str] | None ``` Notice that the state contains only raw data \u2013 no prompt templates, no formatted strings, no instructions. The classification output is stored as a single dictionary, straight from the LLM. ## Step 4: Build your nodes Now we implement each step as a function. A node in LangGraph is just a Python function that takes the current state and returns updates to it. ### Handle errors appropriately Different errors need different handling strategies: | Error Type | Who Fixes It | Strategy | When to Use | | --------------------------------------------------------------- | ------------------ | ---------------------------------- | ------------------------------------------------ | | Transient errors (network issues, rate limits) | System (automatic) | Retry policy | Temporary failures that usually resolve on retry | | LLM-recoverable errors (tool failures, parsing issues) | LLM | Store error in state and loop back | LLM can see the error and adjust its approach | | User-fixable errors (missing information, unclear instructions) | Human | Pause with `interrupt()` | Need user input to proceed | | Unexpected errors | Developer | Let them bubble up | Unknown issues that need debugging |   Add a retry policy to automatically retry network issues and rate limits: ```python theme={null} from langgraph.types import RetryPolicy workflow.add\\_node( \"search\\_documentation\", search\\_documentation, retry\\_policy=RetryPolicy(max\\_attempts=3, initial\\_interval=1.0) ) ```   Store the error in state and loop back so the LLM can see what went wrong and try again: ```python theme={null} from langgraph.types import Command def execute\\_tool(state: State) -> Command[Literal[\"agent\", \"execute\\_tool\"]]: try: result = run\\_tool(state['tool\\_call']) return Command(update={\"tool\\_result\": result}, goto=\"agent\") except ToolError as e: # Let the LLM see what went wrong and try again return Command( update={\"tool\\_result\": f\"Tool error: {str(e)}\"}, goto=\"agent\" ) ```   Pause and collect information from the user when needed (like account IDs, order numbers, or clarifications): ```python theme={null} from langgraph.types import Command def lookup\\_customer\\_history(state: State) -> Command[Literal[\"draft\\_response\"]]: if not state.get('customer\\_id'): user\\_input = interrupt({ \"message\": \"Customer ID needed\", \"request\": \"Please provide the customer's account ID to look up their subscription history\" }) return Command( update={\"customer\\_id\": user\\_input['customer\\_id']}, goto=\"lookup\\_customer\\_history\" ) # Now proceed with the lookup customer\\_data = fetch\\_customer\\_history(state['customer\\_id']) return Command(update={\"customer\\_history\": customer\\_data}, goto=\"draft\\_response\") ```   Let them bubble up for debugging. Don't catch what you can't handle: ```python theme={null} def send\\_reply(state: EmailAgentState): try: email\\_service.send(state[\"draft\\_response\"]) except Exception: raise # Surface unexpected errors ```   ### Implementing our email agent nodes We'll implement each node as a simple function. Remember: nodes take state, do work, and return updates.   ```python theme={null} from typing import Literal from langgraph.graph import StateGraph, START, END from langgraph.types import interrupt, Command, RetryPolicy from langchain\\_openai import ChatOpenAI from langchain.messages import HumanMessage llm = ChatOpenAI(model=\"gpt-4\") def read\\_email(state: EmailAgentState) -> dict: \"\"\"Extract and parse email content\"\"\" # In production, this would connect to your email service return { \"messages\": [HumanMessage(content=f\"Processing email: {state['email\\_content']}\")] } def classify\\_intent(state: EmailAgentState) -> Command[Literal[\"search\\_documentation\", \"human\\_review\", \"draft\\_response\", \"bug\\_tracking\"]]: \"\"\"Use LLM to classify email intent and urgency, then route accordingly\"\"\" # Create structured LLM that returns EmailClassification dict structured\\_llm = llm.with\\_structured\\_output(EmailClassification) # Format the prompt on-demand, not stored in state classification\\_prompt = f\"\"\" Analyze this customer email and classify it: Email: {state['email\\_content']} From: {state['sender\\_email']} Provide classification including intent, urgency, topic, and summary. \"\"\" # Get structured response directly as dict classification = structured\\_llm.invoke(classification\\_prompt) # Determine next node based on classification if classification['intent'] == 'billing' or classification['urgency'] == 'critical': goto = \"human\\_review\" elif classification['intent'] in ['question', 'feature']: goto = \"search\\_documentation\" elif classification['intent'] == 'bug': goto = \"bug\\_tracking\" else: goto = \"draft\\_response\" # Store classification as a single dict in state return Command( update={\"classification\": classification}, goto=goto ) ```   ```python theme={null} def search\\_documentation(state: EmailAgentState) -> Command[Literal[\"draft\\_response\"]]: \"\"\"Search knowledge base for relevant information\"\"\" # Build search query from classification classification = state.get('classification', {}) query = f\"{classification.get('intent', '')} {classification.get('topic', '')}\" try: # Implement your search logic here # Store raw search results, not formatted text search\\_results = [ \"Reset password via Settings > Security > Change Password\", \"Password must be at least 12 characters\", \"Include uppercase, lowercase, numbers, and symbols\" ] except SearchAPIError as e: # For recoverable search errors, store error and continue search\\_results = [f\"Search temporarily unavailable: {str(e)}\"] return Command( update={\"search\\_results\": search\\_results}, # Store raw results or error goto=\"draft\\_response\" ) def bug\\_tracking(state: EmailAgentState) -> Command[Literal[\"draft\\_response\"]]: \"\"\"Create or update bug tracking ticket\"\"\" # Create ticket in your bug tracking system ticket\\_id = \"BUG-12345\" # Would be created via API return Command( update={ \"search\\_results\": [f\"Bug ticket {ticket\\_id} created\"], \"current\\_step\": \"bug\\_tracked\" }, goto=\"draft\\_response\" ) ```   ```python theme={null} def draft\\_response(state: EmailAgentState) -> Command[Literal[\"human\\_review\", \"send\\_reply\"]]: \"\"\"Generate response using context and route based on quality\"\"\" classification = state.get('classification', {}) # Format context from raw state data on-demand context\\_sections = [] if state.get('search\\_results'): # Format search results for the prompt formatted\\_docs = \"\\n\".join([f\"- {doc}\" for doc in state['search\\_results']]) context\\_sections.append(f\"Relevant documentation:\\n{formatted\\_docs}\") if state.get('customer\\_history'): # Format customer data for the prompt context\\_sections.append(f\"Customer tier: {state['customer\\_history'].get('tier', 'standard')}\") # Build the prompt with formatted context draft\\_prompt = f\"\"\" Draft a response to this customer email: {state['email\\_content']} Email intent: {classification.get('intent', 'unknown')} Urgency level: {classification.get('urgency', 'medium')} {chr(10).join(context\\_sections)} Guidelines: - Be professional and helpful - Address their specific concern - Use the provided documentation when relevant \"\"\" response = llm.invoke(draft\\_prompt) # Determine if human review needed based on urgency and intent needs\\_review = ( classification.get('urgency') in ['high', 'critical'] or classification.get('intent') == 'complex' ) # Route to appropriate next node goto = \"human\\_review\" if needs\\_review else \"send\\_reply\" return Command( update={\"draft\\_response\": response.content}, # Store only the raw response goto=goto ) def human\\_review(state: EmailAgentState) -> Command[Literal[\"send\\_reply\", END]]: \"\"\"Pause for human review using interrupt and route based on decision\"\"\" classification = state.get('classification', {}) # interrupt() must come first - any code before it will re-run on resume human\\_decision = interrupt({ \"email\\_id\": state.get('email\\_id',''), \"original\\_email\": state.get('email\\_content',''), \"draft\\_response\": state.get('draft\\_response',''), \"urgency\": classification.get('urgency'), \"intent\": classification.get('intent'), \"action\": \"Please review and approve/edit this response\" }) # Now process the human's decision if human\\_decision.get(\"approved\"): return Command( update={\"draft\\_response\": human\\_decision.get(\"edited\\_response\", state.get('draft\\_response',''))}, goto=\"send\\_reply\" ) else: # Rejection means human will handle directly return Command(update={}, goto=END) def send\\_reply(state: EmailAgentState) -> dict: \"\"\"Send the email response\"\"\" # Integrate with email service print(f\"Sending reply: {state['draft\\_response'][:100]}...\") return {} ```   ## Step 5: Wire it together Now we connect our nodes into a working graph. Since our nodes handle their own routing decisions, we only need a few essential edges. To enable [human-in-the-loop](/oss/python/langgraph/interrupts) with `interrupt()`, we need to compile with a [checkpointer](/oss/python/langgraph/persistence) to save state between runs:  ```python theme={null} from langgraph.checkpoint.memory import MemorySaver from langgraph.types import RetryPolicy # Create the graph workflow = StateGraph(EmailAgentState) # Add nodes with appropriate error handling workflow.add\\_node(\"read\\_email\", read\\_email) workflow.add\\_node(\"classify\\_intent\", classify\\_intent) # Add retry policy for nodes that might have transient failures workflow.add\\_node( \"search\\_documentation\", search\\_documentation, retry\\_policy=RetryPolicy(max\\_attempts=3) ) workflow.add\\_node(\"bug\\_tracking\", bug\\_tracking) workflow.add\\_node(\"draft\\_response\", draft\\_response) workflow.add\\_node(\"human\\_review\", human\\_review) workflow.add\\_node(\"send\\_reply\", send\\_reply) # Add only the essential edges workflow.add\\_edge(START, \"read\\_email\") workflow.add\\_edge(\"read\\_email\", \"classify\\_intent\") workflow.add\\_edge(\"send\\_reply\", END) # Compile with checkpointer for persistence, in case run graph with Local\\_Server --> Please compile without checkpointer memory = MemorySaver() app = workflow.compile(checkpointer=memory) ```  The graph structure is minimal because routing happens inside nodes through [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) objects. Each node declares where it can go using type hints like `Command[Literal[\"node1\", \"node2\"]]`, making the flow explicit and traceable. ### Try out your agent Let's run our agent with an urgent billing issue that needs human review:  ```python theme={null} # Test with an urgent billing issue initial\\_state = { \"email\\_content\": \"I was charged twice for my subscription! This is urgent!\", \"sender\\_email\": \"customer@example.com\", \"email\\_id\": \"email\\_123\", \"messages\": [] } # Run with a thread\\_id for persistence config = {\"configurable\": {\"thread\\_id\": \"customer\\_123\"}} result = app.invoke(initial\\_state, config) # The graph will pause at human\\_review print(f\"Draft ready for review: {result['draft\\_response'][:100]}...\") # When ready, provide human input to resume from langgraph.types import Command human\\_response = Command( resume={ \"approved\": True, \"edited\\_response\": \"We sincerely apologize for the double charge. I've initiated an immediate refund...\" } ) # Resume execution final\\_result = app.invoke(human\\_response, config) print(f\"Email sent successfully!\") ```  The graph pauses when it hits `interrupt()`, saves everything to the checkpointer, and waits. It can resume days later, picking up exactly where it left off. The `thread\\_id` ensures all state for this conversation is preserved together. ## Summary and next steps ### Key Insights Building this email agent has shown us the LangGraph way of thinking:   Each node does one thing well. This decomposition enables streaming progress updates, durable execution that can pause and resume, and clear debugging since you can inspect state between steps.   Store raw data, not formatted text. This lets different nodes use the same information in different ways.   They take state, do work, and return updates. When they need to make routing decisions, they specify both the state updates and the next destination.   Transient failures get retries, LLM-recoverable errors loop back with context, user-fixable problems pause for input, and unexpected errors bubble up for debugging.   The `interrupt()` function pauses execution indefinitely, saves all state, and resumes exactly where it left off when you provide input. When combined with other operations in a node, it must come first.   You define the essential connections, and your nodes handle their own routing logic. This keeps control flow explicit and traceable - you can always understand what your agent will do next by looking at the current node.   ### Advanced considerations   This section explores the trade-offs in node granularity design. Most applications can skip this and use the patterns shown above.  You might wonder: why not combine `Read Email` and `Classify Intent` into one node? Or why separate Doc Search from Draft Reply? The answer involves trade-offs between resilience and observability. \\*\\*The resilience consideration:\\*\\* LangGraph's [durable execution](/oss/python/langgraph/durable-execution) creates checkpoints at node boundaries. When a workflow resumes after an interruption or failure, it starts from the beginning of the node where execution stopped. Smaller nodes mean more frequent checkpoints, which means less work to repeat if something goes wrong. If you combine multiple operations into one large node, a failure near the end means re-executing everything from the start of that node. Why we chose this breakdown for the email agent: \\* \\*\\*Isolation of external services:\\*\\* Doc Search and Bug Track are separate nodes because they call external APIs. If the search service is slow or fails, we want to isolate that from the LLM calls. We can add retry policies to these specific nodes without affecting others. \\* \\*\\*Intermediate visibility:\\*\\* Having `Classify Intent` as its own node lets us inspect what the LLM decided before taking action. This is valuable for debugging and monitoring\u2014you can see exactly when and why the agent routes to human review. \\* \\*\\*Different failure modes:\\*\\* LLM calls, database lookups, and email sending have different retry strategies. Separate nodes let you configure these independently. \\* \\*\\*Reusability and testing:\\*\\* Smaller nodes are easier to test in isolation and reuse in other workflows. A different valid approach: You could combine `Read Email` and `Classify Intent` into a single node. You'd lose the ability to inspect the raw email before classification and would repeat both operations on any failure in that node. For most applications, the observability and debugging benefits of separate nodes are worth the trade-off. Application-level concerns: The caching discussion in Step 2 (whether to cache search results) is an application-level decision, not a LangGraph framework feature. You implement caching within your node functions based on your specific requirements\u2014LangGraph doesn't prescribe this. Performance considerations: More nodes doesn't mean slower execution. LangGraph writes checkpoints in the background by default ([async durability mode](/oss/python/langgraph/durable-execution#durability-modes)), so your graph continues running without waiting for checkpoints to complete. This means you get frequent checkpoints with minimal performance impact. You can adjust this behavior if needed\u2014use `\"exit\"` mode to checkpoint only at completion, or `\"sync\"` mode to block execution until each checkpoint is written.  ### Where to go from here This was an introduction to thinking about building agents with LangGraph. You can extend this foundation with:   Learn how to add tool approval before execution, batch approval, and other patterns   Create subgraphs for complex multi-step operations   Add streaming to show real-time progress to users   Add observability with LangSmith for debugging and monitoring   Integrate more tools for web search, database queries, and API calls   Implement retry logic with exponential backoff for failed operations   \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/thinking-in-langgraph.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/ui.md",
      "title": null,
      "raw_content": "# Agent Chat UI LangChain provides a powerful prebuilt user interface that work seamlessly with agents created using [`create\\_agent`](/oss/python/langchain/agents). This UI is designed to provide rich, interactive experiences for your agents with minimal setup, whether you're running locally or in a deployed context (such as [LangSmith](/langsmith/)). ## Agent Chat UI [Agent Chat UI](https://github.com/langchain-ai/agent-chat-ui) is a Next.js application that provides a conversational interface for interacting with any LangChain agent. It supports real-time chat, tool visualization, and advanced features like time-travel debugging and state forking. Agent Chat UI is open source and can be adapted to your application needs. ### Features  Studio automatically renders tool calls and results in an intuitive interface.   Navigate through conversation history and fork from any point   View and modify agent state at any point during execution   Built-in support for reviewing and responding to agent requests   You can use generative UI in the Agent Chat UI. For more information, see [Implement generative user interfaces with LangGraph](/langsmith/generative-ui-react).  ### Quick start The fastest way to get started is using the hosted version: 1. \\*\\*Visit [Agent Chat UI](https://agentchat.vercel.app)\\*\\* 2. \\*\\*Connect your agent\\*\\* by entering your deployment URL or local server address 3. \\*\\*Start chatting\\*\\* - the UI will automatically detect and render tool calls and interrupts ### Local development For customization or local development, you can run Agent Chat UI locally:  ```bash Use npx theme={null} # Create a new Agent Chat UI project npx create-agent-chat-app --project-name my-chat-ui cd my-chat-ui # Install dependencies and start pnpm install pnpm dev ``` ```bash Clone repository theme={null} # Clone the repository git clone https://github.com/langchain-ai/agent-chat-ui.git cd agent-chat-ui # Install dependencies and start pnpm install pnpm dev ```  ### Connect to your agent Agent Chat UI can connect to both [local](/oss/python/langgraph/studio#setup-local-agent-server) and [deployed agents](/oss/python/langgraph/deploy). After starting Agent Chat UI, you'll need to configure it to connect to your agent: 1. \\*\\*Graph ID\\*\\*: Enter your graph name (find this under `graphs` in your `langgraph.json` file) 2. \\*\\*Deployment URL\\*\\*: Your Agent server's endpoint (e.g., `http://localhost:2024` for local development, or your deployed agent's URL) 3. \\*\\*LangSmith API key (optional)\\*\\*: Add your LangSmith API key (not required if you're using a local Agent server) Once configured, Agent Chat UI will automatically fetch and display any interrupted threads from your agent.  Agent Chat UI has out-of-the-box support for rendering tool calls and tool result messages. To customize what messages are shown, see [Hiding Messages in the Chat](https://github.com/langchain-ai/agent-chat-ui?tab=readme-ov-file#hiding-messages-in-the-chat).  \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/ui.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/use-functional-api.md",
      "title": null,
      "raw_content": "# Use the functional API The [\\*\\*Functional API\\*\\*](/oss/python/langgraph/functional-api) allows you to add LangGraph's key features \u2014 [persistence](/oss/python/langgraph/persistence), [memory](/oss/python/langgraph/add-memory), [human-in-the-loop](/oss/python/langgraph/interrupts), and [streaming](/oss/python/langgraph/streaming) \u2014 to your applications with minimal changes to your existing code.  For conceptual information on the functional API, see [Functional API](/oss/python/langgraph/functional-api).  ## Creating a simple workflow When defining an `entrypoint`, input is restricted to the first argument of the function. To pass multiple inputs, you can use a dictionary. ```python theme={null} @entrypoint(checkpointer=checkpointer) def my\\_workflow(inputs: dict) -> int: value = inputs[\"value\"] another\\_value = inputs[\"another\\_value\"] ... my\\_workflow.invoke({\"value\": 1, \"another\\_value\": 2}) ```  ```python theme={null} import uuid from langgraph.func import entrypoint, task from langgraph.checkpoint.memory import InMemorySaver # Task that checks if a number is even @task def is\\_even(number: int) -> bool: return number % 2 == 0 # Task that formats a message @task def format\\_message(is\\_even: bool) -> str: return \"The number is even.\" if is\\_even else \"The number is odd.\" # Create a checkpointer for persistence checkpointer = InMemorySaver() @entrypoint(checkpointer=checkpointer) def workflow(inputs: dict) -> str: \"\"\"Simple workflow to classify a number.\"\"\" even = is\\_even(inputs[\"number\"]).result() return format\\_message(even).result() # Run the workflow with a unique thread ID config = {\"configurable\": {\"thread\\_id\": str(uuid.uuid4())}} result = workflow.invoke({\"number\": 7}, config=config) print(result) ```   This example demonstrates how to use the `@task` and `@entrypoint` decorators syntactically. Given that a checkpointer is provided, the workflow results will be persisted in the checkpointer. ```python theme={null} import uuid from langchain.chat\\_models import init\\_chat\\_model from langgraph.func import entrypoint, task from langgraph.checkpoint.memory import InMemorySaver model = init\\_chat\\_model('gpt-3.5-turbo') # Task: generate essay using an LLM @task def compose\\_essay(topic: str) -> str: \"\"\"Generate an essay about the given topic.\"\"\" return model.invoke([ {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes essays.\"}, {\"role\": \"user\", \"content\": f\"Write an essay about {topic}.\"} ]).content # Create a checkpointer for persistence checkpointer = InMemorySaver() @entrypoint(checkpointer=checkpointer) def workflow(topic: str) -> str: \"\"\"Simple workflow that generates an essay with an LLM.\"\"\" return compose\\_essay(topic).result() # Execute the workflow config = {\"configurable\": {\"thread\\_id\": str(uuid.uuid4())}} result = workflow.invoke(\"the history of flight\", config=config) print(result) ```  ## Parallel execution Tasks can be executed in parallel by invoking them concurrently and waiting for the results. This is useful for improving performance in IO bound tasks (e.g., calling APIs for LLMs). ```python theme={null} @task def add\\_one(number: int) -> int: return number + 1 @entrypoint(checkpointer=checkpointer) def graph(numbers: list[int]) -> list[str]: futures = [add\\_one(i) for i in numbers] return [f.result() for f in futures] ```  This example demonstrates how to run multiple LLM calls in parallel using `@task`. Each call generates a paragraph on a different topic, and results are joined into a single text output. ```python theme={null} import uuid from langchain.chat\\_models import init\\_chat\\_model from langgraph.func import entrypoint, task from langgraph.checkpoint.memory import InMemorySaver # Initialize the LLM model model = init\\_chat\\_model(\"gpt-3.5-turbo\") # Task that generates a paragraph about a given topic @task def generate\\_paragraph(topic: str) -> str: response = model.invoke([ {\"role\": \"system\", \"content\": \"You are a helpful assistant that writes educational paragraphs.\"}, {\"role\": \"user\", \"content\": f\"Write a paragraph about {topic}.\"} ]) return response.content # Create a checkpointer for persistence checkpointer = InMemorySaver() @entrypoint(checkpointer=checkpointer) def workflow(topics: list[str]) -> str: \"\"\"Generates multiple paragraphs in parallel and combines them.\"\"\" futures = [generate\\_paragraph(topic) for topic in topics] paragraphs = [f.result() for f in futures] return \"\\n\\n\".join(paragraphs) # Run the workflow config = {\"configurable\": {\"thread\\_id\": str(uuid.uuid4())}} result = workflow.invoke([\"quantum computing\", \"climate change\", \"history of aviation\"], config=config) print(result) ``` This example uses LangGraph's concurrency model to improve execution time, especially when tasks involve I/O like LLM completions.  ## Calling graphs The \\*\\*Functional API\\*\\* and the [\\*\\*Graph API\\*\\*](/oss/python/langgraph/graph-api) can be used together in the same application as they share the same underlying runtime. ```python theme={null} from langgraph.func import entrypoint from langgraph.graph import StateGraph builder = StateGraph() ... some\\_graph = builder.compile() @entrypoint() def some\\_workflow(some\\_input: dict) -> int: # Call a graph defined using the graph API result\\_1 = some\\_graph.invoke(...) # Call another graph defined using the graph API result\\_2 = another\\_graph.invoke(...) return { \"result\\_1\": result\\_1, \"result\\_2\": result\\_2 } ```  ```python theme={null} import uuid from typing import TypedDict from langgraph.func import entrypoint from langgraph.checkpoint.memory import InMemorySaver from langgraph.graph import StateGraph # Define the shared state type class State(TypedDict): foo: int # Define a simple transformation node def double(state: State) -> State: return {\"foo\": state[\"foo\"] \\* 2} # Build the graph using the Graph API builder = StateGraph(State) builder.add\\_node(\"double\", double) builder.set\\_entry\\_point(\"double\") graph = builder.compile() # Define the functional API workflow checkpointer = InMemorySaver() @entrypoint(checkpointer=checkpointer) def workflow(x: int) -> dict: result = graph.invoke({\"foo\": x}) return {\"bar\": result[\"foo\"]} # Execute the workflow config = {\"configurable\": {\"thread\\_id\": str(uuid.uuid4())}} print(workflow.invoke(5, config=config)) # Output: {'bar': 10} ```  ## Call other entrypoints You can call other \\*\\*entrypoints\\*\\* from within an \\*\\*entrypoint\\*\\* or a \\*\\*task\\*\\*. ```python theme={null} @entrypoint() # Will automatically use the checkpointer from the parent entrypoint def some\\_other\\_workflow(inputs: dict) -> int: return inputs[\"value\"] @entrypoint(checkpointer=checkpointer) def my\\_workflow(inputs: dict) -> int: value = some\\_other\\_workflow.invoke({\"value\": 1}) return value ```  ```python theme={null} import uuid from langgraph.func import entrypoint from langgraph.checkpoint.memory import InMemorySaver # Initialize a checkpointer checkpointer = InMemorySaver() # A reusable sub-workflow that multiplies a number @entrypoint() def multiply(inputs: dict) -> int: return inputs[\"a\"] \\* inputs[\"b\"] # Main workflow that invokes the sub-workflow @entrypoint(checkpointer=checkpointer) def main(inputs: dict) -> dict: result = multiply.invoke({\"a\": inputs[\"x\"], \"b\": inputs[\"y\"]}) return {\"product\": result} # Execute the main workflow config = {\"configurable\": {\"thread\\_id\": str(uuid.uuid4())}} print(main.invoke({\"x\": 6, \"y\": 7}, config=config)) # Output: {'product': 42} ```  ## Streaming The \\*\\*Functional API\\*\\* uses the same streaming mechanism as the \\*\\*Graph API\\*\\*. Please read the [\\*\\*streaming guide\\*\\*](/oss/python/langgraph/streaming) section for more details. Example of using the streaming API to stream both updates and custom data. ```python theme={null} from langgraph.func import entrypoint from langgraph.checkpoint.memory import InMemorySaver from langgraph.config import get\\_stream\\_writer # [!code highlight] checkpointer = InMemorySaver() @entrypoint(checkpointer=checkpointer) def main(inputs: dict) -> int: writer = get\\_stream\\_writer() # [!code highlight] writer(\"Started processing\") # [!code highlight] result = inputs[\"x\"] \\* 2 writer(f\"Result is {result}\") # [!code highlight] return result config = {\"configurable\": {\"thread\\_id\": \"abc\"}} for mode, chunk in main.stream( # [!code highlight] {\"x\": 5}, stream\\_mode=[\"custom\", \"updates\"], # [!code highlight] config=config ): print(f\"{mode}: {chunk}\") ``` 1. Import [`get\\_stream\\_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get\\_stream\\_writer) from `langgraph.config`. 2. Obtain a stream writer instance within the entrypoint. 3. Emit custom data before computation begins. 4. Emit another custom message after computing the result. 5. Use `.stream()` to process streamed output. 6. Specify which streaming modes to use. ```pycon theme={null} ('updates', {'add\\_one': 2}) ('updates', {'add\\_two': 3}) ('custom', 'hello') ('custom', 'world') ('updates', {'main': 5}) ```  \\*\\*Async with Python \\< 3.11\\*\\* If using Python \\< 3.11 and writing async code, using [`get\\_stream\\_writer`](https://reference.langchain.com/python/langgraph/config/#langgraph.config.get\\_stream\\_writer) will not work. Instead please use the `StreamWriter` class directly. See [Async with Python \\< 3.11](/oss/python/langgraph/streaming#async) for more details. ```python theme={null} from langgraph.types import StreamWriter @entrypoint(checkpointer=checkpointer) async def main(inputs: dict, writer: StreamWriter) -> int: # [!code highlight] ... ```  ## Retry policy ```python theme={null} from langgraph.checkpoint.memory import InMemorySaver from langgraph.func import entrypoint, task from langgraph.types import RetryPolicy # This variable is just used for demonstration purposes to simulate a network failure. # It's not something you will have in your actual code. attempts = 0 # Let's configure the RetryPolicy to retry on ValueError. # The default RetryPolicy is optimized for retrying specific network errors. retry\\_policy = RetryPolicy(retry\\_on=ValueError) @task(retry\\_policy=retry\\_policy) def get\\_info(): global attempts attempts += 1 if attempts < 2: raise ValueError('Failure') return \"OK\" checkpointer = InMemorySaver() @entrypoint(checkpointer=checkpointer) def main(inputs, writer): return get\\_info().result() config = { \"configurable\": { \"thread\\_id\": \"1\" } } main.invoke({'any\\_input': 'foobar'}, config=config) ``` ```pycon theme={null} 'OK' ``` ## Caching Tasks ```python theme={null} import time from langgraph.cache.memory import InMemoryCache from langgraph.func import entrypoint, task from langgraph.types import CachePolicy @task(cache\\_policy=CachePolicy(ttl=120)) # [!code highlight] def slow\\_add(x: int) -> int: time.sleep(1) return x \\* 2 @entrypoint(cache=InMemoryCache()) def main(inputs: dict) -> dict[str, int]: result1 = slow\\_add(inputs[\"x\"]).result() result2 = slow\\_add(inputs[\"x\"]).result() return {\"result1\": result1, \"result2\": result2} for chunk in main.stream({\"x\": 5}, stream\\_mode=\"updates\"): print(chunk) #> {'slow\\_add': 10} #> {'slow\\_add': 10, '\\_\\_metadata\\_\\_': {'cached': True}} #> {'main': {'result1': 10, 'result2': 10}} ``` 1. `ttl` is specified in seconds. The cache will be invalidated after this time. ## Resuming after an error ```python theme={null} import time from langgraph.checkpoint.memory import InMemorySaver from langgraph.func import entrypoint, task from langgraph.types import StreamWriter # This variable is just used for demonstration purposes to simulate a network failure. # It's not something you will have in your actual code. attempts = 0 @task() def get\\_info(): \"\"\" Simulates a task that fails once before succeeding. Raises an exception on the first attempt, then returns \"OK\" on subsequent tries. \"\"\" global attempts attempts += 1 if attempts < 2: raise ValueError(\"Failure\") # Simulate a failure on the first attempt return \"OK\" # Initialize an in-memory checkpointer for persistence checkpointer = InMemorySaver() @task def slow\\_task(): \"\"\" Simulates a slow-running task by introducing a 1-second delay. \"\"\" time.sleep(1) return \"Ran slow task.\" @entrypoint(checkpointer=checkpointer) def main(inputs, writer: StreamWriter): \"\"\" Main workflow function that runs the slow\\_task and get\\_info tasks sequentially. Parameters: - inputs: Dictionary containing workflow input values. - writer: StreamWriter for streaming custom data. The workflow first executes `slow\\_task` and then attempts to execute `get\\_info`, which will fail on the first invocation. \"\"\" slow\\_task\\_result = slow\\_task().result() # Blocking call to slow\\_task get\\_info().result() # Exception will be raised here on the first attempt return slow\\_task\\_result # Workflow execution configuration with a unique thread identifier config = { \"configurable\": { \"thread\\_id\": \"1\" # Unique identifier to track workflow execution } } # This invocation will take ~1 second due to the slow\\_task execution try: # First invocation will raise an exception due to the `get\\_info` task failing main.invoke({'any\\_input': 'foobar'}, config=config) except ValueError: pass # Handle the failure gracefully ``` When we resume execution, we won't need to re-run the `slow\\_task` as its result is already saved in the checkpoint. ```python theme={null} main.invoke(None, config=config) ``` ```pycon theme={null} 'Ran slow task.' ``` ## Human-in-the-loop The functional API supports [human-in-the-loop](/oss/python/langgraph/interrupts) workflows using the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) function and the `Command` primitive. ### Basic human-in-the-loop workflow We will create three [tasks](/oss/python/langgraph/functional-api#task): 1. Append `\"bar\"`. 2. Pause for human input. When resuming, append human input. 3. Append `\"qux\"`. ```python theme={null} from langgraph.func import entrypoint, task from langgraph.types import Command, interrupt @task def step\\_1(input\\_query): \"\"\"Append bar.\"\"\" return f\"{input\\_query} bar\" @task def human\\_feedback(input\\_query): \"\"\"Append user input.\"\"\" feedback = interrupt(f\"Please provide feedback: {input\\_query}\") return f\"{input\\_query} {feedback}\" @task def step\\_3(input\\_query): \"\"\"Append qux.\"\"\" return f\"{input\\_query} qux\" ``` We can now compose these tasks in an [entrypoint](/oss/python/langgraph/functional-api#entrypoint): ```python theme={null} from langgraph.checkpoint.memory import InMemorySaver checkpointer = InMemorySaver() @entrypoint(checkpointer=checkpointer) def graph(input\\_query): result\\_1 = step\\_1(input\\_query).result() result\\_2 = human\\_feedback(result\\_1).result() result\\_3 = step\\_3(result\\_2).result() return result\\_3 ``` [interrupt()](/oss/python/langgraph/interrupts#pause-using-interrupt) is called inside a task, enabling a human to review and edit the output of the previous task. The results of prior tasks-- in this case `step\\_1`-- are persisted, so that they are not run again following the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt). Let's send in a query string: ```python theme={null} config = {\"configurable\": {\"thread\\_id\": \"1\"}} for event in graph.stream(\"foo\", config): print(event) print(\"\\n\") ``` Note that we've paused with an [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) after `step\\_1`. The interrupt provides instructions to resume the run. To resume, we issue a [`Command`](/oss/python/langgraph/interrupts#resuming-interrupts) containing the data expected by the `human\\_feedback` task. ```python theme={null} # Continue execution for event in graph.stream(Command(resume=\"baz\"), config): print(event) print(\"\\n\") ``` After resuming, the run proceeds through the remaining step and terminates as expected. ### Review tool calls To review tool calls before execution, we add a `review\\_tool\\_call` function that calls [`interrupt`](/oss/python/langgraph/interrupts#pause-using-interrupt). When this function is called, execution will be paused until we issue a command to resume it. Given a tool call, our function will [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt) for human review. At that point we can either: \\* Accept the tool call \\* Revise the tool call and continue \\* Generate a custom tool message (e.g., instructing the model to re-format its tool call) ```python theme={null} from typing import Union def review\\_tool\\_call(tool\\_call: ToolCall) -> Union[ToolCall, ToolMessage]: \"\"\"Review a tool call, returning a validated version.\"\"\" human\\_review = interrupt( { \"question\": \"Is this correct?\", \"tool\\_call\": tool\\_call, } ) review\\_action = human\\_review[\"action\"] review\\_data = human\\_review.get(\"data\") if review\\_action == \"continue\": return tool\\_call elif review\\_action == \"update\": updated\\_tool\\_call = {\\*\\*tool\\_call, \\*\\*{\"args\": review\\_data}} return updated\\_tool\\_call elif review\\_action == \"feedback\": return ToolMessage( content=review\\_data, name=tool\\_call[\"name\"], tool\\_call\\_id=tool\\_call[\"id\"] ) ``` We can now update our [entrypoint](/oss/python/langgraph/functional-api#entrypoint) to review the generated tool calls. If a tool call is accepted or revised, we execute in the same way as before. Otherwise, we just append the [`ToolMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ToolMessage) supplied by the human. The results of prior tasks \u2014 in this case the initial model call \u2014 are persisted, so that they are not run again following the [`interrupt`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.interrupt). ```python theme={null} from langgraph.checkpoint.memory import InMemorySaver from langgraph.graph.message import add\\_messages from langgraph.types import Command, interrupt checkpointer = InMemorySaver() @entrypoint(checkpointer=checkpointer) def agent(messages, previous): if previous is not None: messages = add\\_messages(previous, messages) model\\_response = call\\_model(messages).result() while True: if not model\\_response.tool\\_calls: break # Review tool calls tool\\_results = [] tool\\_calls = [] for i, tool\\_call in enumerate(model\\_response.tool\\_calls): review = review\\_tool\\_call(tool\\_call) if isinstance(review, ToolMessage): tool\\_results.append(review) else: # is a validated tool call tool\\_calls.append(review) if review != tool\\_call: model\\_response.tool\\_calls[i] = review # update message # Execute remaining tool calls tool\\_result\\_futures = [call\\_tool(tool\\_call) for tool\\_call in tool\\_calls] remaining\\_tool\\_results = [fut.result() for fut in tool\\_result\\_futures] # Append to message list messages = add\\_messages( messages, [model\\_response, \\*tool\\_results, \\*remaining\\_tool\\_results], ) # Call model again model\\_response = call\\_model(messages).result() # Generate final response messages = add\\_messages(messages, model\\_response) return entrypoint.final(value=model\\_response, save=messages) ``` ## Short-term memory Short-term memory allows storing information across different \\*\\*invocations\\*\\* of the same \\*\\*thread id\\*\\*. See [short-term memory](/oss/python/langgraph/functional-api#short-term-memory) for more details. ### Manage checkpoints You can view and delete the information stored by the checkpointer.  #### View thread state ```python theme={null} config = { \"configurable\": { \"thread\\_id\": \"1\", # [!code highlight] # optionally provide an ID for a specific checkpoint, # otherwise the latest checkpoint is shown # \"checkpoint\\_id\": \"1f029ca3-1f5b-6704-8004-820c16b69a5a\" # [!code highlight] } } graph.get\\_state(config) # [!code highlight] ``` ``` StateSnapshot( values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today?), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]}, next=(), config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}}, metadata={ 'source': 'loop', 'writes': {'call\\_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread\\_id': '1' }, created\\_at='2025-05-05T16:01:24.680462+00:00', parent\\_config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}}, tasks=(), interrupts=() ) ```  #### View the history of the thread ```python theme={null} config = { \"configurable\": { \"thread\\_id\": \"1\" # [!code highlight] } } list(graph.get\\_state\\_history(config)) # [!code highlight] ``` ``` [ StateSnapshot( values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\"), AIMessage(content='Your name is Bob.')]}, next=(), config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1f029ca3-1f5b-6704-8004-820c16b69a5a'}}, metadata={'source': 'loop', 'writes': {'call\\_model': {'messages': AIMessage(content='Your name is Bob.')}}, 'step': 4, 'parents': {}, 'thread\\_id': '1'}, created\\_at='2025-05-05T16:01:24.680462+00:00', parent\\_config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}}, tasks=(), interrupts=() ), StateSnapshot( values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?'), HumanMessage(content=\"what's my name?\")]}, next=('call\\_model',), config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1f029ca3-1790-6b0a-8003-baf965b6a38f'}}, metadata={'source': 'loop', 'writes': None, 'step': 3, 'parents': {}, 'thread\\_id': '1'}, created\\_at='2025-05-05T16:01:23.863421+00:00', parent\\_config={...} tasks=(PregelTask(id='8ab4155e-6b15-b885-9ce5-bed69a2c305c', name='call\\_model', path=('\\_\\_pregel\\_pull', 'call\\_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Your name is Bob.')}),), interrupts=() ), StateSnapshot( values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]}, next=('\\_\\_start\\_\\_',), config={...}, metadata={'source': 'input', 'writes': {'\\_\\_start\\_\\_': {'messages': [{'role': 'user', 'content': \"what's my name?\"}]}}, 'step': 2, 'parents': {}, 'thread\\_id': '1'}, created\\_at='2025-05-05T16:01:23.863173+00:00', parent\\_config={...} tasks=(PregelTask(id='24ba39d6-6db1-4c9b-f4c5-682aeaf38dcd', name='\\_\\_start\\_\\_', path=('\\_\\_pregel\\_pull', '\\_\\_start\\_\\_'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': \"what's my name?\"}]}),), interrupts=() ), StateSnapshot( values={'messages': [HumanMessage(content=\"hi! I'm bob\"), AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')]}, next=(), config={...}, metadata={'source': 'loop', 'writes': {'call\\_model': {'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}}, 'step': 1, 'parents': {}, 'thread\\_id': '1'}, created\\_at='2025-05-05T16:01:23.862295+00:00', parent\\_config={...} tasks=(), interrupts=() ), StateSnapshot( values={'messages': [HumanMessage(content=\"hi! I'm bob\")]}, next=('call\\_model',), config={...}, metadata={'source': 'loop', 'writes': None, 'step': 0, 'parents': {}, 'thread\\_id': '1'}, created\\_at='2025-05-05T16:01:22.278960+00:00', parent\\_config={...} tasks=(PregelTask(id='8cbd75e0-3720-b056-04f7-71ac805140a0', name='call\\_model', path=('\\_\\_pregel\\_pull', 'call\\_model'), error=None, interrupts=(), state=None, result={'messages': AIMessage(content='Hi Bob! How are you doing today? Is there anything I can help you with?')}),), interrupts=() ), StateSnapshot( values={'messages': []}, next=('\\_\\_start\\_\\_',), config={'configurable': {'thread\\_id': '1', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1f029ca3-0870-6ce2-bfff-1f3f14c3e565'}}, metadata={'source': 'input', 'writes': {'\\_\\_start\\_\\_': {'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}}, 'step': -1, 'parents': {}, 'thread\\_id': '1'}, created\\_at='2025-05-05T16:01:22.277497+00:00', parent\\_config=None, tasks=(PregelTask(id='d458367b-8265-812c-18e2-33001d199ce6', name='\\_\\_start\\_\\_', path=('\\_\\_pregel\\_pull', '\\_\\_start\\_\\_'), error=None, interrupts=(), state=None, result={'messages': [{'role': 'user', 'content': \"hi! I'm bob\"}]}),), interrupts=() ) ] ``` ### Decouple return value from saved value Use `entrypoint.final` to decouple what is returned to the caller from what is persisted in the checkpoint. This is useful when: \\* You want to return a computed result (e.g., a summary or status), but save a different internal value for use on the next invocation. \\* You need to control what gets passed to the previous parameter on the next run. ```python theme={null} from langgraph.func import entrypoint from langgraph.checkpoint.memory import InMemorySaver checkpointer = InMemorySaver() @entrypoint(checkpointer=checkpointer) def accumulate(n: int, \\*, previous: int | None) -> entrypoint.final[int, int]: previous = previous or 0 total = previous + n # Return the \\*previous\\* value to the caller but save the \\*new\\* total to the checkpoint. return entrypoint.final(value=previous, save=total) config = {\"configurable\": {\"thread\\_id\": \"my-thread\"}} print(accumulate.invoke(1, config=config)) # 0 print(accumulate.invoke(2, config=config)) # 1 print(accumulate.invoke(3, config=config)) # 3 ``` ### Chatbot example An example of a simple chatbot using the functional API and the [`InMemorySaver`](https://reference.langchain.com/python/langgraph/checkpoints/#langgraph.checkpoint.memory.InMemorySaver) checkpointer. The bot is able to remember the previous conversation and continue from where it left off. ```python theme={null} from langchain.messages import BaseMessage from langgraph.graph import add\\_messages from langgraph.func import entrypoint, task from langgraph.checkpoint.memory import InMemorySaver from langchain\\_anthropic import ChatAnthropic model = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\") @task def call\\_model(messages: list[BaseMessage]): response = model.invoke(messages) return response checkpointer = InMemorySaver() @entrypoint(checkpointer=checkpointer) def workflow(inputs: list[BaseMessage], \\*, previous: list[BaseMessage]): if previous: inputs = add\\_messages(previous, inputs) response = call\\_model(inputs).result() return entrypoint.final(value=response, save=add\\_messages(inputs, response)) config = {\"configurable\": {\"thread\\_id\": \"1\"}} input\\_message = {\"role\": \"user\", \"content\": \"hi! I'm bob\"} for chunk in workflow.stream([input\\_message], config, stream\\_mode=\"values\"): chunk.pretty\\_print() input\\_message = {\"role\": \"user\", \"content\": \"what's my name?\"} for chunk in workflow.stream([input\\_message], config, stream\\_mode=\"values\"): chunk.pretty\\_print() ``` ## Long-term memory [long-term memory](/oss/python/concepts/memory#long-term-memory) allows storing information across different \\*\\*thread ids\\*\\*. This could be useful for learning information about a given user in one conversation and using it in another. ## Workflows \\* [Workflows and agent](/oss/python/langgraph/workflows-agents) guide for more examples of how to build workflows using the Functional API. ## Integrate with other libraries \\* [Add LangGraph's features to other frameworks using the functional API](/langsmith/autogen-integration): Add LangGraph features like persistence, memory and streaming to other agent frameworks that do not provide them out of the box. \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/use-functional-api.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/use-graph-api.md",
      "title": null,
      "raw_content": "# Use the graph API This guide demonstrates the basics of LangGraph's Graph API. It walks through [state](#define-and-update-state), as well as composing common graph structures such as [sequences](#create-a-sequence-of-steps), [branches](#create-branches), and [loops](#create-and-control-loops). It also covers LangGraph's control features, including the [Send API](#map-reduce-and-the-send-api) for map-reduce workflows and the [Command API](#combine-control-flow-and-state-updates-with-command) for combining state updates with \"hops\" across nodes. ## Setup Install `langgraph`:  ```bash pip theme={null} pip install -U langgraph ``` ```bash uv theme={null} uv add langgraph ```   \\*\\*Set up LangSmith for better debugging\\*\\* Sign up for [LangSmith](https://smith.langchain.com) to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started in the [docs](/langsmith/observability).  ## Define and update state Here we show how to define and update [state](/oss/python/langgraph/graph-api#state) in LangGraph. We will demonstrate: 1. How to use state to define a graph's [schema](/oss/python/langgraph/graph-api#schema) 2. How to use [reducers](/oss/python/langgraph/graph-api#reducers) to control how state updates are processed. ### Define state [State](/oss/python/langgraph/graph-api#state) in LangGraph can be a `TypedDict`, `Pydantic` model, or dataclass. Below we will use `TypedDict`. See [this section](#use-pydantic-models-for-graph-state) for detail on using Pydantic. By default, graphs will have the same input and output schema, and the state determines that schema. See [this section](#define-input-and-output-schemas) for how to define distinct input and output schemas. Let's consider a simple example using [messages](/oss/python/langgraph/graph-api#messagesstate). This represents a versatile formulation of state for many LLM applications. See our [concepts page](/oss/python/langgraph/graph-api#working-with-messages-in-graph-state) for more detail. ```python theme={null} from langchain.messages import AnyMessage from typing\\_extensions import TypedDict class State(TypedDict): messages: list[AnyMessage] extra\\_field: int ``` This state tracks a list of [message](https://python.langchain.com/docs/concepts/messages/) objects, as well as an extra integer field. ### Update state Let's build an example graph with a single node. Our [node](/oss/python/langgraph/graph-api#nodes) is just a Python function that reads our graph's state and makes updates to it. The first argument to this function will always be the state: ```python theme={null} from langchain.messages import AIMessage def node(state: State): messages = state[\"messages\"] new\\_message = AIMessage(\"Hello!\") return {\"messages\": messages + [new\\_message], \"extra\\_field\": 10} ``` This node simply appends a message to our message list, and populates an extra field.  Nodes should return updates to the state directly, instead of mutating the state.  Let's next define a simple graph containing this node. We use [`StateGraph`](/oss/python/langgraph/graph-api#stategraph) to define a graph that operates on this state. We then use [`add\\_node`](/oss/python/langgraph/graph-api#nodes) populate our graph. ```python theme={null} from langgraph.graph import StateGraph builder = StateGraph(State) builder.add\\_node(node) builder.set\\_entry\\_point(\"node\") graph = builder.compile() ``` LangGraph provides built-in utilities for visualizing your graph. Let's inspect our graph. See [this section](#visualize-your-graph) for detail on visualization. ```python theme={null} from IPython.display import Image, display display(Image(graph.get\\_graph().draw\\_mermaid\\_png())) ``` In this case, our graph just executes a single node. Let's proceed with a simple invocation: ```python theme={null} from langchain.messages import HumanMessage result = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]}) result ``` ``` {'messages': [HumanMessage(content='Hi'), AIMessage(content='Hello!')], 'extra\\_field': 10} ``` Note that: \\* We kicked off invocation by updating a single key of the state. \\* We receive the entire state in the invocation result. For convenience, we frequently inspect the content of [message objects](https://python.langchain.com/docs/concepts/messages/) via pretty-print: ```python theme={null} for message in result[\"messages\"]: message.pretty\\_print() ``` ``` ================================ Human Message ================================ Hi ================================== Ai Message ================================== Hello! ``` ### Process state updates with reducers Each key in the state can have its own independent [reducer](/oss/python/langgraph/graph-api#reducers) function, which controls how updates from nodes are applied. If no reducer function is explicitly specified then it is assumed that all updates to the key should override it. For `TypedDict` state schemas, we can define reducers by annotating the corresponding field of the state with a reducer function. In the earlier example, our node updated the `\"messages\"` key in the state by appending a message to it. Below, we add a reducer to this key, such that updates are automatically appended: ```python theme={null} from typing\\_extensions import Annotated def add(left, right): \"\"\"Can also import `add` from the `operator` built-in.\"\"\" return left + right class State(TypedDict): messages: Annotated[list[AnyMessage], add] # [!code highlight] extra\\_field: int ``` Now our node can be simplified: ```python theme={null} def node(state: State): new\\_message = AIMessage(\"Hello!\") return {\"messages\": [new\\_message], \"extra\\_field\": 10} # [!code highlight] ``` ```python theme={null} from langgraph.graph import START graph = StateGraph(State).add\\_node(node).add\\_edge(START, \"node\").compile() result = graph.invoke({\"messages\": [HumanMessage(\"Hi\")]}) for message in result[\"messages\"]: message.pretty\\_print() ``` ``` ================================ Human Message ================================ Hi ================================== Ai Message ================================== Hello! ``` #### MessagesState In practice, there are additional considerations for updating lists of messages: \\* We may wish to update an existing message in the state. \\* We may want to accept short-hands for [message formats](/oss/python/langgraph/graph-api#using-messages-in-your-graph), such as [OpenAI format](https://python.langchain.com/docs/concepts/messages/#openai-format). LangGraph includes a built-in reducer [`add\\_messages`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.message.add\\_messages) that handles these considerations: ```python theme={null} from langgraph.graph.message import add\\_messages class State(TypedDict): messages: Annotated[list[AnyMessage], add\\_messages] # [!code highlight] extra\\_field: int def node(state: State): new\\_message = AIMessage(\"Hello!\") return {\"messages\": [new\\_message], \"extra\\_field\": 10} graph = StateGraph(State).add\\_node(node).set\\_entry\\_point(\"node\").compile() ``` ```python theme={null} input\\_message = {\"role\": \"user\", \"content\": \"Hi\"} # [!code highlight] result = graph.invoke({\"messages\": [input\\_message]}) for message in result[\"messages\"]: message.pretty\\_print() ``` ``` ================================ Human Message ================================ Hi ================================== Ai Message ================================== Hello! ``` This is a versatile representation of state for applications involving [chat models](https://python.langchain.com/docs/concepts/chat\\_models/). LangGraph includes a pre-built `MessagesState` for convenience, so that we can have: ```python theme={null} from langgraph.graph import MessagesState class State(MessagesState): extra\\_field: int ``` ### Bypass reducers with `Overwrite` In some cases, you may want to bypass a reducer and directly overwrite a state value. LangGraph provides the [`Overwrite`](https://reference.langchain.com/python/langgraph/types/) type for this purpose. When a node returns a value wrapped with `Overwrite`, the reducer is bypassed and the channel is set directly to that value. This is useful when you want to reset or replace accumulated state rather than merge it with existing values. ```python theme={null} from langgraph.graph import StateGraph, START, END from langgraph.types import Overwrite from typing\\_extensions import Annotated, TypedDict import operator class State(TypedDict): messages: Annotated[list, operator.add] def add\\_message(state: State): return {\"messages\": [\"first message\"]} def replace\\_messages(state: State): # Bypass the reducer and replace the entire messages list return {\"messages\": Overwrite([\"replacement message\"])} builder = StateGraph(State) builder.add\\_node(\"add\\_message\", add\\_message) builder.add\\_node(\"replace\\_messages\", replace\\_messages) builder.add\\_edge(START, \"add\\_message\") builder.add\\_edge(\"add\\_message\", \"replace\\_messages\") builder.add\\_edge(\"replace\\_messages\", END) graph = builder.compile() result = graph.invoke({\"messages\": [\"initial\"]}) print(result[\"messages\"]) ``` ``` ['replacement message'] ``` You can also use JSON format with the special key `\"\\_\\_overwrite\\_\\_\"`: ```python theme={null} def replace\\_messages(state: State): return {\"messages\": {\"\\_\\_overwrite\\_\\_\": [\"replacement message\"]}} ```  When nodes execute in parallel, only one node can use `Overwrite` on the same state key in a given super-step. If multiple nodes attempt to overwrite the same key in the same super-step, an `InvalidUpdateError` will be raised.  ### Define input and output schemas By default, `StateGraph` operates with a single schema, and all nodes are expected to communicate using that schema. However, it's also possible to define distinct input and output schemas for a graph. When distinct schemas are specified, an internal schema will still be used for communication between nodes. The input schema ensures that the provided input matches the expected structure, while the output schema filters the internal data to return only the relevant information according to the defined output schema. Below, we'll see how to define distinct input and output schema. ```python theme={null} from langgraph.graph import StateGraph, START, END from typing\\_extensions import TypedDict # Define the schema for the input class InputState(TypedDict): question: str # Define the schema for the output class OutputState(TypedDict): answer: str # Define the overall schema, combining both input and output class OverallState(InputState, OutputState): pass # Define the node that processes the input and generates an answer def answer\\_node(state: InputState): # Example answer and an extra key return {\"answer\": \"bye\", \"question\": state[\"question\"]} # Build the graph with input and output schemas specified builder = StateGraph(OverallState, input\\_schema=InputState, output\\_schema=OutputState) builder.add\\_node(answer\\_node) # Add the answer node builder.add\\_edge(START, \"answer\\_node\") # Define the starting edge builder.add\\_edge(\"answer\\_node\", END) # Define the ending edge graph = builder.compile() # Compile the graph # Invoke the graph with an input and print the result print(graph.invoke({\"question\": \"hi\"})) ``` ``` {'answer': 'bye'} ``` Notice that the output of invoke only includes the output schema. ### Pass private state between nodes In some cases, you may want nodes to exchange information that is crucial for intermediate logic but doesn't need to be part of the main schema of the graph. This private data is not relevant to the overall input/output of the graph and should only be shared between certain nodes. Below, we'll create an example sequential graph consisting of three nodes (node\\\\_1, node\\\\_2 and node\\\\_3), where private data is passed between the first two steps (node\\\\_1 and node\\\\_2), while the third step (node\\\\_3) only has access to the public overall state. ```python theme={null} from langgraph.graph import StateGraph, START, END from typing\\_extensions import TypedDict # The overall state of the graph (this is the public state shared across nodes) class OverallState(TypedDict): a: str # Output from node\\_1 contains private data that is not part of the overall state class Node1Output(TypedDict): private\\_data: str # The private data is only shared between node\\_1 and node\\_2 def node\\_1(state: OverallState) -> Node1Output: output = {\"private\\_data\": \"set by node\\_1\"} print(f\"Entered node `node\\_1`:\\n\\tInput: {state}.\\n\\tReturned: {output}\") return output # Node 2 input only requests the private data available after node\\_1 class Node2Input(TypedDict): private\\_data: str def node\\_2(state: Node2Input) -> OverallState: output = {\"a\": \"set by node\\_2\"} print(f\"Entered node `node\\_2`:\\n\\tInput: {state}.\\n\\tReturned: {output}\") return output # Node 3 only has access to the overall state (no access to private data from node\\_1) def node\\_3(state: OverallState) -> OverallState: output = {\"a\": \"set by node\\_3\"} print(f\"Entered node `node\\_3`:\\n\\tInput: {state}.\\n\\tReturned: {output}\") return output # Connect nodes in a sequence # node\\_2 accepts private data from node\\_1, whereas # node\\_3 does not see the private data. builder = StateGraph(OverallState).add\\_sequence([node\\_1, node\\_2, node\\_3]) builder.add\\_edge(START, \"node\\_1\") graph = builder.compile() # Invoke the graph with the initial state response = graph.invoke( { \"a\": \"set at start\", } ) print() print(f\"Output of graph invocation: {response}\") ``` ``` Entered node `node\\_1`: Input: {'a': 'set at start'}. Returned: {'private\\_data': 'set by node\\_1'} Entered node `node\\_2`: Input: {'private\\_data': 'set by node\\_1'}. Returned: {'a': 'set by node\\_2'} Entered node `node\\_3`: Input: {'a': 'set by node\\_2'}. Returned: {'a': 'set by node\\_3'} Output of graph invocation: {'a': 'set by node\\_3'} ``` ### Use Pydantic models for graph state A [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs.md#langgraph.graph.StateGraph) accepts a [`state\\_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state\\_schema) argument on initialization that specifies the \"shape\" of the state that the nodes in the graph can access and update. In our examples, we typically use a python-native `TypedDict` or [`dataclass`](https://docs.python.org/3/library/dataclasses.html) for `state\\_schema`, but [`state\\_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state\\_schema) can be any [type](https://docs.python.org/3/library/stdtypes.html#type-objects). Here, we'll see how a [Pydantic BaseModel](https://docs.pydantic.dev/latest/api/base\\_model/) can be used for [`state\\_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state\\_schema) to add run-time validation on \\*\\*inputs\\*\\*.  \\*\\*Known Limitations\\*\\* \\* Currently, the output of the graph will \\*\\*NOT\\*\\* be an instance of a pydantic model. \\* Run-time validation only occurs on inputs to the first node in the graph, not on subsequent nodes or outputs. \\* The validation error trace from pydantic does not show which node the error arises in. \\* Pydantic's recursive validation can be slow. For performance-sensitive applications, you may want to consider using a `dataclass` instead.  ```python theme={null} from langgraph.graph import StateGraph, START, END from typing\\_extensions import TypedDict from pydantic import BaseModel # The overall state of the graph (this is the public state shared across nodes) class OverallState(BaseModel): a: str def node(state: OverallState): return {\"a\": \"goodbye\"} # Build the state graph builder = StateGraph(OverallState) builder.add\\_node(node) # node\\_1 is the first node builder.add\\_edge(START, \"node\") # Start the graph with node\\_1 builder.add\\_edge(\"node\", END) # End the graph after node\\_1 graph = builder.compile() # Test the graph with a valid input graph.invoke({\"a\": \"hello\"}) ``` Invoke the graph with an \\*\\*invalid\\*\\* input ```python theme={null} try: graph.invoke({\"a\": 123}) # Should be a string except Exception as e: print(\"An exception was raised because `a` is an integer rather than a string.\") print(e) ``` ``` An exception was raised because `a` is an integer rather than a string. 1 validation error for OverallState a Input should be a valid string [type=string\\_type, input\\_value=123, input\\_type=int] For further information visit https://errors.pydantic.dev/2.9/v/string\\_type ``` See below for additional features of Pydantic model state:  When using Pydantic models as state schemas, it's important to understand how serialization works, especially when: \\* Passing Pydantic objects as inputs \\* Receiving outputs from the graph \\* Working with nested Pydantic models Let's see these behaviors in action. ```python theme={null} from langgraph.graph import StateGraph, START, END from pydantic import BaseModel class NestedModel(BaseModel): value: str class ComplexState(BaseModel): text: str count: int nested: NestedModel def process\\_node(state: ComplexState): # Node receives a validated Pydantic object print(f\"Input state type: {type(state)}\") print(f\"Nested type: {type(state.nested)}\") # Return a dictionary update return {\"text\": state.text + \" processed\", \"count\": state.count + 1} # Build the graph builder = StateGraph(ComplexState) builder.add\\_node(\"process\", process\\_node) builder.add\\_edge(START, \"process\") builder.add\\_edge(\"process\", END) graph = builder.compile() # Create a Pydantic instance for input input\\_state = ComplexState(text=\"hello\", count=0, nested=NestedModel(value=\"test\")) print(f\"Input object type: {type(input\\_state)}\") # Invoke graph with a Pydantic instance result = graph.invoke(input\\_state) print(f\"Output type: {type(result)}\") print(f\"Output content: {result}\") # Convert back to Pydantic model if needed output\\_model = ComplexState(\\*\\*result) print(f\"Converted back to Pydantic: {type(output\\_model)}\") ```   Pydantic performs runtime type coercion for certain data types. This can be helpful but also lead to unexpected behavior if you're not aware of it. ```python theme={null} from langgraph.graph import StateGraph, START, END from pydantic import BaseModel class CoercionExample(BaseModel): # Pydantic will coerce string numbers to integers number: int # Pydantic will parse string booleans to bool flag: bool def inspect\\_node(state: CoercionExample): print(f\"number: {state.number} (type: {type(state.number)})\") print(f\"flag: {state.flag} (type: {type(state.flag)})\") return {} builder = StateGraph(CoercionExample) builder.add\\_node(\"inspect\", inspect\\_node) builder.add\\_edge(START, \"inspect\") builder.add\\_edge(\"inspect\", END) graph = builder.compile() # Demonstrate coercion with string inputs that will be converted result = graph.invoke({\"number\": \"42\", \"flag\": \"true\"}) # This would fail with a validation error try: graph.invoke({\"number\": \"not-a-number\", \"flag\": \"true\"}) except Exception as e: print(f\"\\nExpected validation error: {e}\") ```   When working with LangChain message types in your state schema, there are important considerations for serialization. You should use `AnyMessage` (rather than `BaseMessage`) for proper serialization/deserialization when using message objects over the wire. ```python theme={null} from langgraph.graph import StateGraph, START, END from pydantic import BaseModel from langchain.messages import HumanMessage, AIMessage, AnyMessage from typing import List class ChatState(BaseModel): messages: List[AnyMessage] context: str def add\\_message(state: ChatState): return {\"messages\": state.messages + [AIMessage(content=\"Hello there!\")]} builder = StateGraph(ChatState) builder.add\\_node(\"add\\_message\", add\\_message) builder.add\\_edge(START, \"add\\_message\") builder.add\\_edge(\"add\\_message\", END) graph = builder.compile() # Create input with a message initial\\_state = ChatState( messages=[HumanMessage(content=\"Hi\")], context=\"Customer support chat\" ) result = graph.invoke(initial\\_state) print(f\"Output: {result}\") # Convert back to Pydantic model to see message types output\\_model = ChatState(\\*\\*result) for i, msg in enumerate(output\\_model.messages): print(f\"Message {i}: {type(msg).\\_\\_name\\_\\_} - {msg.content}\") ```  ## Add runtime configuration Sometimes you want to be able to configure your graph when calling it. For example, you might want to be able to specify what LLM or system prompt to use at runtime, \\*without polluting the graph state with these parameters\\*. To add runtime configuration: 1. Specify a schema for your configuration 2. Add the configuration to the function signature for nodes or conditional edges 3. Pass the configuration into the graph. See below for a simple example: ```python theme={null} from langgraph.graph import END, StateGraph, START from langgraph.runtime import Runtime from typing\\_extensions import TypedDict # 1. Specify config schema class ContextSchema(TypedDict): my\\_runtime\\_value: str # 2. Define a graph that accesses the config in a node class State(TypedDict): my\\_state\\_value: str def node(state: State, runtime: Runtime[ContextSchema]): # [!code highlight] if runtime.context[\"my\\_runtime\\_value\"] == \"a\": # [!code highlight] return {\"my\\_state\\_value\": 1} elif runtime.context[\"my\\_runtime\\_value\"] == \"b\": # [!code highlight] return {\"my\\_state\\_value\": 2} else: raise ValueError(\"Unknown values.\") builder = StateGraph(State, context\\_schema=ContextSchema) # [!code highlight] builder.add\\_node(node) builder.add\\_edge(START, \"node\") builder.add\\_edge(\"node\", END) graph = builder.compile() # 3. Pass in configuration at runtime: print(graph.invoke({}, context={\"my\\_runtime\\_value\": \"a\"})) # [!code highlight] print(graph.invoke({}, context={\"my\\_runtime\\_value\": \"b\"})) # [!code highlight] ``` ``` {'my\\_state\\_value': 1} {'my\\_state\\_value': 2} ```  Below we demonstrate a practical example in which we configure what LLM to use at runtime. We will use both OpenAI and Anthropic models. ```python theme={null} from dataclasses import dataclass from langchain.chat\\_models import init\\_chat\\_model from langgraph.graph import MessagesState, END, StateGraph, START from langgraph.runtime import Runtime from typing\\_extensions import TypedDict @dataclass class ContextSchema: model\\_provider: str = \"anthropic\" MODELS = { \"anthropic\": init\\_chat\\_model(\"claude-haiku-4-5-20251001\"), \"openai\": init\\_chat\\_model(\"gpt-4.1-mini\"), } def call\\_model(state: MessagesState, runtime: Runtime[ContextSchema]): model = MODELS[runtime.context.model\\_provider] response = model.invoke(state[\"messages\"]) return {\"messages\": [response]} builder = StateGraph(MessagesState, context\\_schema=ContextSchema) builder.add\\_node(\"model\", call\\_model) builder.add\\_edge(START, \"model\") builder.add\\_edge(\"model\", END) graph = builder.compile() # Usage input\\_message = {\"role\": \"user\", \"content\": \"hi\"} # With no configuration, uses default (Anthropic) response\\_1 = graph.invoke({\"messages\": [input\\_message]}, context=ContextSchema())[\"messages\"][-1] # Or, can set OpenAI response\\_2 = graph.invoke({\"messages\": [input\\_message]}, context={\"model\\_provider\": \"openai\"})[\"messages\"][-1] print(response\\_1.response\\_metadata[\"model\\_name\"]) print(response\\_2.response\\_metadata[\"model\\_name\"]) ``` ``` claude-haiku-4-5-20251001 gpt-4.1-mini-2025-04-14 ```   Below we demonstrate a practical example in which we configure two parameters: the LLM and system message to use at runtime. ```python theme={null} from dataclasses import dataclass from langchain.chat\\_models import init\\_chat\\_model from langchain.messages import SystemMessage from langgraph.graph import END, MessagesState, StateGraph, START from langgraph.runtime import Runtime from typing\\_extensions import TypedDict @dataclass class ContextSchema: model\\_provider: str = \"anthropic\" system\\_message: str | None = None MODELS = { \"anthropic\": init\\_chat\\_model(\"claude-haiku-4-5-20251001\"), \"openai\": init\\_chat\\_model(\"gpt-4.1-mini\"), } def call\\_model(state: MessagesState, runtime: Runtime[ContextSchema]): model = MODELS[runtime.context.model\\_provider] messages = state[\"messages\"] if (system\\_message := runtime.context.system\\_message): messages = [SystemMessage(system\\_message)] + messages response = model.invoke(messages) return {\"messages\": [response]} builder = StateGraph(MessagesState, context\\_schema=ContextSchema) builder.add\\_node(\"model\", call\\_model) builder.add\\_edge(START, \"model\") builder.add\\_edge(\"model\", END) graph = builder.compile() # Usage input\\_message = {\"role\": \"user\", \"content\": \"hi\"} response = graph.invoke({\"messages\": [input\\_message]}, context={\"model\\_provider\": \"openai\", \"system\\_message\": \"Respond in Italian.\"}) for message in response[\"messages\"]: message.pretty\\_print() ``` ``` ================================ Human Message ================================ hi ================================== Ai Message ================================== Ciao! Come posso aiutarti oggi? ```  ## Add retry policies There are many use cases where you may wish for your node to have a custom retry policy, for example if you are calling an API, querying a database, or calling an LLM, etc. LangGraph lets you add retry policies to nodes. To configure a retry policy, pass the `retry\\_policy` parameter to the [`add\\_node`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add\\_node). The `retry\\_policy` parameter takes in a `RetryPolicy` named tuple object. Below we instantiate a `RetryPolicy` object with the default parameters and associate it with a node: ```python theme={null} from langgraph.types import RetryPolicy builder.add\\_node( \"node\\_name\", node\\_function, retry\\_policy=RetryPolicy(), ) ``` By default, the `retry\\_on` parameter uses the `default\\_retry\\_on` function, which retries on any exception except for the following: \\* `ValueError` \\* `TypeError` \\* `ArithmeticError` \\* `ImportError` \\* `LookupError` \\* `NameError` \\* `SyntaxError` \\* `RuntimeError` \\* `ReferenceError` \\* `StopIteration` \\* `StopAsyncIteration` \\* `OSError` In addition, for exceptions from popular http request libraries such as `requests` and `httpx` it only retries on 5xx status codes.  Consider an example in which we are reading from a SQL database. Below we pass two different retry policies to nodes: ```python theme={null} import sqlite3 from typing\\_extensions import TypedDict from langchain.chat\\_models import init\\_chat\\_model from langgraph.graph import END, MessagesState, StateGraph, START from langgraph.types import RetryPolicy from langchain\\_community.utilities import SQLDatabase from langchain.messages import AIMessage db = SQLDatabase.from\\_uri(\"sqlite:///:memory:\") model = init\\_chat\\_model(\"claude-haiku-4-5-20251001\") def query\\_database(state: MessagesState): query\\_result = db.run(\"SELECT \\* FROM Artist LIMIT 10;\") return {\"messages\": [AIMessage(content=query\\_result)]} def call\\_model(state: MessagesState): response = model.invoke(state[\"messages\"]) return {\"messages\": [response]} # Define a new graph builder = StateGraph(MessagesState) builder.add\\_node( \"query\\_database\", query\\_database, retry\\_policy=RetryPolicy(retry\\_on=sqlite3.OperationalError), ) builder.add\\_node(\"model\", call\\_model, retry\\_policy=RetryPolicy(max\\_attempts=5)) builder.add\\_edge(START, \"model\") builder.add\\_edge(\"model\", \"query\\_database\") builder.add\\_edge(\"query\\_database\", END) graph = builder.compile() ```  ## Add node caching Node caching is useful in cases where you want to avoid repeating operations, like when doing something expensive (either in terms of time or cost). LangGraph lets you add individualized caching policies to nodes in a graph. To configure a cache policy, pass the `cache\\_policy` parameter to the [`add\\_node`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add\\_node) function. In the following example, a [`CachePolicy`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.CachePolicy) object is instantiated with a time to live of 120 seconds and the default `key\\_func` generator. Then it is associated with a node: ```python theme={null} from langgraph.types import CachePolicy builder.add\\_node( \"node\\_name\", node\\_function, cache\\_policy=CachePolicy(ttl=120), ) ``` Then, to enable node-level caching for a graph, set the `cache` argument when compiling the graph. The example below uses `InMemoryCache` to set up a graph with in-memory cache, but `SqliteCache` is also available. ```python theme={null} from langgraph.cache.memory import InMemoryCache graph = builder.compile(cache=InMemoryCache()) ``` ## Create a sequence of steps  \\*\\*Prerequisites\\*\\* This guide assumes familiarity with the above section on [state](#define-and-update-state).  Here we demonstrate how to construct a simple sequence of steps. We will show: 1. How to build a sequential graph 2. Built-in short-hand for constructing similar graphs. To add a sequence of nodes, we use the [`add\\_node`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add\\_node) and [`add\\_edge`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add\\_edge) methods of our [graph](/oss/python/langgraph/graph-api#stategraph): ```python theme={null} from langgraph.graph import START, StateGraph builder = StateGraph(State) # Add nodes builder.add\\_node(step\\_1) builder.add\\_node(step\\_2) builder.add\\_node(step\\_3) # Add edges builder.add\\_edge(START, \"step\\_1\") builder.add\\_edge(\"step\\_1\", \"step\\_2\") builder.add\\_edge(\"step\\_2\", \"step\\_3\") ``` We can also use the built-in shorthand `.add\\_sequence`: ```python theme={null} builder = StateGraph(State).add\\_sequence([step\\_1, step\\_2, step\\_3]) builder.add\\_edge(START, \"step\\_1\") ```  LangGraph makes it easy to add an underlying persistence layer to your application. This allows state to be checkpointed in between the execution of nodes, so your LangGraph nodes govern: \\* How state updates are [checkpointed](/oss/python/langgraph/persistence) \\* How interruptions are resumed in [human-in-the-loop](/oss/python/langgraph/interrupts) workflows \\* How we can \"rewind\" and branch-off executions using LangGraph's [time travel](/oss/python/langgraph/use-time-travel) features They also determine how execution steps are [streamed](/oss/python/langgraph/streaming), and how your application is visualized and debugged using [Studio](/langsmith/studio). Let's demonstrate an end-to-end example. We will create a sequence of three steps: 1. Populate a value in a key of the state 2. Update the same value 3. Populate a different value Let's first define our [state](/oss/python/langgraph/graph-api#state). This governs the [schema of the graph](/oss/python/langgraph/graph-api#schema), and can also specify how to apply updates. See [this section](#process-state-updates-with-reducers) for more detail. In our case, we will just keep track of two values: ```python theme={null} from typing\\_extensions import TypedDict class State(TypedDict): value\\_1: str value\\_2: int ``` Our [nodes](/oss/python/langgraph/graph-api#nodes) are just Python functions that read our graph's state and make updates to it. The first argument to this function will always be the state: ```python theme={null} def step\\_1(state: State): return {\"value\\_1\": \"a\"} def step\\_2(state: State): current\\_value\\_1 = state[\"value\\_1\"] return {\"value\\_1\": f\"{current\\_value\\_1} b\"} def step\\_3(state: State): return {\"value\\_2\": 10} ```  Note that when issuing updates to the state, each node can just specify the value of the key it wishes to update. By default, this will \\*\\*overwrite\\*\\* the value of the corresponding key. You can also use [reducers](/oss/python/langgraph/graph-api#reducers) to control how updates are processed\u2014 for example, you can append successive updates to a key instead. See [this section](#process-state-updates-with-reducers) for more detail.  Finally, we define the graph. We use [StateGraph](/oss/python/langgraph/graph-api#stategraph) to define a graph that operates on this state. We will then use [`add\\_node`](/oss/python/langgraph/graph-api#messagesstate) and [`add\\_edge`](/oss/python/langgraph/graph-api#edges) to populate our graph and define its control flow. ```python theme={null} from langgraph.graph import START, StateGraph builder = StateGraph(State) # Add nodes builder.add\\_node(step\\_1) builder.add\\_node(step\\_2) builder.add\\_node(step\\_3) # Add edges builder.add\\_edge(START, \"step\\_1\") builder.add\\_edge(\"step\\_1\", \"step\\_2\") builder.add\\_edge(\"step\\_2\", \"step\\_3\") ```  \\*\\*Specifying custom names\\*\\* You can specify custom names for nodes using [`add\\_node`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add\\_node): ```python theme={null} builder.add\\_node(\"my\\_node\", step\\_1) ```  Note that: \\* [`add\\_edge`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add\\_edge) takes the names of nodes, which for functions defaults to `node.\\_\\_name\\_\\_`. \\* We must specify the entry point of the graph. For this we add an edge with the [START node](/oss/python/langgraph/graph-api#start-node). \\* The graph halts when there are no more nodes to execute. We next [compile](/oss/python/langgraph/graph-api#compiling-your-graph) our graph. This provides a few basic checks on the structure of the graph (e.g., identifying orphaned nodes). If we were adding persistence to our application via a [checkpointer](/oss/python/langgraph/persistence), it would also be passed in here. ```python theme={null} graph = builder.compile() ``` LangGraph provides built-in utilities for visualizing your graph. Let's inspect our sequence. See [this guide](#visualize-your-graph) for detail on visualization. ```python theme={null} from IPython.display import Image, display display(Image(graph.get\\_graph().draw\\_mermaid\\_png())) ``` Let's proceed with a simple invocation: ```python theme={null} graph.invoke({\"value\\_1\": \"c\"}) ``` ``` {'value\\_1': 'a b', 'value\\_2': 10} ``` Note that: \\* We kicked off invocation by providing a value for a single state key. We must always provide a value for at least one key. \\* The value we passed in was overwritten by the first node. \\* The second node updated the value. \\* The third node populated a different value.  \\*\\*Built-in shorthand\\*\\* `langgraph>=0.2.46` includes a built-in short-hand `add\\_sequence` for adding node sequences. You can compile the same graph as follows: ```python theme={null} builder = StateGraph(State).add\\_sequence([step\\_1, step\\_2, step\\_3]) # [!code highlight] builder.add\\_edge(START, \"step\\_1\") graph = builder.compile() graph.invoke({\"value\\_1\": \"c\"}) ```   ## Create branches Parallel execution of nodes is essential to speed up overall graph operation. LangGraph offers native support for parallel execution of nodes, which can significantly enhance the performance of graph-based workflows. This parallelization is achieved through fan-out and fan-in mechanisms, utilizing both standard edges and [conditional\\\\_edges](https://langchain-ai.github.io/langgraph/reference/graphs.md#langgraph.graph.MessageGraph.add\\_conditional\\_edges). Below are some examples showing how to add create branching dataflows that work for you. ### Run graph nodes in parallel In this example, we fan out from `Node A` to `B and C` and then fan in to `D`. With our state, [we specify the reducer add operation](/oss/python/langgraph/graph-api#reducers). This will combine or accumulate values for the specific key in the State, rather than simply overwriting the existing value. For lists, this means concatenating the new list with the existing list. See the above section on [state reducers](#process-state-updates-with-reducers) for more detail on updating state with reducers. ```python theme={null} import operator from typing import Annotated, Any from typing\\_extensions import TypedDict from langgraph.graph import StateGraph, START, END class State(TypedDict): # The operator.add reducer fn makes this append-only aggregate: Annotated[list, operator.add] def a(state: State): print(f'Adding \"A\" to {state[\"aggregate\"]}') return {\"aggregate\": [\"A\"]} def b(state: State): print(f'Adding \"B\" to {state[\"aggregate\"]}') return {\"aggregate\": [\"B\"]} def c(state: State): print(f'Adding \"C\" to {state[\"aggregate\"]}') return {\"aggregate\": [\"C\"]} def d(state: State): print(f'Adding \"D\" to {state[\"aggregate\"]}') return {\"aggregate\": [\"D\"]} builder = StateGraph(State) builder.add\\_node(a) builder.add\\_node(b) builder.add\\_node(c) builder.add\\_node(d) builder.add\\_edge(START, \"a\") builder.add\\_edge(\"a\", \"b\") builder.add\\_edge(\"a\", \"c\") builder.add\\_edge(\"b\", \"d\") builder.add\\_edge(\"c\", \"d\") builder.add\\_edge(\"d\", END) graph = builder.compile() ``` ```python theme={null} from IPython.display import Image, display display(Image(graph.get\\_graph().draw\\_mermaid\\_png())) ``` With the reducer, you can see that the values added in each node are accumulated. ```python theme={null} graph.invoke({\"aggregate\": []}, {\"configurable\": {\"thread\\_id\": \"foo\"}}) ``` ``` Adding \"A\" to [] Adding \"B\" to ['A'] Adding \"C\" to ['A'] Adding \"D\" to ['A', 'B', 'C'] ```  In the above example, nodes `\"b\"` and `\"c\"` are executed concurrently in the same [superstep](/oss/python/langgraph/graph-api#graphs). Because they are in the same step, node `\"d\"` executes after both `\"b\"` and `\"c\"` are finished. Importantly, updates from a parallel superstep may not be ordered consistently. If you need a consistent, predetermined ordering of updates from a parallel superstep, you should write the outputs to a separate field in the state together with a value with which to order them.   LangGraph executes nodes within [supersteps](/oss/python/langgraph/graph-api#graphs), meaning that while parallel branches are executed in parallel, the entire superstep is \\*\\*transactional\\*\\*. If any of these branches raises an exception, \\*\\*none\\*\\* of the updates are applied to the state (the entire superstep errors). Importantly, when using a [checkpointer](/oss/python/langgraph/persistence), results from successful nodes within a superstep are saved, and don't repeat when resumed. If you have error-prone (perhaps want to handle flakey API calls), LangGraph provides two ways to address this: 1. You can write regular python code within your node to catch and handle exceptions. 2. You can set a \\*\\*[retry\\\\_policy](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.RetryPolicy)\\*\\* to direct the graph to retry nodes that raise certain types of exceptions. Only failing branches are retried, so you needn't worry about performing redundant work. Together, these let you perform parallel execution and fully control exception handling.   \\*\\*Set max concurrency\\*\\* You can control the maximum number of concurrent tasks by setting `max\\_concurrency` in the [configuration](https://python.langchain.com/api\\_reference/core/runnables/langchain\\_core.runnables.config.RunnableConfig.html) when invoking the graph. ```python theme={null} graph.invoke({\"value\\_1\": \"c\"}, {\"configurable\": {\"max\\_concurrency\": 10}}) ```  ### Defer node execution Deferring node execution is useful when you want to delay the execution of a node until all other pending tasks are completed. This is particularly relevant when branches have different lengths, which is common in workflows like map-reduce flows. The above example showed how to fan-out and fan-in when each path was only one step. But what if one branch had more than one step? Let's add a node `\"b\\_2\"` in the `\"b\"` branch: ```python theme={null} import operator from typing import Annotated, Any from typing\\_extensions import TypedDict from langgraph.graph import StateGraph, START, END class State(TypedDict): # The operator.add reducer fn makes this append-only aggregate: Annotated[list, operator.add] def a(state: State): print(f'Adding \"A\" to {state[\"aggregate\"]}') return {\"aggregate\": [\"A\"]} def b(state: State): print(f'Adding \"B\" to {state[\"aggregate\"]}') return {\"aggregate\": [\"B\"]} def b\\_2(state: State): print(f'Adding \"B\\_2\" to {state[\"aggregate\"]}') return {\"aggregate\": [\"B\\_2\"]} def c(state: State): print(f'Adding \"C\" to {state[\"aggregate\"]}') return {\"aggregate\": [\"C\"]} def d(state: State): print(f'Adding \"D\" to {state[\"aggregate\"]}') return {\"aggregate\": [\"D\"]} builder = StateGraph(State) builder.add\\_node(a) builder.add\\_node(b) builder.add\\_node(b\\_2) builder.add\\_node(c) builder.add\\_node(d, defer=True) # [!code highlight] builder.add\\_edge(START, \"a\") builder.add\\_edge(\"a\", \"b\") builder.add\\_edge(\"a\", \"c\") builder.add\\_edge(\"b\", \"b\\_2\") builder.add\\_edge(\"b\\_2\", \"d\") builder.add\\_edge(\"c\", \"d\") builder.add\\_edge(\"d\", END) graph = builder.compile() ``` ```python theme={null} from IPython.display import Image, display display(Image(graph.get\\_graph().draw\\_mermaid\\_png())) ``` ```python theme={null} graph.invoke({\"aggregate\": []}) ``` ``` Adding \"A\" to [] Adding \"B\" to ['A'] Adding \"C\" to ['A'] Adding \"B\\_2\" to ['A', 'B', 'C'] Adding \"D\" to ['A', 'B', 'C', 'B\\_2'] ``` In the above example, nodes `\"b\"` and `\"c\"` are executed concurrently in the same superstep. We set `defer=True` on node `d` so it will not execute until all pending tasks are finished. In this case, this means that `\"d\"` waits to execute until the entire `\"b\"` branch is finished. ### Conditional branching If your fan-out should vary at runtime based on the state, you can use [`add\\_conditional\\_edges`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add\\_conditional\\_edges) to select one or more paths using the graph state. See example below, where node `a` generates a state update that determines the following node. ```python theme={null} import operator from typing import Annotated, Literal, Sequence from typing\\_extensions import TypedDict from langgraph.graph import StateGraph, START, END class State(TypedDict): aggregate: Annotated[list, operator.add] # Add a key to the state. We will set this key to determine # how we branch. which: str def a(state: State): print(f'Adding \"A\" to {state[\"aggregate\"]}') return {\"aggregate\": [\"A\"], \"which\": \"c\"} # [!code highlight] def b(state: State): print(f'Adding \"B\" to {state[\"aggregate\"]}') return {\"aggregate\": [\"B\"]} def c(state: State): print(f'Adding \"C\" to {state[\"aggregate\"]}') return {\"aggregate\": [\"C\"]} builder = StateGraph(State) builder.add\\_node(a) builder.add\\_node(b) builder.add\\_node(c) builder.add\\_edge(START, \"a\") builder.add\\_edge(\"b\", END) builder.add\\_edge(\"c\", END) def conditional\\_edge(state: State) -> Literal[\"b\", \"c\"]: # Fill in arbitrary logic here that uses the state # to determine the next node return state[\"which\"] builder.add\\_conditional\\_edges(\"a\", conditional\\_edge) # [!code highlight] graph = builder.compile() ``` ```python theme={null} from IPython.display import Image, display display(Image(graph.get\\_graph().draw\\_mermaid\\_png())) ``` ```python theme={null} result = graph.invoke({\"aggregate\": []}) print(result) ``` ``` Adding \"A\" to [] Adding \"C\" to ['A'] {'aggregate': ['A', 'C'], 'which': 'c'} ```  Your conditional edges can route to multiple destination nodes. For example: ```python theme={null} def route\\_bc\\_or\\_cd(state: State) -> Sequence[str]: if state[\"which\"] == \"cd\": return [\"c\", \"d\"] return [\"b\", \"c\"] ```  ## Map-Reduce and the Send API LangGraph supports map-reduce and other advanced branching patterns using the Send API. Here is an example of how to use it: ```python theme={null} from langgraph.graph import StateGraph, START, END from langgraph.types import Send from typing\\_extensions import TypedDict, Annotated import operator class OverallState(TypedDict): topic: str subjects: list[str] jokes: Annotated[list[str], operator.add] best\\_selected\\_joke: str def generate\\_topics(state: OverallState): return {\"subjects\": [\"lions\", \"elephants\", \"penguins\"]} def generate\\_joke(state: OverallState): joke\\_map = { \"lions\": \"Why don't lions like fast food? Because they can't catch it!\", \"elephants\": \"Why don't elephants use computers? They're afraid of the mouse!\", \"penguins\": \"Why don't penguins like talking to strangers at parties? Because they find it hard to break the ice.\" } return {\"jokes\": [joke\\_map[state[\"subject\"]]]} def continue\\_to\\_jokes(state: OverallState): return [Send(\"generate\\_joke\", {\"subject\": s}) for s in state[\"subjects\"]] def best\\_joke(state: OverallState): return {\"best\\_selected\\_joke\": \"penguins\"} builder = StateGraph(OverallState) builder.add\\_node(\"generate\\_topics\", generate\\_topics) builder.add\\_node(\"generate\\_joke\", generate\\_joke) builder.add\\_node(\"best\\_joke\", best\\_joke) builder.add\\_edge(START, \"generate\\_topics\") builder.add\\_conditional\\_edges(\"generate\\_topics\", continue\\_to\\_jokes, [\"generate\\_joke\"]) builder.add\\_edge(\"generate\\_joke\", \"best\\_joke\") builder.add\\_edge(\"best\\_joke\", END) graph = builder.compile() ``` ```python theme={null} from IPython.display import Image, display display(Image(graph.get\\_graph().draw\\_mermaid\\_png())) ``` ```python theme={null} # Call the graph: here we call it to generate a list of jokes for step in graph.stream({\"topic\": \"animals\"}): print(step) ``` ``` {'generate\\_topics': {'subjects': ['lions', 'elephants', 'penguins']}} {'generate\\_joke': {'jokes': [\"Why don't lions like fast food? Because they can't catch it!\"]}} {'generate\\_joke': {'jokes': [\"Why don't elephants use computers? They're afraid of the mouse!\"]}} {'generate\\_joke': {'jokes': ['Why don't penguins like talking to strangers at parties? Because they find it hard to break the ice.']}} {'best\\_joke': {'best\\_selected\\_joke': 'penguins'}} ``` ## Create and control loops When creating a graph with a loop, we require a mechanism for terminating execution. This is most commonly done by adding a [conditional edge](/oss/python/langgraph/graph-api#conditional-edges) that routes to the [END](/oss/python/langgraph/graph-api#end-node) node once we reach some termination condition. You can also set the graph recursion limit when invoking or streaming the graph. The recursion limit sets the number of [supersteps](/oss/python/langgraph/graph-api#graphs) that the graph is allowed to execute before it raises an error. Read more about the concept of recursion limits [here](/oss/python/langgraph/graph-api#recursion-limit). Let's consider a simple graph with a loop to better understand how these mechanisms work.  To return the last value of your state instead of receiving a recursion limit error, see the [next section](#impose-a-recursion-limit).  When creating a loop, you can include a conditional edge that specifies a termination condition: ```python theme={null} builder = StateGraph(State) builder.add\\_node(a) builder.add\\_node(b) def route(state: State) -> Literal[\"b\", END]: if termination\\_condition(state): return END else: return \"b\" builder.add\\_edge(START, \"a\") builder.add\\_conditional\\_edges(\"a\", route) builder.add\\_edge(\"b\", \"a\") graph = builder.compile() ``` To control the recursion limit, specify `\"recursionLimit\"` in the config. This will raise a `GraphRecursionError`, which you can catch and handle: ```python theme={null} from langgraph.errors import GraphRecursionError try: graph.invoke(inputs, {\"recursion\\_limit\": 3}) except GraphRecursionError: print(\"Recursion Error\") ``` Let's define a graph with a simple loop. Note that we use a conditional edge to implement a termination condition. ```python theme={null} import operator from typing import Annotated, Literal from typing\\_extensions import TypedDict from langgraph.graph import StateGraph, START, END class State(TypedDict): # The operator.add reducer fn makes this append-only aggregate: Annotated[list, operator.add] def a(state: State): print(f'Node A sees {state[\"aggregate\"]}') return {\"aggregate\": [\"A\"]} def b(state: State): print(f'Node B sees {state[\"aggregate\"]}') return {\"aggregate\": [\"B\"]} # Define nodes builder = StateGraph(State) builder.add\\_node(a) builder.add\\_node(b) # Define edges def route(state: State) -> Literal[\"b\", END]: if len(state[\"aggregate\"]) < 7: return \"b\" else: return END builder.add\\_edge(START, \"a\") builder.add\\_conditional\\_edges(\"a\", route) builder.add\\_edge(\"b\", \"a\") graph = builder.compile() ``` ```python theme={null} from IPython.display import Image, display display(Image(graph.get\\_graph().draw\\_mermaid\\_png())) ``` This architecture is similar to a [ReAct agent](/oss/python/langgraph/workflows-agents) in which node `\"a\"` is a tool-calling model, and node `\"b\"` represents the tools. In our `route` conditional edge, we specify that we should end after the `\"aggregate\"` list in the state passes a threshold length. Invoking the graph, we see that we alternate between nodes `\"a\"` and `\"b\"` before terminating once we reach the termination condition. ```python theme={null} graph.invoke({\"aggregate\": []}) ``` ``` Node A sees [] Node B sees ['A'] Node A sees ['A', 'B'] Node B sees ['A', 'B', 'A'] Node A sees ['A', 'B', 'A', 'B'] Node B sees ['A', 'B', 'A', 'B', 'A'] Node A sees ['A', 'B', 'A', 'B', 'A', 'B'] ``` ### Impose a recursion limit In some applications, we may not have a guarantee that we will reach a given termination condition. In these cases, we can set the graph's [recursion limit](/oss/python/langgraph/graph-api#recursion-limit). This will raise a `GraphRecursionError` after a given number of [supersteps](/oss/python/langgraph/graph-api#graphs). We can then catch and handle this exception: ```python theme={null} from langgraph.errors import GraphRecursionError try: graph.invoke({\"aggregate\": []}, {\"recursion\\_limit\": 4}) except GraphRecursionError: print(\"Recursion Error\") ``` ``` Node A sees [] Node B sees ['A'] Node C sees ['A', 'B'] Node D sees ['A', 'B'] Node A sees ['A', 'B', 'C', 'D'] Recursion Error ```  Instead of raising `GraphRecursionError`, we can introduce a new key to the state that keeps track of the number of steps remaining until reaching the recursion limit. We can then use this key to determine if we should end the run. LangGraph implements a special `RemainingSteps` annotation. Under the hood, it creates a `ManagedValue` channel -- a state channel that will exist for the duration of our graph run and no longer. ```python theme={null} import operator from typing import Annotated, Literal from typing\\_extensions import TypedDict from langgraph.graph import StateGraph, START, END from langgraph.managed.is\\_last\\_step import RemainingSteps class State(TypedDict): aggregate: Annotated[list, operator.add] remaining\\_steps: RemainingSteps def a(state: State): print(f'Node A sees {state[\"aggregate\"]}') return {\"aggregate\": [\"A\"]} def b(state: State): print(f'Node B sees {state[\"aggregate\"]}') return {\"aggregate\": [\"B\"]} # Define nodes builder = StateGraph(State) builder.add\\_node(a) builder.add\\_node(b) # Define edges def route(state: State) -> Literal[\"b\", END]: if state[\"remaining\\_steps\"] <= 2: return END else: return \"b\" builder.add\\_edge(START, \"a\") builder.add\\_conditional\\_edges(\"a\", route) builder.add\\_edge(\"b\", \"a\") graph = builder.compile() # Test it out result = graph.invoke({\"aggregate\": []}, {\"recursion\\_limit\": 4}) print(result) ``` ``` Node A sees [] Node B sees ['A'] Node A sees ['A', 'B'] {'aggregate': ['A', 'B', 'A']} ```   To better understand how the recursion limit works, let's consider a more complex example. Below we implement a loop, but one step fans out into two nodes: ```python theme={null} import operator from typing import Annotated, Literal from typing\\_extensions import TypedDict from langgraph.graph import StateGraph, START, END class State(TypedDict): aggregate: Annotated[list, operator.add] def a(state: State): print(f'Node A sees {state[\"aggregate\"]}') return {\"aggregate\": [\"A\"]} def b(state: State): print(f'Node B sees {state[\"aggregate\"]}') return {\"aggregate\": [\"B\"]} def c(state: State): print(f'Node C sees {state[\"aggregate\"]}') return {\"aggregate\": [\"C\"]} def d(state: State): print(f'Node D sees {state[\"aggregate\"]}') return {\"aggregate\": [\"D\"]} # Define nodes builder = StateGraph(State) builder.add\\_node(a) builder.add\\_node(b) builder.add\\_node(c) builder.add\\_node(d) # Define edges def route(state: State) -> Literal[\"b\", END]: if len(state[\"aggregate\"]) < 7: return \"b\" else: return END builder.add\\_edge(START, \"a\") builder.add\\_conditional\\_edges(\"a\", route) builder.add\\_edge(\"b\", \"c\") builder.add\\_edge(\"b\", \"d\") builder.add\\_edge([\"c\", \"d\"], \"a\") graph = builder.compile() ``` ```python theme={null} from IPython.display import Image, display display(Image(graph.get\\_graph().draw\\_mermaid\\_png())) ``` This graph looks complex, but can be conceptualized as loop of [supersteps](/oss/python/langgraph/graph-api#graphs): 1. Node A 2. Node B 3. Nodes C and D 4. Node A 5. ... We have a loop of four supersteps, where nodes C and D are executed concurrently. Invoking the graph as before, we see that we complete two full \"laps\" before hitting the termination condition: ```python theme={null} result = graph.invoke({\"aggregate\": []}) ``` ``` Node A sees [] Node B sees ['A'] Node D sees ['A', 'B'] Node C sees ['A', 'B'] Node A sees ['A', 'B', 'C', 'D'] Node B sees ['A', 'B', 'C', 'D', 'A'] Node D sees ['A', 'B', 'C', 'D', 'A', 'B'] Node C sees ['A', 'B', 'C', 'D', 'A', 'B'] Node A sees ['A', 'B', 'C', 'D', 'A', 'B', 'C', 'D'] ``` However, if we set the recursion limit to four, we only complete one lap because each lap is four supersteps: ```python theme={null} from langgraph.errors import GraphRecursionError try: result = graph.invoke({\"aggregate\": []}, {\"recursion\\_limit\": 4}) except GraphRecursionError: print(\"Recursion Error\") ``` ``` Node A sees [] Node B sees ['A'] Node C sees ['A', 'B'] Node D sees ['A', 'B'] Node A sees ['A', 'B', 'C', 'D'] Recursion Error ```  ## Async Using the async programming paradigm can produce significant performance improvements when running [IO-bound](https://en.wikipedia.org/wiki/I/O\\_bound) code concurrently (e.g., making concurrent API requests to a chat model provider). To convert a `sync` implementation of the graph to an `async` implementation, you will need to: 1. Update `nodes` use `async def` instead of `def`. 2. Update the code inside to use `await` appropriately. 3. Invoke the graph with `.ainvoke` or `.astream` as desired. Because many LangChain objects implement the [Runnable Protocol](https://python.langchain.com/docs/expression\\_language/interface/) which has `async` variants of all the `sync` methods it's typically fairly quick to upgrade a `sync` graph to an `async` graph. See example below. To demonstrate async invocations of underlying LLMs, we will include a chat model:   \ud83d\udc49 Read the [OpenAI chat model integration docs](/oss/python/integrations/chat/openai/) ```shell theme={null} pip install -U \"langchain[openai]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"OPENAI\\_API\\_KEY\"] = \"sk-...\" model = init\\_chat\\_model(\"gpt-4.1\") ``` ```python Model Class theme={null} import os from langchain\\_openai import ChatOpenAI os.environ[\"OPENAI\\_API\\_KEY\"] = \"sk-...\" model = ChatOpenAI(model=\"gpt-4.1\") ```    \ud83d\udc49 Read the [Anthropic chat model integration docs](/oss/python/integrations/chat/anthropic/) ```shell theme={null} pip install -U \"langchain[anthropic]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"ANTHROPIC\\_API\\_KEY\"] = \"sk-...\" model = init\\_chat\\_model(\"claude-sonnet-4-5-20250929\") ``` ```python Model Class theme={null} import os from langchain\\_anthropic import ChatAnthropic os.environ[\"ANTHROPIC\\_API\\_KEY\"] = \"sk-...\" model = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\") ```    \ud83d\udc49 Read the [Azure chat model integration docs](/oss/python/integrations/chat/azure\\_chat\\_openai/) ```shell theme={null} pip install -U \"langchain[openai]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"AZURE\\_OPENAI\\_API\\_KEY\"] = \"...\" os.environ[\"AZURE\\_OPENAI\\_ENDPOINT\"] = \"...\" os.environ[\"OPENAI\\_API\\_VERSION\"] = \"2025-03-01-preview\" model = init\\_chat\\_model( \"azure\\_openai:gpt-4.1\", azure\\_deployment=os.environ[\"AZURE\\_OPENAI\\_DEPLOYMENT\\_NAME\"], ) ``` ```python Model Class theme={null} import os from langchain\\_openai import AzureChatOpenAI os.environ[\"AZURE\\_OPENAI\\_API\\_KEY\"] = \"...\" os.environ[\"AZURE\\_OPENAI\\_ENDPOINT\"] = \"...\" os.environ[\"OPENAI\\_API\\_VERSION\"] = \"2025-03-01-preview\" model = AzureChatOpenAI( model=\"gpt-4.1\", azure\\_deployment=os.environ[\"AZURE\\_OPENAI\\_DEPLOYMENT\\_NAME\"] ) ```    \ud83d\udc49 Read the [Google GenAI chat model integration docs](/oss/python/integrations/chat/google\\_generative\\_ai/) ```shell theme={null} pip install -U \"langchain[google-genai]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"GOOGLE\\_API\\_KEY\"] = \"...\" model = init\\_chat\\_model(\"google\\_genai:gemini-2.5-flash-lite\") ``` ```python Model Class theme={null} import os from langchain\\_google\\_genai import ChatGoogleGenerativeAI os.environ[\"GOOGLE\\_API\\_KEY\"] = \"...\" model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash-lite\") ```    \ud83d\udc49 Read the [AWS Bedrock chat model integration docs](/oss/python/integrations/chat/bedrock/) ```shell theme={null} pip install -U \"langchain[aws]\" ```  ```python init\\_chat\\_model theme={null} from langchain.chat\\_models import init\\_chat\\_model # Follow the steps here to configure your credentials: # https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html model = init\\_chat\\_model( \"anthropic.claude-3-5-sonnet-20240620-v1:0\", model\\_provider=\"bedrock\\_converse\", ) ``` ```python Model Class theme={null} from langchain\\_aws import ChatBedrock model = ChatBedrock(model=\"anthropic.claude-3-5-sonnet-20240620-v1:0\") ```   \ud83d\udc49 Read the [HuggingFace chat model integration docs](/oss/python/integrations/chat/huggingface/) ```shell theme={null} pip install -U \"langchain[huggingface]\" ```  ```python init\\_chat\\_model theme={null} import os from langchain.chat\\_models import init\\_chat\\_model os.environ[\"HUGGINGFACEHUB\\_API\\_TOKEN\"] = \"hf\\_...\" model = init\\_chat\\_model( \"microsoft/Phi-3-mini-4k-instruct\", model\\_provider=\"huggingface\", temperature=0.7, max\\_tokens=1024, ) ``` ```python Model Class theme={null} import os from langchain\\_huggingface import ChatHuggingFace, HuggingFaceEndpoint os.environ[\"HUGGINGFACEHUB\\_API\\_TOKEN\"] = \"hf\\_...\" llm = HuggingFaceEndpoint( repo\\_id=\"microsoft/Phi-3-mini-4k-instruct\", temperature=0.7, max\\_length=1024, ) model = ChatHuggingFace(llm=llm) ```     ```python theme={null} from langchain.chat\\_models import init\\_chat\\_model from langgraph.graph import MessagesState, StateGraph async def node(state: MessagesState): # [!code highlight] new\\_message = await llm.ainvoke(state[\"messages\"]) # [!code highlight] return {\"messages\": [new\\_message]} builder = StateGraph(MessagesState).add\\_node(node).set\\_entry\\_point(\"node\") graph = builder.compile() input\\_message = {\"role\": \"user\", \"content\": \"Hello\"} result = await graph.ainvoke({\"messages\": [input\\_message]}) # [!code highlight] ```  \\*\\*Async streaming\\*\\* See the [streaming guide](/oss/python/langgraph/streaming) for examples of streaming with async.  ## Combine control flow and state updates with `Command` It can be useful to combine control flow (edges) and state updates (nodes). For example, you might want to BOTH perform state updates AND decide which node to go to next in the SAME node. LangGraph provides a way to do so by returning a [Command](https://langchain-ai.github.io/langgraph/reference/types/#langgraph.types.Command) object from node functions: ```python theme={null} def my\\_node(state: State) -> Command[Literal[\"my\\_other\\_node\"]]: return Command( # state update update={\"foo\": \"bar\"}, # control flow goto=\"my\\_other\\_node\" ) ``` We show an end-to-end example below. Let's create a simple graph with 3 nodes: A, B and C. We will first execute node A, and then decide whether to go to Node B or Node C next based on the output of node A. ```python theme={null} import random from typing\\_extensions import TypedDict, Literal from langgraph.graph import StateGraph, START from langgraph.types import Command # Define graph state class State(TypedDict): foo: str # Define the nodes def node\\_a(state: State) -> Command[Literal[\"node\\_b\", \"node\\_c\"]]: print(\"Called A\") value = random.choice([\"b\", \"c\"]) # this is a replacement for a conditional edge function if value == \"b\": goto = \"node\\_b\" else: goto = \"node\\_c\" # note how Command allows you to BOTH update the graph state AND route to the next node return Command( # this is the state update update={\"foo\": value}, # this is a replacement for an edge goto=goto, ) def node\\_b(state: State): print(\"Called B\") return {\"foo\": state[\"foo\"] + \"b\"} def node\\_c(state: State): print(\"Called C\") return {\"foo\": state[\"foo\"] + \"c\"} ``` We can now create the [`StateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) with the above nodes. Notice that the graph doesn't have [conditional edges](/oss/python/langgraph/graph-api#conditional-edges) for routing! This is because control flow is defined with [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) inside `node\\_a`. ```python theme={null} builder = StateGraph(State) builder.add\\_edge(START, \"node\\_a\") builder.add\\_node(node\\_a) builder.add\\_node(node\\_b) builder.add\\_node(node\\_c) # NOTE: there are no edges between nodes A, B and C! graph = builder.compile() ```  You might have noticed that we used [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) as a return type annotation, e.g. `Command[Literal[\"node\\_b\", \"node\\_c\"]]`. This is necessary for the graph rendering and tells LangGraph that `node\\_a` can navigate to `node\\_b` and `node\\_c`.  ```python theme={null} from IPython.display import display, Image display(Image(graph.get\\_graph().draw\\_mermaid\\_png())) ``` If we run the graph multiple times, we'd see it take different paths (A -> B or A -> C) based on the random choice in node A. ```python theme={null} graph.invoke({\"foo\": \"\"}) ``` ``` Called A Called C ``` ### Navigate to a node in a parent graph If you are using [subgraphs](/oss/python/langgraph/use-subgraphs), you might want to navigate from a node within a subgraph to a different subgraph (i.e. a different node in the parent graph). To do so, you can specify `graph=Command.PARENT` in `Command`: ```python theme={null} def my\\_node(state: State) -> Command[Literal[\"my\\_other\\_node\"]]: return Command( update={\"foo\": \"bar\"}, goto=\"other\\_subgraph\", # where `other\\_subgraph` is a node in the parent graph graph=Command.PARENT ) ``` Let's demonstrate this using the above example. We'll do so by changing `nodeA` in the above example into a single-node graph that we'll add as a subgraph to our parent graph.  \\*\\*State updates with `Command.PARENT`\\*\\* When you send updates from a subgraph node to a parent graph node for a key that's shared by both parent and subgraph [state schemas](/oss/python/langgraph/graph-api#schema), you \\*\\*must\\*\\* define a [reducer](/oss/python/langgraph/graph-api#reducers) for the key you're updating in the parent graph state. See the example below.  ```python theme={null} import operator from typing\\_extensions import Annotated class State(TypedDict): # NOTE: we define a reducer here foo: Annotated[str, operator.add] # [!code highlight] def node\\_a(state: State): print(\"Called A\") value = random.choice([\"a\", \"b\"]) # this is a replacement for a conditional edge function if value == \"a\": goto = \"node\\_b\" else: goto = \"node\\_c\" # note how Command allows you to BOTH update the graph state AND route to the next node return Command( update={\"foo\": value}, goto=goto, # this tells LangGraph to navigate to node\\_b or node\\_c in the parent graph # NOTE: this will navigate to the closest parent graph relative to the subgraph graph=Command.PARENT, # [!code highlight] ) subgraph = StateGraph(State).add\\_node(node\\_a).add\\_edge(START, \"node\\_a\").compile() def node\\_b(state: State): print(\"Called B\") # NOTE: since we've defined a reducer, we don't need to manually append # new characters to existing 'foo' value. instead, reducer will append these # automatically (via operator.add) return {\"foo\": \"b\"} # [!code highlight] def node\\_c(state: State): print(\"Called C\") return {\"foo\": \"c\"} # [!code highlight] builder = StateGraph(State) builder.add\\_edge(START, \"subgraph\") builder.add\\_node(\"subgraph\", subgraph) builder.add\\_node(node\\_b) builder.add\\_node(node\\_c) graph = builder.compile() ``` ```python theme={null} graph.invoke({\"foo\": \"\"}) ``` ``` Called A Called C ``` ### Use inside tools A common use case is updating graph state from inside a tool. For example, in a customer support application you might want to look up customer information based on their account number or ID in the beginning of the conversation. To update the graph state from the tool, you can return `Command(update={\"my\\_custom\\_key\": \"foo\", \"messages\": [...]})` from the tool: ```python theme={null} @tool def lookup\\_user\\_info(tool\\_call\\_id: Annotated[str, InjectedToolCallId], config: RunnableConfig): \"\"\"Use this to look up user information to better assist them with their questions.\"\"\" user\\_info = get\\_user\\_info(config.get(\"configurable\", {}).get(\"user\\_id\")) return Command( update={ # update the state keys \"user\\_info\": user\\_info, # update the message history \"messages\": [ToolMessage(\"Successfully looked up user information\", tool\\_call\\_id=tool\\_call\\_id)] } ) ```  You MUST include `messages` (or any state key used for the message history) in `Command.update` when returning [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) from a tool and the list of messages in `messages` MUST contain a `ToolMessage`. This is necessary for the resulting message history to be valid (LLM providers require AI messages with tool calls to be followed by the tool result messages).  If you are using tools that update state via [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command), we recommend using prebuilt [`ToolNode`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.tool\\_node.ToolNode) which automatically handles tools returning [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) objects and propagates them to the graph state. If you're writing a custom node that calls tools, you would need to manually propagate [`Command`](https://reference.langchain.com/python/langgraph/types/#langgraph.types.Command) objects returned by the tools as the update from the node. ## Visualize your graph Here we demonstrate how to visualize the graphs you create. You can visualize any arbitrary [Graph](https://langchain-ai.github.io/langgraph/reference/graphs/), including [StateGraph](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.state.StateGraph). Let's have some fun by drawing fractals :). ```python theme={null} import random from typing import Annotated, Literal from typing\\_extensions import TypedDict from langgraph.graph import StateGraph, START, END from langgraph.graph.message import add\\_messages class State(TypedDict): messages: Annotated[list, add\\_messages] class MyNode: def \\_\\_init\\_\\_(self, name: str): self.name = name def \\_\\_call\\_\\_(self, state: State): return {\"messages\": [(\"assistant\", f\"Called node {self.name}\")]} def route(state) -> Literal[\"entry\\_node\", END]: if len(state[\"messages\"]) > 10: return END return \"entry\\_node\" def add\\_fractal\\_nodes(builder, current\\_node, level, max\\_level): if level > max\\_level: return # Number of nodes to create at this level num\\_nodes = random.randint(1, 3) # Adjust randomness as needed for i in range(num\\_nodes): nm = [\"A\", \"B\", \"C\"][i] node\\_name = f\"node\\_{current\\_node}\\_{nm}\" builder.add\\_node(node\\_name, MyNode(node\\_name)) builder.add\\_edge(current\\_node, node\\_name) # Recursively add more nodes r = random.random() if r > 0.2 and level + 1 < max\\_level: add\\_fractal\\_nodes(builder, node\\_name, level + 1, max\\_level) elif r > 0.05: builder.add\\_conditional\\_edges(node\\_name, route, node\\_name) else: # End builder.add\\_edge(node\\_name, END) def build\\_fractal\\_graph(max\\_level: int): builder = StateGraph(State) entry\\_point = \"entry\\_node\" builder.add\\_node(entry\\_point, MyNode(entry\\_point)) builder.add\\_edge(START, entry\\_point) add\\_fractal\\_nodes(builder, entry\\_point, 1, max\\_level) # Optional: set a finish point if required builder.add\\_edge(entry\\_point, END) # or any specific node return builder.compile() app = build\\_fractal\\_graph(3) ``` ### Mermaid We can also convert a graph class into Mermaid syntax. ```python theme={null} print(app.get\\_graph().draw\\_mermaid()) ``` ``` %%{init: {'flowchart': {'curve': 'linear'}}}%% graph TD; tart\\_\\_([\n]):::first ry\\_node(entry\\_node) e\\_entry\\_node\\_A(node\\_entry\\_node\\_A) e\\_entry\\_node\\_B(node\\_entry\\_node\\_B) e\\_node\\_entry\\_node\\_B\\_A(node\\_node\\_entry\\_node\\_B\\_A) e\\_node\\_entry\\_node\\_B\\_B(node\\_node\\_entry\\_node\\_B\\_B) e\\_node\\_entry\\_node\\_B\\_C(node\\_node\\_entry\\_node\\_B\\_C) nd\\_\\_([\n]):::last tart\\_\\_ --> entry\\_node; ry\\_node --> \\_\\_end\\_\\_; ry\\_node --> node\\_entry\\_node\\_A; ry\\_node --> node\\_entry\\_node\\_B; e\\_entry\\_node\\_B --> node\\_node\\_entry\\_node\\_B\\_A; e\\_entry\\_node\\_B --> node\\_node\\_entry\\_node\\_B\\_B; e\\_entry\\_node\\_B --> node\\_node\\_entry\\_node\\_B\\_C; e\\_entry\\_node\\_A -.-> entry\\_node; e\\_entry\\_node\\_A -.-> \\_\\_end\\_\\_; e\\_node\\_entry\\_node\\_B\\_A -.-> entry\\_node; e\\_node\\_entry\\_node\\_B\\_A -.-> \\_\\_end\\_\\_; e\\_node\\_entry\\_node\\_B\\_B -.-> entry\\_node; e\\_node\\_entry\\_node\\_B\\_B -.-> \\_\\_end\\_\\_; e\\_node\\_entry\\_node\\_B\\_C -.-> entry\\_node; e\\_node\\_entry\\_node\\_B\\_C -.-> \\_\\_end\\_\\_; ssDef default fill:#f2f0ff,line-height:1.2 ssDef first fill-opacity:0 ssDef last fill:#bfb6fc ``` ### PNG If preferred, we could render the Graph into a `.png`. Here we could use three options: \\* Using Mermaid.ink API (does not require additional packages) \\* Using Mermaid + Pyppeteer (requires `pip install pyppeteer`) \\* Using graphviz (which requires `pip install graphviz`) \\*\\*Using Mermaid.Ink\\*\\* By default, `draw\\_mermaid\\_png()` uses Mermaid.Ink's API to generate the diagram. ```python theme={null} from IPython.display import Image, display from langchain\\_core.runnables.graph import CurveStyle, MermaidDrawMethod, NodeStyles display(Image(app.get\\_graph().draw\\_mermaid\\_png())) ``` \\*\\*Using Mermaid + Pyppeteer\\*\\* ```python theme={null} import nest\\_asyncio nest\\_asyncio.apply() # Required for Jupyter Notebook to run async functions display( Image( app.get\\_graph().draw\\_mermaid\\_png( curve\\_style=CurveStyle.LINEAR, node\\_colors=NodeStyles(first=\"#ffdfba\", last=\"#baffc9\", default=\"#fad7de\"), wrap\\_label\\_n\\_words=9, output\\_file\\_path=None, draw\\_method=MermaidDrawMethod.PYPPETEER, background\\_color=\"white\", padding=10, ) ) ) ``` \\*\\*Using Graphviz\\*\\* ```python theme={null} try: display(Image(app.get\\_graph().draw\\_png())) except ImportError: print( \"You likely need to install dependencies for pygraphviz, see more here https://github.com/pygraphviz/pygraphviz/blob/main/INSTALL.txt\" ) ``` \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/use-graph-api.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/use-subgraphs.md",
      "title": null,
      "raw_content": "# Subgraphs This guide explains the mechanics of using subgraphs. A subgraph is a [graph](/oss/python/langgraph/graph-api#graphs) that is used as a [node](/oss/python/langgraph/graph-api#nodes) in another graph. Subgraphs are useful for: \\* Building [multi-agent systems](/oss/python/langchain/multi-agent) \\* Re-using a set of nodes in multiple graphs \\* Distributing development: when you want different teams to work on different parts of the graph independently, you can define each part as a subgraph, and as long as the subgraph interface (the input and output schemas) is respected, the parent graph can be built without knowing any details of the subgraph When adding subgraphs, you need to define how the parent graph and the subgraph communicate: \\* [Invoke a graph from a node](#invoke-a-graph-from-a-node) \u2014 subgraphs are called from inside a node in the parent graph \\* [Add a graph as a node](#add-a-graph-as-a-node) \u2014 a subgraph is added directly as a node in the parent and \\*\\*shares [state keys](/oss/python/langgraph/graph-api#state)\\*\\* with the parent ## Setup  ```bash pip theme={null} pip install -U langgraph ``` ```bash uv theme={null} uv add langgraph ```   \\*\\*Set up LangSmith for LangGraph development\\*\\* Sign up for [LangSmith](https://smith.langchain.com) to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph \u2014 read more about how to get started [here](https://docs.smith.langchain.com).  ## Invoke a graph from a node A simple way to implement a subgraph is to invoke a graph from inside the node of another graph. In this case subgraphs can have \\*\\*completely different schemas\\*\\* from the parent graph (no shared keys). For example, you might want to keep a private message history for each of the agents in a [multi-agent](/oss/python/langchain/multi-agent) system. If that's the case for your application, you need to define a node \\*\\*function that invokes the subgraph\\*\\*. This function needs to transform the input (parent) state to the subgraph state before invoking the subgraph, and transform the results back to the parent state before returning the state update from the node. ```python theme={null} from typing\\_extensions import TypedDict from langgraph.graph.state import StateGraph, START class SubgraphState(TypedDict): bar: str # Subgraph def subgraph\\_node\\_1(state: SubgraphState): return {\"bar\": \"hi! \" + state[\"bar\"]} subgraph\\_builder = StateGraph(SubgraphState) subgraph\\_builder.add\\_node(subgraph\\_node\\_1) subgraph\\_builder.add\\_edge(START, \"subgraph\\_node\\_1\") subgraph = subgraph\\_builder.compile() # Parent graph class State(TypedDict): foo: str def call\\_subgraph(state: State): # Transform the state to the subgraph state subgraph\\_output = subgraph.invoke({\"bar\": state[\"foo\"]}) # [!code highlight] # Transform response back to the parent state return {\"foo\": subgraph\\_output[\"bar\"]} builder = StateGraph(State) builder.add\\_node(\"node\\_1\", call\\_subgraph) builder.add\\_edge(START, \"node\\_1\") graph = builder.compile() ```  ```python theme={null} from typing\\_extensions import TypedDict from langgraph.graph.state import StateGraph, START # Define subgraph class SubgraphState(TypedDict): # note that none of these keys are shared with the parent graph state bar: str baz: str def subgraph\\_node\\_1(state: SubgraphState): return {\"baz\": \"baz\"} def subgraph\\_node\\_2(state: SubgraphState): return {\"bar\": state[\"bar\"] + state[\"baz\"]} subgraph\\_builder = StateGraph(SubgraphState) subgraph\\_builder.add\\_node(subgraph\\_node\\_1) subgraph\\_builder.add\\_node(subgraph\\_node\\_2) subgraph\\_builder.add\\_edge(START, \"subgraph\\_node\\_1\") subgraph\\_builder.add\\_edge(\"subgraph\\_node\\_1\", \"subgraph\\_node\\_2\") subgraph = subgraph\\_builder.compile() # Define parent graph class ParentState(TypedDict): foo: str def node\\_1(state: ParentState): return {\"foo\": \"hi! \" + state[\"foo\"]} def node\\_2(state: ParentState): # Transform the state to the subgraph state response = subgraph.invoke({\"bar\": state[\"foo\"]}) # Transform response back to the parent state return {\"foo\": response[\"bar\"]} builder = StateGraph(ParentState) builder.add\\_node(\"node\\_1\", node\\_1) builder.add\\_node(\"node\\_2\", node\\_2) builder.add\\_edge(START, \"node\\_1\") builder.add\\_edge(\"node\\_1\", \"node\\_2\") graph = builder.compile() for chunk in graph.stream({\"foo\": \"foo\"}, subgraphs=True): print(chunk) ``` ``` ((), {'node\\_1': {'foo': 'hi! foo'}}) (('node\\_2:9c36dd0f-151a-cb42-cbad-fa2f851f9ab7',), {'grandchild\\_1': {'my\\_grandchild\\_key': 'hi Bob, how are you'}}) (('node\\_2:9c36dd0f-151a-cb42-cbad-fa2f851f9ab7',), {'grandchild\\_2': {'bar': 'hi! foobaz'}}) ((), {'node\\_2': {'foo': 'hi! foobaz'}}) ```   This is an example with two levels of subgraphs: parent -> child -> grandchild. ```python theme={null} # Grandchild graph from typing\\_extensions import TypedDict from langgraph.graph.state import StateGraph, START, END class GrandChildState(TypedDict): my\\_grandchild\\_key: str def grandchild\\_1(state: GrandChildState) -> GrandChildState: # NOTE: child or parent keys will not be accessible here return {\"my\\_grandchild\\_key\": state[\"my\\_grandchild\\_key\"] + \", how are you\"} grandchild = StateGraph(GrandChildState) grandchild.add\\_node(\"grandchild\\_1\", grandchild\\_1) grandchild.add\\_edge(START, \"grandchild\\_1\") grandchild.add\\_edge(\"grandchild\\_1\", END) grandchild\\_graph = grandchild.compile() # Child graph class ChildState(TypedDict): my\\_child\\_key: str def call\\_grandchild\\_graph(state: ChildState) -> ChildState: # NOTE: parent or grandchild keys won't be accessible here grandchild\\_graph\\_input = {\"my\\_grandchild\\_key\": state[\"my\\_child\\_key\"]} grandchild\\_graph\\_output = grandchild\\_graph.invoke(grandchild\\_graph\\_input) return {\"my\\_child\\_key\": grandchild\\_graph\\_output[\"my\\_grandchild\\_key\"] + \" today?\"} child = StateGraph(ChildState) # We're passing a function here instead of just compiled graph (`grandchild\\_graph`) child.add\\_node(\"child\\_1\", call\\_grandchild\\_graph) child.add\\_edge(START, \"child\\_1\") child.add\\_edge(\"child\\_1\", END) child\\_graph = child.compile() # Parent graph class ParentState(TypedDict): my\\_key: str def parent\\_1(state: ParentState) -> ParentState: # NOTE: child or grandchild keys won't be accessible here return {\"my\\_key\": \"hi \" + state[\"my\\_key\"]} def parent\\_2(state: ParentState) -> ParentState: return {\"my\\_key\": state[\"my\\_key\"] + \" bye!\"} def call\\_child\\_graph(state: ParentState) -> ParentState: child\\_graph\\_input = {\"my\\_child\\_key\": state[\"my\\_key\"]} child\\_graph\\_output = child\\_graph.invoke(child\\_graph\\_input) return {\"my\\_key\": child\\_graph\\_output[\"my\\_child\\_key\"]} parent = StateGraph(ParentState) parent.add\\_node(\"parent\\_1\", parent\\_1) # We're passing a function here instead of just a compiled graph (`child\\_graph`) parent.add\\_node(\"child\", call\\_child\\_graph) parent.add\\_node(\"parent\\_2\", parent\\_2) parent.add\\_edge(START, \"parent\\_1\") parent.add\\_edge(\"parent\\_1\", \"child\") parent.add\\_edge(\"child\", \"parent\\_2\") parent.add\\_edge(\"parent\\_2\", END) parent\\_graph = parent.compile() for chunk in parent\\_graph.stream({\"my\\_key\": \"Bob\"}, subgraphs=True): print(chunk) ``` ``` ((), {'parent\\_1': {'my\\_key': 'hi Bob'}}) (('child:2e26e9ce-602f-862c-aa66-1ea5a4655e3b', 'child\\_1:781bb3b1-3971-84ce-810b-acf819a03f9c'), {'grandchild\\_1': {'my\\_grandchild\\_key': 'hi Bob, how are you'}}) (('child:2e26e9ce-602f-862c-aa66-1ea5a4655e3b',), {'child\\_1': {'my\\_child\\_key': 'hi Bob, how are you today?'}}) ((), {'child': {'my\\_key': 'hi Bob, how are you today?'}}) ((), {'parent\\_2': {'my\\_key': 'hi Bob, how are you today? bye!'}}) ```  ## Add a graph as a node When the parent graph and subgraph can communicate over a shared state key (channel) in the [schema](/oss/python/langgraph/graph-api#state), you can add a graph as a [node](/oss/python/langgraph/graph-api#nodes) in another graph. For example, in [multi-agent](/oss/python/langchain/multi-agent) systems, the agents often communicate over a shared [messages](/oss/python/langgraph/graph-api#why-use-messages) key. If your subgraph shares state keys with the parent graph, you can follow these steps to add it to your graph: 1. Define the subgraph workflow (`subgraph\\_builder` in the example below) and compile it 2. Pass compiled subgraph to the [`add\\_node`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph.add\\_node) method when defining the parent graph workflow ```python theme={null} from typing\\_extensions import TypedDict from langgraph.graph.state import StateGraph, START class State(TypedDict): foo: str # Subgraph def subgraph\\_node\\_1(state: State): return {\"foo\": \"hi! \" + state[\"foo\"]} subgraph\\_builder = StateGraph(State) subgraph\\_builder.add\\_node(subgraph\\_node\\_1) subgraph\\_builder.add\\_edge(START, \"subgraph\\_node\\_1\") subgraph = subgraph\\_builder.compile() # Parent graph builder = StateGraph(State) builder.add\\_node(\"node\\_1\", subgraph) # [!code highlight] builder.add\\_edge(START, \"node\\_1\") graph = builder.compile() ```  ```python theme={null} from typing\\_extensions import TypedDict from langgraph.graph.state import StateGraph, START # Define subgraph class SubgraphState(TypedDict): foo: str # shared with parent graph state bar: str # private to SubgraphState def subgraph\\_node\\_1(state: SubgraphState): return {\"bar\": \"bar\"} def subgraph\\_node\\_2(state: SubgraphState): # note that this node is using a state key ('bar') that is only available in the subgraph # and is sending update on the shared state key ('foo') return {\"foo\": state[\"foo\"] + state[\"bar\"]} subgraph\\_builder = StateGraph(SubgraphState) subgraph\\_builder.add\\_node(subgraph\\_node\\_1) subgraph\\_builder.add\\_node(subgraph\\_node\\_2) subgraph\\_builder.add\\_edge(START, \"subgraph\\_node\\_1\") subgraph\\_builder.add\\_edge(\"subgraph\\_node\\_1\", \"subgraph\\_node\\_2\") subgraph = subgraph\\_builder.compile() # Define parent graph class ParentState(TypedDict): foo: str def node\\_1(state: ParentState): return {\"foo\": \"hi! \" + state[\"foo\"]} builder = StateGraph(ParentState) builder.add\\_node(\"node\\_1\", node\\_1) builder.add\\_node(\"node\\_2\", subgraph) builder.add\\_edge(START, \"node\\_1\") builder.add\\_edge(\"node\\_1\", \"node\\_2\") graph = builder.compile() for chunk in graph.stream({\"foo\": \"foo\"}): print(chunk) ``` ``` {'node\\_1': {'foo': 'hi! foo'}} {'node\\_2': {'foo': 'hi! foobar'}} ```  ## Add persistence You only need to \\*\\*provide the checkpointer when compiling the parent graph\\*\\*. LangGraph will automatically propagate the checkpointer to the child subgraphs. ```python theme={null} from langgraph.graph import START, StateGraph from langgraph.checkpoint.memory import MemorySaver from typing\\_extensions import TypedDict class State(TypedDict): foo: str # Subgraph def subgraph\\_node\\_1(state: State): return {\"foo\": state[\"foo\"] + \"bar\"} subgraph\\_builder = StateGraph(State) subgraph\\_builder.add\\_node(subgraph\\_node\\_1) subgraph\\_builder.add\\_edge(START, \"subgraph\\_node\\_1\") subgraph = subgraph\\_builder.compile() # Parent graph builder = StateGraph(State) builder.add\\_node(\"node\\_1\", subgraph) builder.add\\_edge(START, \"node\\_1\") checkpointer = MemorySaver() graph = builder.compile(checkpointer=checkpointer) ``` If you want the subgraph to \\*\\*have its own memory\\*\\*, you can compile it with the appropriate checkpointer option. This is useful in [multi-agent](/oss/python/langchain/multi-agent) systems, if you want agents to keep track of their internal message histories: ```python theme={null} subgraph\\_builder = StateGraph(...) subgraph = subgraph\\_builder.compile(checkpointer=True) ``` ## View subgraph state When you enable [persistence](/oss/python/langgraph/persistence), you can [inspect the graph state](/oss/python/langgraph/persistence#checkpoints) (checkpoint) via the appropriate method. To view the subgraph state, you can use the subgraphs option. You can inspect the graph state via `graph.get\\_state(config)`. To view the subgraph state, you can use `graph.get\\_state(config, subgraphs=True)`.  \\*\\*Available \\*\\*only\\*\\* when interrupted\\*\\* Subgraph state can only be viewed \\*\\*when the subgraph is interrupted\\*\\*. Once you resume the graph, you won't be able to access the subgraph state.   ```python theme={null} from langgraph.graph import START, StateGraph from langgraph.checkpoint.memory import MemorySaver from langgraph.types import interrupt, Command from typing\\_extensions import TypedDict class State(TypedDict): foo: str # Subgraph def subgraph\\_node\\_1(state: State): value = interrupt(\"Provide value:\") return {\"foo\": state[\"foo\"] + value} subgraph\\_builder = StateGraph(State) subgraph\\_builder.add\\_node(subgraph\\_node\\_1) subgraph\\_builder.add\\_edge(START, \"subgraph\\_node\\_1\") subgraph = subgraph\\_builder.compile() # Parent graph builder = StateGraph(State) builder.add\\_node(\"node\\_1\", subgraph) builder.add\\_edge(START, \"node\\_1\") checkpointer = MemorySaver() graph = builder.compile(checkpointer=checkpointer) config = {\"configurable\": {\"thread\\_id\": \"1\"}} graph.invoke({\"foo\": \"\"}, config) parent\\_state = graph.get\\_state(config) # This will be available only when the subgraph is interrupted. # Once you resume the graph, you won't be able to access the subgraph state. subgraph\\_state = graph.get\\_state(config, subgraphs=True).tasks[0].state # resume the subgraph graph.invoke(Command(resume=\"bar\"), config) ``` 1. This will be available only when the subgraph is interrupted. Once you resume the graph, you won't be able to access the subgraph state.  ## Stream subgraph outputs To include outputs from subgraphs in the streamed outputs, you can set the subgraphs option in the stream method of the parent graph. This will stream outputs from both the parent graph and any subgraphs. ```python theme={null} for chunk in graph.stream( {\"foo\": \"foo\"}, subgraphs=True, # [!code highlight] stream\\_mode=\"updates\", ): print(chunk) ```  ```python theme={null} from typing\\_extensions import TypedDict from langgraph.graph.state import StateGraph, START # Define subgraph class SubgraphState(TypedDict): foo: str bar: str def subgraph\\_node\\_1(state: SubgraphState): return {\"bar\": \"bar\"} def subgraph\\_node\\_2(state: SubgraphState): # note that this node is using a state key ('bar') that is only available in the subgraph # and is sending update on the shared state key ('foo') return {\"foo\": state[\"foo\"] + state[\"bar\"]} subgraph\\_builder = StateGraph(SubgraphState) subgraph\\_builder.add\\_node(subgraph\\_node\\_1) subgraph\\_builder.add\\_node(subgraph\\_node\\_2) subgraph\\_builder.add\\_edge(START, \"subgraph\\_node\\_1\") subgraph\\_builder.add\\_edge(\"subgraph\\_node\\_1\", \"subgraph\\_node\\_2\") subgraph = subgraph\\_builder.compile() # Define parent graph class ParentState(TypedDict): foo: str def node\\_1(state: ParentState): return {\"foo\": \"hi! \" + state[\"foo\"]} builder = StateGraph(ParentState) builder.add\\_node(\"node\\_1\", node\\_1) builder.add\\_node(\"node\\_2\", subgraph) builder.add\\_edge(START, \"node\\_1\") builder.add\\_edge(\"node\\_1\", \"node\\_2\") graph = builder.compile() for chunk in graph.stream( {\"foo\": \"foo\"}, stream\\_mode=\"updates\", subgraphs=True, # [!code highlight] ): print(chunk) ``` ``` ((), {'node\\_1': {'foo': 'hi! foo'}}) (('node\\_2:e58e5673-a661-ebb0-70d4-e298a7fc28b7',), {'subgraph\\_node\\_1': {'bar': 'bar'}}) (('node\\_2:e58e5673-a661-ebb0-70d4-e298a7fc28b7',), {'subgraph\\_node\\_2': {'foo': 'hi! foobar'}}) ((), {'node\\_2': {'foo': 'hi! foobar'}}) ```  \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/use-subgraphs.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/use-time-travel.md",
      "title": null,
      "raw_content": "# Use time-travel When working with non-deterministic systems that make model-based decisions (e.g., agents powered by LLMs), it can be useful to examine their decision-making process in detail: 1. \\*\\*Understand reasoning\\*\\*: Analyze the steps that led to a successful result. 2. \\*\\*Debug mistakes\\*\\*: Identify where and why errors occurred. 3. \\*\\*Explore alternatives\\*\\*: Test different paths to uncover better solutions. LangGraph provides [time travel](/oss/python/langgraph/use-time-travel) functionality to support these use cases. Specifically, you can resume execution from a prior checkpoint \u2014 either replaying the same state or modifying it to explore alternatives. In all cases, resuming past execution produces a new fork in the history. To use [time-travel](/oss/python/langgraph/use-time-travel) in LangGraph: 1. [Run the graph](#1-run-the-graph) with initial inputs using [`invoke`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.invoke) or [`stream`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.stream) methods. 2. [Identify a checkpoint in an existing thread](#2-identify-a-checkpoint): Use the [`get\\_state\\_history`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.get\\_state\\_history) method to retrieve the execution history for a specific `thread\\_id` and locate the desired `checkpoint\\_id`. Alternatively, set an [interrupt](/oss/python/langgraph/interrupts) before the node(s) where you want execution to pause. You can then find the most recent checkpoint recorded up to that interrupt. 3. [Update the graph state (optional)](#3-update-the-state-optional): Use the [`update\\_state`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.update\\_state) method to modify the graph's state at the checkpoint and resume execution from alternative state. 4. [Resume execution from the checkpoint](#4-resume-execution-from-the-checkpoint): Use the `invoke` or `stream` methods with an input of `None` and a configuration containing the appropriate `thread\\_id` and `checkpoint\\_id`.  For a conceptual overview of time-travel, see [Time travel](/oss/python/langgraph/use-time-travel).  ## In a workflow This example builds a simple LangGraph workflow that generates a joke topic and writes a joke using an LLM. It demonstrates how to run the graph, retrieve past execution checkpoints, optionally modify the state, and resume execution from a chosen checkpoint to explore alternate outcomes. ### Setup First we need to install the packages required ```python theme={null} %%capture --no-stderr pip install --quiet -U langgraph langchain\\_anthropic ``` Next, we need to set API keys for Anthropic (the LLM we will use) ```python theme={null} import getpass import os def \\_set\\_env(var: str): if not os.environ.get(var): os.environ[var] = getpass.getpass(f\"{var}: \") \\_set\\_env(\"ANTHROPIC\\_API\\_KEY\") ```  Sign up for [LangSmith](https://smith.langchain.com) to quickly spot issues and improve the performance of your LangGraph projects. LangSmith lets you use trace data to debug, test, and monitor your LLM apps built with LangGraph.  ```python theme={null} import uuid from typing\\_extensions import TypedDict, NotRequired from langgraph.graph import StateGraph, START, END from langchain.chat\\_models import init\\_chat\\_model from langgraph.checkpoint.memory import InMemorySaver class State(TypedDict): topic: NotRequired[str] joke: NotRequired[str] model = init\\_chat\\_model( \"claude-sonnet-4-5-20250929\", temperature=0, ) def generate\\_topic(state: State): \"\"\"LLM call to generate a topic for the joke\"\"\" msg = model.invoke(\"Give me a funny topic for a joke\") return {\"topic\": msg.content} def write\\_joke(state: State): \"\"\"LLM call to write a joke based on the topic\"\"\" msg = model.invoke(f\"Write a short joke about {state['topic']}\") return {\"joke\": msg.content} # Build workflow workflow = StateGraph(State) # Add nodes workflow.add\\_node(\"generate\\_topic\", generate\\_topic) workflow.add\\_node(\"write\\_joke\", write\\_joke) # Add edges to connect nodes workflow.add\\_edge(START, \"generate\\_topic\") workflow.add\\_edge(\"generate\\_topic\", \"write\\_joke\") workflow.add\\_edge(\"write\\_joke\", END) # Compile checkpointer = InMemorySaver() graph = workflow.compile(checkpointer=checkpointer) graph ``` ### 1. Run the graph ```python theme={null} config = { \"configurable\": { \"thread\\_id\": uuid.uuid4(), } } state = graph.invoke({}, config) print(state[\"topic\"]) print() print(state[\"joke\"]) ``` \\*\\*Output:\\*\\* ``` How about \"The Secret Life of Socks in the Dryer\"? You know, exploring the mysterious phenomenon of how socks go into the laundry as pairs but come out as singles. Where do they go? Are they starting new lives elsewhere? Is there a sock paradise we don't know about? There's a lot of comedic potential in the everyday mystery that unites us all! # The Secret Life of Socks in the Dryer I finally discovered where all my missing socks go after the dryer. Turns out they're not missing at all\u2014they've just eloped with someone else's socks from the laundromat to start new lives together. My blue argyle is now living in Bermuda with a red polka dot, posting vacation photos on Sockstagram and sending me lint as alimony. ``` ### 2. Identify a checkpoint ```python theme={null} # The states are returned in reverse chronological order. states = list(graph.get\\_state\\_history(config)) for state in states: print(state.next) print(state.config[\"configurable\"][\"checkpoint\\_id\"]) print() ``` \\*\\*Output:\\*\\* ``` () 1f02ac4a-ec9f-6524-8002-8f7b0bbeed0e ('write\\_joke',) 1f02ac4a-ce2a-6494-8001-cb2e2d651227 ('generate\\_topic',) 1f02ac4a-a4e0-630d-8000-b73c254ba748 ('\\_\\_start\\_\\_',) 1f02ac4a-a4dd-665e-bfff-e6c8c44315d9 ``` ```python theme={null} # This is the state before last (states are listed in chronological order) selected\\_state = states[1] print(selected\\_state.next) print(selected\\_state.values) ``` \\*\\*Output:\\*\\* ``` ('write\\_joke',) {'topic': 'How about \"The Secret Life of Socks in the Dryer\"? You know, exploring the mysterious phenomenon of how socks go into the laundry as pairs but come out as singles. Where do they go? Are they starting new lives elsewhere? Is there a sock paradise we don\\\\'t know about? There\\\\'s a lot of comedic potential in the everyday mystery that unites us all!'} ```  ### 3. Update the state [`update\\_state`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.CompiledStateGraph.update\\_state) will create a new checkpoint. The new checkpoint will be associated with the same thread, but a new checkpoint ID. ```python theme={null} new\\_config = graph.update\\_state(selected\\_state.config, values={\"topic\": \"chickens\"}) print(new\\_config) ``` \\*\\*Output:\\*\\* ``` {'configurable': {'thread\\_id': 'c62e2e03-c27b-4cb6-8cea-ea9bfedae006', 'checkpoint\\_ns': '', 'checkpoint\\_id': '1f02ac4a-ecee-600b-8002-a1d21df32e4c'}} ``` ### 4. Resume execution from the checkpoint ```python theme={null} graph.invoke(None, new\\_config) ``` \\*\\*Output:\\*\\* ```python theme={null} {'topic': 'chickens', 'joke': 'Why did the chicken join a band?\\n\\nBecause it had excellent drumsticks!'} ``` \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/use-time-travel.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/langgraph/workflows-agents.md",
      "title": null,
      "raw_content": "# Workflows and agents This guide reviews common workflow and agent patterns. \\* Workflows have predetermined code paths and are designed to operate in a certain order. \\* Agents are dynamic and define their own processes and tool usage. LangGraph offers several benefits when building agents and workflows, including [persistence](/oss/python/langgraph/persistence), [streaming](/oss/python/langgraph/streaming), and support for debugging as well as [deployment](/oss/python/langgraph/deploy). ## Setup To build a workflow or agent, you can use [any chat model](/oss/python/integrations/chat) that supports structured outputs and tool calling. The following example uses Anthropic: 1. Install dependencies: ```bash theme={null} pip install langchain\\_core langchain-anthropic langgraph ``` 2. Initialize the LLM: ```python theme={null} import os import getpass from langchain\\_anthropic import ChatAnthropic def \\_set\\_env(var: str): if not os.environ.get(var): os.environ[var] = getpass.getpass(f\"{var}: \") \\_set\\_env(\"ANTHROPIC\\_API\\_KEY\") llm = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\") ``` ## LLMs and augmentations Workflows and agentic systems are based on LLMs and the various augmentations you add to them. [Tool calling](/oss/python/langchain/tools), [structured outputs](/oss/python/langchain/structured-output), and [short term memory](/oss/python/langchain/short-term-memory) are a few options for tailoring LLMs to your needs. ```python theme={null} # Schema for structured output from pydantic import BaseModel, Field class SearchQuery(BaseModel): search\\_query: str = Field(None, description=\"Query that is optimized web search.\") justification: str = Field( None, description=\"Why this query is relevant to the user's request.\" ) # Augment the LLM with schema for structured output structured\\_llm = llm.with\\_structured\\_output(SearchQuery) # Invoke the augmented LLM output = structured\\_llm.invoke(\"How does Calcium CT score relate to high cholesterol?\") # Define a tool def multiply(a: int, b: int) -> int: return a \\* b # Augment the LLM with tools llm\\_with\\_tools = llm.bind\\_tools([multiply]) # Invoke the LLM with input that triggers the tool call msg = llm\\_with\\_tools.invoke(\"What is 2 times 3?\") # Get the tool call msg.tool\\_calls ``` ## Prompt chaining Prompt chaining is when each LLM call processes the output of the previous call. It's often used for performing well-defined tasks that can be broken down into smaller, verifiable steps. Some examples include: \\* Translating documents into different languages \\* Verifying generated content for consistency  ```python Graph API theme={null} from typing\\_extensions import TypedDict from langgraph.graph import StateGraph, START, END from IPython.display import Image, display # Graph state class State(TypedDict): topic: str joke: str improved\\_joke: str final\\_joke: str # Nodes def generate\\_joke(state: State): \"\"\"First LLM call to generate initial joke\"\"\" msg = llm.invoke(f\"Write a short joke about {state['topic']}\") return {\"joke\": msg.content} def check\\_punchline(state: State): \"\"\"Gate function to check if the joke has a punchline\"\"\" # Simple check - does the joke contain \"?\" or \"!\" if \"?\" in state[\"joke\"] or \"!\" in state[\"joke\"]: return \"Pass\" return \"Fail\" def improve\\_joke(state: State): \"\"\"Second LLM call to improve the joke\"\"\" msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {state['joke']}\") return {\"improved\\_joke\": msg.content} def polish\\_joke(state: State): \"\"\"Third LLM call for final polish\"\"\" msg = llm.invoke(f\"Add a surprising twist to this joke: {state['improved\\_joke']}\") return {\"final\\_joke\": msg.content} # Build workflow workflow = StateGraph(State) # Add nodes workflow.add\\_node(\"generate\\_joke\", generate\\_joke) workflow.add\\_node(\"improve\\_joke\", improve\\_joke) workflow.add\\_node(\"polish\\_joke\", polish\\_joke) # Add edges to connect nodes workflow.add\\_edge(START, \"generate\\_joke\") workflow.add\\_conditional\\_edges( \"generate\\_joke\", check\\_punchline, {\"Fail\": \"improve\\_joke\", \"Pass\": END} ) workflow.add\\_edge(\"improve\\_joke\", \"polish\\_joke\") workflow.add\\_edge(\"polish\\_joke\", END) # Compile chain = workflow.compile() # Show workflow display(Image(chain.get\\_graph().draw\\_mermaid\\_png())) # Invoke state = chain.invoke({\"topic\": \"cats\"}) print(\"Initial joke:\") print(state[\"joke\"]) print(\"\\n--- --- ---\\n\") if \"improved\\_joke\" in state: print(\"Improved joke:\") print(state[\"improved\\_joke\"]) print(\"\\n--- --- ---\\n\") print(\"Final joke:\") print(state[\"final\\_joke\"]) else: print(\"Final joke:\") print(state[\"joke\"]) ``` ```python Functional API theme={null} from langgraph.func import entrypoint, task # Tasks @task def generate\\_joke(topic: str): \"\"\"First LLM call to generate initial joke\"\"\" msg = llm.invoke(f\"Write a short joke about {topic}\") return msg.content def check\\_punchline(joke: str): \"\"\"Gate function to check if the joke has a punchline\"\"\" # Simple check - does the joke contain \"?\" or \"!\" if \"?\" in joke or \"!\" in joke: return \"Fail\" return \"Pass\" @task def improve\\_joke(joke: str): \"\"\"Second LLM call to improve the joke\"\"\" msg = llm.invoke(f\"Make this joke funnier by adding wordplay: {joke}\") return msg.content @task def polish\\_joke(joke: str): \"\"\"Third LLM call for final polish\"\"\" msg = llm.invoke(f\"Add a surprising twist to this joke: {joke}\") return msg.content @entrypoint() def prompt\\_chaining\\_workflow(topic: str): original\\_joke = generate\\_joke(topic).result() if check\\_punchline(original\\_joke) == \"Pass\": return original\\_joke improved\\_joke = improve\\_joke(original\\_joke).result() return polish\\_joke(improved\\_joke).result() # Invoke for step in prompt\\_chaining\\_workflow.stream(\"cats\", stream\\_mode=\"updates\"): print(step) print(\"\\n\") ```  ## Parallelization With parallelization, LLMs work simultaneously on a task. This is either done by running multiple independent subtasks at the same time, or running the same task multiple times to check for different outputs. Parallelization is commonly used to: \\* Split up subtasks and run them in parallel, which increases speed \\* Run tasks multiple times to check for different outputs, which increases confidence Some examples include: \\* Running one subtask that processes a document for keywords, and a second subtask to check for formatting errors \\* Running a task multiple times that scores a document for accuracy based on different criteria, like the number of citations, the number of sources used, and the quality of the sources  ```python Graph API theme={null} # Graph state class State(TypedDict): topic: str joke: str story: str poem: str combined\\_output: str # Nodes def call\\_llm\\_1(state: State): \"\"\"First LLM call to generate initial joke\"\"\" msg = llm.invoke(f\"Write a joke about {state['topic']}\") return {\"joke\": msg.content} def call\\_llm\\_2(state: State): \"\"\"Second LLM call to generate story\"\"\" msg = llm.invoke(f\"Write a story about {state['topic']}\") return {\"story\": msg.content} def call\\_llm\\_3(state: State): \"\"\"Third LLM call to generate poem\"\"\" msg = llm.invoke(f\"Write a poem about {state['topic']}\") return {\"poem\": msg.content} def aggregator(state: State): \"\"\"Combine the joke and story into a single output\"\"\" combined = f\"Here's a story, joke, and poem about {state['topic']}!\\n\\n\" combined += f\"STORY:\\n{state['story']}\\n\\n\" combined += f\"JOKE:\\n{state['joke']}\\n\\n\" combined += f\"POEM:\\n{state['poem']}\" return {\"combined\\_output\": combined} # Build workflow parallel\\_builder = StateGraph(State) # Add nodes parallel\\_builder.add\\_node(\"call\\_llm\\_1\", call\\_llm\\_1) parallel\\_builder.add\\_node(\"call\\_llm\\_2\", call\\_llm\\_2) parallel\\_builder.add\\_node(\"call\\_llm\\_3\", call\\_llm\\_3) parallel\\_builder.add\\_node(\"aggregator\", aggregator) # Add edges to connect nodes parallel\\_builder.add\\_edge(START, \"call\\_llm\\_1\") parallel\\_builder.add\\_edge(START, \"call\\_llm\\_2\") parallel\\_builder.add\\_edge(START, \"call\\_llm\\_3\") parallel\\_builder.add\\_edge(\"call\\_llm\\_1\", \"aggregator\") parallel\\_builder.add\\_edge(\"call\\_llm\\_2\", \"aggregator\") parallel\\_builder.add\\_edge(\"call\\_llm\\_3\", \"aggregator\") parallel\\_builder.add\\_edge(\"aggregator\", END) parallel\\_workflow = parallel\\_builder.compile() # Show workflow display(Image(parallel\\_workflow.get\\_graph().draw\\_mermaid\\_png())) # Invoke state = parallel\\_workflow.invoke({\"topic\": \"cats\"}) print(state[\"combined\\_output\"]) ``` ```python Functional API theme={null} @task def call\\_llm\\_1(topic: str): \"\"\"First LLM call to generate initial joke\"\"\" msg = llm.invoke(f\"Write a joke about {topic}\") return msg.content @task def call\\_llm\\_2(topic: str): \"\"\"Second LLM call to generate story\"\"\" msg = llm.invoke(f\"Write a story about {topic}\") return msg.content @task def call\\_llm\\_3(topic): \"\"\"Third LLM call to generate poem\"\"\" msg = llm.invoke(f\"Write a poem about {topic}\") return msg.content @task def aggregator(topic, joke, story, poem): \"\"\"Combine the joke and story into a single output\"\"\" combined = f\"Here's a story, joke, and poem about {topic}!\\n\\n\" combined += f\"STORY:\\n{story}\\n\\n\" combined += f\"JOKE:\\n{joke}\\n\\n\" combined += f\"POEM:\\n{poem}\" return combined # Build workflow @entrypoint() def parallel\\_workflow(topic: str): joke\\_fut = call\\_llm\\_1(topic) story\\_fut = call\\_llm\\_2(topic) poem\\_fut = call\\_llm\\_3(topic) return aggregator( topic, joke\\_fut.result(), story\\_fut.result(), poem\\_fut.result() ).result() # Invoke for step in parallel\\_workflow.stream(\"cats\", stream\\_mode=\"updates\"): print(step) print(\"\\n\") ```  ## Routing Routing workflows process inputs and then directs them to context-specific tasks. This allows you to define specialized flows for complex tasks. For example, a workflow built to answer product related questions might process the type of question first, and then route the request to specific processes for pricing, refunds, returns, etc.  ```python Graph API theme={null} from typing\\_extensions import Literal from langchain.messages import HumanMessage, SystemMessage # Schema for structured output to use as routing logic class Route(BaseModel): step: Literal[\"poem\", \"story\", \"joke\"] = Field( None, description=\"The next step in the routing process\" ) # Augment the LLM with schema for structured output router = llm.with\\_structured\\_output(Route) # State class State(TypedDict): input: str decision: str output: str # Nodes def llm\\_call\\_1(state: State): \"\"\"Write a story\"\"\" result = llm.invoke(state[\"input\"]) return {\"output\": result.content} def llm\\_call\\_2(state: State): \"\"\"Write a joke\"\"\" result = llm.invoke(state[\"input\"]) return {\"output\": result.content} def llm\\_call\\_3(state: State): \"\"\"Write a poem\"\"\" result = llm.invoke(state[\"input\"]) return {\"output\": result.content} def llm\\_call\\_router(state: State): \"\"\"Route the input to the appropriate node\"\"\" # Run the augmented LLM with structured output to serve as routing logic decision = router.invoke( [ SystemMessage( content=\"Route the input to story, joke, or poem based on the user's request.\" ), HumanMessage(content=state[\"input\"]), ] ) return {\"decision\": decision.step} # Conditional edge function to route to the appropriate node def route\\_decision(state: State): # Return the node name you want to visit next if state[\"decision\"] == \"story\": return \"llm\\_call\\_1\" elif state[\"decision\"] == \"joke\": return \"llm\\_call\\_2\" elif state[\"decision\"] == \"poem\": return \"llm\\_call\\_3\" # Build workflow router\\_builder = StateGraph(State) # Add nodes router\\_builder.add\\_node(\"llm\\_call\\_1\", llm\\_call\\_1) router\\_builder.add\\_node(\"llm\\_call\\_2\", llm\\_call\\_2) router\\_builder.add\\_node(\"llm\\_call\\_3\", llm\\_call\\_3) router\\_builder.add\\_node(\"llm\\_call\\_router\", llm\\_call\\_router) # Add edges to connect nodes router\\_builder.add\\_edge(START, \"llm\\_call\\_router\") router\\_builder.add\\_conditional\\_edges( \"llm\\_call\\_router\", route\\_decision, { # Name returned by route\\_decision : Name of next node to visit \"llm\\_call\\_1\": \"llm\\_call\\_1\", \"llm\\_call\\_2\": \"llm\\_call\\_2\", \"llm\\_call\\_3\": \"llm\\_call\\_3\", }, ) router\\_builder.add\\_edge(\"llm\\_call\\_1\", END) router\\_builder.add\\_edge(\"llm\\_call\\_2\", END) router\\_builder.add\\_edge(\"llm\\_call\\_3\", END) # Compile workflow router\\_workflow = router\\_builder.compile() # Show the workflow display(Image(router\\_workflow.get\\_graph().draw\\_mermaid\\_png())) # Invoke state = router\\_workflow.invoke({\"input\": \"Write me a joke about cats\"}) print(state[\"output\"]) ``` ```python Functional API theme={null} from typing\\_extensions import Literal from pydantic import BaseModel from langchain.messages import HumanMessage, SystemMessage # Schema for structured output to use as routing logic class Route(BaseModel): step: Literal[\"poem\", \"story\", \"joke\"] = Field( None, description=\"The next step in the routing process\" ) # Augment the LLM with schema for structured output router = llm.with\\_structured\\_output(Route) @task def llm\\_call\\_1(input\\_: str): \"\"\"Write a story\"\"\" result = llm.invoke(input\\_) return result.content @task def llm\\_call\\_2(input\\_: str): \"\"\"Write a joke\"\"\" result = llm.invoke(input\\_) return result.content @task def llm\\_call\\_3(input\\_: str): \"\"\"Write a poem\"\"\" result = llm.invoke(input\\_) return result.content def llm\\_call\\_router(input\\_: str): \"\"\"Route the input to the appropriate node\"\"\" # Run the augmented LLM with structured output to serve as routing logic decision = router.invoke( [ SystemMessage( content=\"Route the input to story, joke, or poem based on the user's request.\" ), HumanMessage(content=input\\_), ] ) return decision.step # Create workflow @entrypoint() def router\\_workflow(input\\_: str): next\\_step = llm\\_call\\_router(input\\_) if next\\_step == \"story\": llm\\_call = llm\\_call\\_1 elif next\\_step == \"joke\": llm\\_call = llm\\_call\\_2 elif next\\_step == \"poem\": llm\\_call = llm\\_call\\_3 return llm\\_call(input\\_).result() # Invoke for step in router\\_workflow.stream(\"Write me a joke about cats\", stream\\_mode=\"updates\"): print(step) print(\"\\n\") ```  ## Orchestrator-worker In an orchestrator-worker configuration, the orchestrator: \\* Breaks down tasks into subtasks \\* Delegates subtasks to workers \\* Synthesizes worker outputs into a final result Orchestrator-worker workflows provide more flexibility and are often used when subtasks cannot be predefined the way they can with [parallelization](#parallelization). This is common with workflows that write code or need to update content across multiple files. For example, a workflow that needs to update installation instructions for multiple Python libraries across an unknown number of documents might use this pattern.  ```python Graph API theme={null} from typing import Annotated, List import operator # Schema for structured output to use in planning class Section(BaseModel): name: str = Field( description=\"Name for this section of the report.\", ) description: str = Field( description=\"Brief overview of the main topics and concepts to be covered in this section.\", ) class Sections(BaseModel): sections: List[Section] = Field( description=\"Sections of the report.\", ) # Augment the LLM with schema for structured output planner = llm.with\\_structured\\_output(Sections) ``` ```python Functional API theme={null} from typing import List # Schema for structured output to use in planning class Section(BaseModel): name: str = Field( description=\"Name for this section of the report.\", ) description: str = Field( description=\"Brief overview of the main topics and concepts to be covered in this section.\", ) class Sections(BaseModel): sections: List[Section] = Field( description=\"Sections of the report.\", ) # Augment the LLM with schema for structured output planner = llm.with\\_structured\\_output(Sections) @task def orchestrator(topic: str): \"\"\"Orchestrator that generates a plan for the report\"\"\" # Generate queries report\\_sections = planner.invoke( [ SystemMessage(content=\"Generate a plan for the report.\"), HumanMessage(content=f\"Here is the report topic: {topic}\"), ] ) return report\\_sections.sections @task def llm\\_call(section: Section): \"\"\"Worker writes a section of the report\"\"\" # Generate section result = llm.invoke( [ SystemMessage(content=\"Write a report section.\"), HumanMessage( content=f\"Here is the section name: {section.name} and description: {section.description}\" ), ] ) # Write the updated section to completed sections return result.content @task def synthesizer(completed\\_sections: list[str]): \"\"\"Synthesize full report from sections\"\"\" final\\_report = \"\\n\\n---\\n\\n\".join(completed\\_sections) return final\\_report @entrypoint() def orchestrator\\_worker(topic: str): sections = orchestrator(topic).result() section\\_futures = [llm\\_call(section) for section in sections] final\\_report = synthesizer( [section\\_fut.result() for section\\_fut in section\\_futures] ).result() return final\\_report # Invoke report = orchestrator\\_worker.invoke(\"Create a report on LLM scaling laws\") from IPython.display import Markdown Markdown(report) ```  ### Creating workers in LangGraph Orchestrator-worker workflows are common and LangGraph has built-in support for them. The `Send` API lets you dynamically create worker nodes and send them specific inputs. Each worker has its own state, and all worker outputs are written to a shared state key that is accessible to the orchestrator graph. This gives the orchestrator access to all worker output and allows it to synthesize them into a final output. The example below iterates over a list of sections and uses the `Send` API to send a section to each worker. ```python theme={null} from langgraph.types import Send # Graph state class State(TypedDict): topic: str # Report topic sections: list[Section] # List of report sections completed\\_sections: Annotated[ list, operator.add ] # All workers write to this key in parallel final\\_report: str # Final report # Worker state class WorkerState(TypedDict): section: Section completed\\_sections: Annotated[list, operator.add] # Nodes def orchestrator(state: State): \"\"\"Orchestrator that generates a plan for the report\"\"\" # Generate queries report\\_sections = planner.invoke( [ SystemMessage(content=\"Generate a plan for the report.\"), HumanMessage(content=f\"Here is the report topic: {state['topic']}\"), ] ) return {\"sections\": report\\_sections.sections} def llm\\_call(state: WorkerState): \"\"\"Worker writes a section of the report\"\"\" # Generate section section = llm.invoke( [ SystemMessage( content=\"Write a report section following the provided name and description. Include no preamble for each section. Use markdown formatting.\" ), HumanMessage( content=f\"Here is the section name: {state['section'].name} and description: {state['section'].description}\" ), ] ) # Write the updated section to completed sections return {\"completed\\_sections\": [section.content]} def synthesizer(state: State): \"\"\"Synthesize full report from sections\"\"\" # List of completed sections completed\\_sections = state[\"completed\\_sections\"] # Format completed section to str to use as context for final sections completed\\_report\\_sections = \"\\n\\n---\\n\\n\".join(completed\\_sections) return {\"final\\_report\": completed\\_report\\_sections} # Conditional edge function to create llm\\_call workers that each write a section of the report def assign\\_workers(state: State): \"\"\"Assign a worker to each section in the plan\"\"\" # Kick off section writing in parallel via Send() API return [Send(\"llm\\_call\", {\"section\": s}) for s in state[\"sections\"]] # Build workflow orchestrator\\_worker\\_builder = StateGraph(State) # Add the nodes orchestrator\\_worker\\_builder.add\\_node(\"orchestrator\", orchestrator) orchestrator\\_worker\\_builder.add\\_node(\"llm\\_call\", llm\\_call) orchestrator\\_worker\\_builder.add\\_node(\"synthesizer\", synthesizer) # Add edges to connect nodes orchestrator\\_worker\\_builder.add\\_edge(START, \"orchestrator\") orchestrator\\_worker\\_builder.add\\_conditional\\_edges( \"orchestrator\", assign\\_workers, [\"llm\\_call\"] ) orchestrator\\_worker\\_builder.add\\_edge(\"llm\\_call\", \"synthesizer\") orchestrator\\_worker\\_builder.add\\_edge(\"synthesizer\", END) # Compile the workflow orchestrator\\_worker = orchestrator\\_worker\\_builder.compile() # Show the workflow display(Image(orchestrator\\_worker.get\\_graph().draw\\_mermaid\\_png())) # Invoke state = orchestrator\\_worker.invoke({\"topic\": \"Create a report on LLM scaling laws\"}) from IPython.display import Markdown Markdown(state[\"final\\_report\"]) ``` ## Evaluator-optimizer In evaluator-optimizer workflows, one LLM call creates a response and the other evaluates that response. If the evaluator or a [human-in-the-loop](/oss/python/langgraph/interrupts) determines the response needs refinement, feedback is provided and the response is recreated. This loop continues until an acceptable response is generated. Evaluator-optimizer workflows are commonly used when there's particular success criteria for a task, but iteration is required to meet that criteria. For example, there's not always a perfect match when translating text between two languages. It might take a few iterations to generate a translation with the same meaning across the two languages.  ```python Graph API theme={null} # Graph state class State(TypedDict): joke: str topic: str feedback: str funny\\_or\\_not: str # Schema for structured output to use in evaluation class Feedback(BaseModel): grade: Literal[\"funny\", \"not funny\"] = Field( description=\"Decide if the joke is funny or not.\", ) feedback: str = Field( description=\"If the joke is not funny, provide feedback on how to improve it.\", ) # Augment the LLM with schema for structured output evaluator = llm.with\\_structured\\_output(Feedback) # Nodes def llm\\_call\\_generator(state: State): \"\"\"LLM generates a joke\"\"\" if state.get(\"feedback\"): msg = llm.invoke( f\"Write a joke about {state['topic']} but take into account the feedback: {state['feedback']}\" ) else: msg = llm.invoke(f\"Write a joke about {state['topic']}\") return {\"joke\": msg.content} def llm\\_call\\_evaluator(state: State): \"\"\"LLM evaluates the joke\"\"\" grade = evaluator.invoke(f\"Grade the joke {state['joke']}\") return {\"funny\\_or\\_not\": grade.grade, \"feedback\": grade.feedback} # Conditional edge function to route back to joke generator or end based upon feedback from the evaluator def route\\_joke(state: State): \"\"\"Route back to joke generator or end based upon feedback from the evaluator\"\"\" if state[\"funny\\_or\\_not\"] == \"funny\": return \"Accepted\" elif state[\"funny\\_or\\_not\"] == \"not funny\": return \"Rejected + Feedback\" # Build workflow optimizer\\_builder = StateGraph(State) # Add the nodes optimizer\\_builder.add\\_node(\"llm\\_call\\_generator\", llm\\_call\\_generator) optimizer\\_builder.add\\_node(\"llm\\_call\\_evaluator\", llm\\_call\\_evaluator) # Add edges to connect nodes optimizer\\_builder.add\\_edge(START, \"llm\\_call\\_generator\") optimizer\\_builder.add\\_edge(\"llm\\_call\\_generator\", \"llm\\_call\\_evaluator\") optimizer\\_builder.add\\_conditional\\_edges( \"llm\\_call\\_evaluator\", route\\_joke, { # Name returned by route\\_joke : Name of next node to visit \"Accepted\": END, \"Rejected + Feedback\": \"llm\\_call\\_generator\", }, ) # Compile the workflow optimizer\\_workflow = optimizer\\_builder.compile() # Show the workflow display(Image(optimizer\\_workflow.get\\_graph().draw\\_mermaid\\_png())) # Invoke state = optimizer\\_workflow.invoke({\"topic\": \"Cats\"}) print(state[\"joke\"]) ``` ```python Functional API theme={null} # Schema for structured output to use in evaluation class Feedback(BaseModel): grade: Literal[\"funny\", \"not funny\"] = Field( description=\"Decide if the joke is funny or not.\", ) feedback: str = Field( description=\"If the joke is not funny, provide feedback on how to improve it.\", ) # Augment the LLM with schema for structured output evaluator = llm.with\\_structured\\_output(Feedback) # Nodes @task def llm\\_call\\_generator(topic: str, feedback: Feedback): \"\"\"LLM generates a joke\"\"\" if feedback: msg = llm.invoke( f\"Write a joke about {topic} but take into account the feedback: {feedback}\" ) else: msg = llm.invoke(f\"Write a joke about {topic}\") return msg.content @task def llm\\_call\\_evaluator(joke: str): \"\"\"LLM evaluates the joke\"\"\" feedback = evaluator.invoke(f\"Grade the joke {joke}\") return feedback @entrypoint() def optimizer\\_workflow(topic: str): feedback = None while True: joke = llm\\_call\\_generator(topic, feedback).result() feedback = llm\\_call\\_evaluator(joke).result() if feedback.grade == \"funny\": break return joke # Invoke for step in optimizer\\_workflow.stream(\"Cats\", stream\\_mode=\"updates\"): print(step) print(\"\\n\") ```  ## Agents Agents are typically implemented as an LLM performing actions using [tools](/oss/python/langchain/tools). They operate in continuous feedback loops, and are used in situations where problems and solutions are unpredictable. Agents have more autonomy than workflows, and can make decisions about the tools they use and how to solve problems. You can still define the available toolset and guidelines for how agents behave.  To get started with agents, see the [quickstart](/oss/python/langchain/quickstart) or read more about [how they work](/oss/python/langchain/agents) in LangChain.  ```python Using tools theme={null} from langchain.tools import tool # Define tools @tool def multiply(a: int, b: int) -> int: \"\"\"Multiply `a` and `b`. Args: a: First int b: Second int \"\"\" return a \\* b @tool def add(a: int, b: int) -> int: \"\"\"Adds `a` and `b`. Args: a: First int b: Second int \"\"\" return a + b @tool def divide(a: int, b: int) -> float: \"\"\"Divide `a` and `b`. Args: a: First int b: Second int \"\"\" return a / b # Augment the LLM with tools tools = [add, multiply, divide] tools\\_by\\_name = {tool.name: tool for tool in tools} llm\\_with\\_tools = llm.bind\\_tools(tools) ```  ```python Graph API theme={null} from langgraph.graph import MessagesState from langchain.messages import SystemMessage, HumanMessage, ToolMessage # Nodes def llm\\_call(state: MessagesState): \"\"\"LLM decides whether to call a tool or not\"\"\" return { \"messages\": [ llm\\_with\\_tools.invoke( [ SystemMessage( content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\" ) ] + state[\"messages\"] ) ] } def tool\\_node(state: dict): \"\"\"Performs the tool call\"\"\" result = [] for tool\\_call in state[\"messages\"][-1].tool\\_calls: tool = tools\\_by\\_name[tool\\_call[\"name\"]] observation = tool.invoke(tool\\_call[\"args\"]) result.append(ToolMessage(content=observation, tool\\_call\\_id=tool\\_call[\"id\"])) return {\"messages\": result} # Conditional edge function to route to the tool node or end based upon whether the LLM made a tool call def should\\_continue(state: MessagesState) -> Literal[\"tool\\_node\", END]: \"\"\"Decide if we should continue the loop or stop based upon whether the LLM made a tool call\"\"\" messages = state[\"messages\"] last\\_message = messages[-1] # If the LLM makes a tool call, then perform an action if last\\_message.tool\\_calls: return \"tool\\_node\" # Otherwise, we stop (reply to the user) return END # Build workflow agent\\_builder = StateGraph(MessagesState) # Add nodes agent\\_builder.add\\_node(\"llm\\_call\", llm\\_call) agent\\_builder.add\\_node(\"tool\\_node\", tool\\_node) # Add edges to connect nodes agent\\_builder.add\\_edge(START, \"llm\\_call\") agent\\_builder.add\\_conditional\\_edges( \"llm\\_call\", should\\_continue, [\"tool\\_node\", END] ) agent\\_builder.add\\_edge(\"tool\\_node\", \"llm\\_call\") # Compile the agent agent = agent\\_builder.compile() # Show the agent display(Image(agent.get\\_graph(xray=True).draw\\_mermaid\\_png())) # Invoke messages = [HumanMessage(content=\"Add 3 and 4.\")] messages = agent.invoke({\"messages\": messages}) for m in messages[\"messages\"]: m.pretty\\_print() ``` ```python Functional API theme={null} from langgraph.graph import add\\_messages from langchain.messages import ( SystemMessage, HumanMessage, ToolCall, ) from langchain\\_core.messages import BaseMessage @task def call\\_llm(messages: list[BaseMessage]): \"\"\"LLM decides whether to call a tool or not\"\"\" return llm\\_with\\_tools.invoke( [ SystemMessage( content=\"You are a helpful assistant tasked with performing arithmetic on a set of inputs.\" ) ] + messages ) @task def call\\_tool(tool\\_call: ToolCall): \"\"\"Performs the tool call\"\"\" tool = tools\\_by\\_name[tool\\_call[\"name\"]] return tool.invoke(tool\\_call) @entrypoint() def agent(messages: list[BaseMessage]): llm\\_response = call\\_llm(messages).result() while True: if not llm\\_response.tool\\_calls: break # Execute tools tool\\_result\\_futures = [ call\\_tool(tool\\_call) for tool\\_call in llm\\_response.tool\\_calls ] tool\\_results = [fut.result() for fut in tool\\_result\\_futures] messages = add\\_messages(messages, [llm\\_response, \\*tool\\_results]) llm\\_response = call\\_llm(messages).result() messages = add\\_messages(messages, llm\\_response) return messages # Invoke messages = [HumanMessage(content=\"Add 3 and 4.\")] for chunk in agent.stream(messages, stream\\_mode=\"updates\"): print(chunk) print(\"\\n\") ```  \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/langgraph/workflows-agents.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/learn.md",
      "title": null,
      "raw_content": "# Learn > Tutorials, conceptual guides, and resources to help you get started. In the \\*\\*Learn\\*\\* section of the documentation, you'll find a collection of tutorials, conceptual overviews, and additional resources to help you build powerful applications with LangChain and LangGraph. ## Use Cases Below are tutorials for common use cases, organized by framework. ### LangChain [LangChain](/oss/python/langchain/overview) [agent](/oss/python/langchain/agents) implementations make it easy to get started for most use cases.  Build a semantic search engine over a PDF with LangChain components.   Create a Retrieval Augmented Generation (RAG) agent.   Build a SQL agent to interact with databases with human-in-the-loop review.   Build a personal assistant that delegates to sub-agents.  ### LangGraph LangChain's [agent](/oss/python/langchain/agents) implementations use [LangGraph](/oss/python/langgraph/overview) primitives. If deeper customization is required, agents can be implemented directly in LangGraph.  Build a RAG agent using LangGraph primitives for fine-grained control.   Implement a SQL agent directly in LangGraph for maximum flexibility.  ## Conceptual Overviews These guides explain the core concepts and APIs underlying LangChain and LangGraph.  Understand persistence of interactions within and across threads.   Learn methods for providing AI applications the right information and tools to accomplish a task.   Explore LangGraph\u2019s declarative graph-building API.   Build agents as a single function.  ## Additional Resources  Courses and exercises to level up your LangChain skills.   See how teams are using LangChain and LangGraph in production.  \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/learn.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/migrate/langchain-v1.md",
      "title": null,
      "raw_content": "# LangChain v1 migration guide This guide outlines the major changes between [LangChain v1](/oss/python/releases/langchain-v1) and previous versions. ## Simplified package The `langchain` package namespace has been significantly reduced in v1 to focus on essential building blocks for agents. The streamlined package makes it easier to discover and use the core functionality. ### Namespace | Module | What's available | Notes | | ------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------- | | [`langchain.agents`](https://reference.langchain.com/python/langchain/agents) | [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent), [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState) | Core agent creation functionality | | [`langchain.messages`](https://reference.langchain.com/python/langchain/messages) | Message types, [content blocks](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ContentBlock), [`trim\\_messages`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.trim\\_messages) | Re-exported from `langchain-core` | | [`langchain.tools`](https://reference.langchain.com/python/langchain/tools) | [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool), [`BaseTool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.BaseTool), injection helpers | Re-exported from `langchain-core` | | [`langchain.chat\\_models`](https://reference.langchain.com/python/langchain/models) | [`init\\_chat\\_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat\\_models.init\\_chat\\_model), [`BaseChatModel`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.language\\_models.chat\\_models.BaseChatModel) | Unified model initialization | | [`langchain.embeddings`](https://reference.langchain.com/python/langchain/embeddings) | [`init\\_embeddings`](https://reference.langchain.com/python/langchain\\_core/embeddings/#langchain\\_core.embeddings.embeddings.Embeddings), [`Embeddings`](https://reference.langchain.com/python/langchain\\_core/embeddings/#langchain\\_core.embeddings.embeddings.Embeddings) | Embedding models | ### `langchain-classic` If you were using any of the following from the `langchain` package, you'll need to install [`langchain-classic`](https://pypi.org/project/langchain-classic/) and update your imports: \\* Legacy chains (`LLMChain`, `ConversationChain`, etc.) \\* Retrievers (e.g. `MultiQueryRetriever` or anything from the previous `langchain.retrievers` module) \\* The indexing API \\* The hub module (for managing prompts programmatically) \\* Embeddings modules (e.g. `CacheBackedEmbeddings` and community embeddings) \\* [`langchain-community`](https://pypi.org/project/langchain-community) re-exports \\* Other deprecated functionality  ```python v1 (new) theme={null} # Chains from langchain\\_classic.chains import LLMChain # Retrievers from langchain\\_classic.retrievers import ... # Indexing from langchain\\_classic.indexes import ... # Hub from langchain\\_classic import hub ``` ```python v0 (old) theme={null} # Chains from langchain.chains import LLMChain # Retrievers from langchain.retrievers import ... # Indexing from langchain.indexes import ... # Hub from langchain import hub ```  Install with:  ```bash pip theme={null} pip install langchain-classic ``` ```bash uv theme={null} uv add langchain-classic ```  \\*\\*\\* ## Migrate to `create\\_agent` Prior to v1.0, we recommended using [`langgraph.prebuilt.create\\_react\\_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat\\_agent\\_executor.create\\_react\\_agent) to build agents. Now, we recommend you use [`langchain.agents.create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent) to build agents. The table below outlines what functionality has changed from [`create\\_react\\_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat\\_agent\\_executor.create\\_react\\_agent) to [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent): | Section | TL;DR - What's changed | | -------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | | [Import path](#import-path) | Package moved from `langgraph.prebuilt` to `langchain.agents` | | [Prompts](#prompts) | Parameter renamed to [`system\\_prompt`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent\\(system\\_prompt\\)), dynamic prompts use middleware | | [Pre-model hook](#pre-model-hook) | Replaced by middleware with `before\\_model` method | | [Post-model hook](#post-model-hook) | Replaced by middleware with `after\\_model` method | | [Custom state](#custom-state) | `TypedDict` only, can be defined via [`state\\_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state\\_schema) or middleware | | [Model](#model) | Dynamic selection via middleware, pre-bound models not supported | | [Tools](#tools) | Tool error handling moved to middleware with `wrap\\_tool\\_call` | | [Structured output](#structured-output) | prompted output removed, use `ToolStrategy`/`ProviderStrategy` | | [Streaming node name](#streaming-node-name-rename) | Node name changed from `\"agent\"` to `\"model\"` | | [Runtime context](#runtime-context) | Dependency injection via `context` argument instead of `config[\"configurable\"]` | | [Namespace](#simplified-package) | Streamlined to focus on agent building blocks, legacy code moved to `langchain-classic` | ### Import path The import path for the agent prebuilt has changed from `langgraph.prebuilt` to `langchain.agents`. The name of the function has changed from [`create\\_react\\_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat\\_agent\\_executor.create\\_react\\_agent) to [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent): ```python theme={null} from langgraph.prebuilt import create\\_react\\_agent # [!code --] from langchain.agents import create\\_agent # [!code ++] ``` For more information, see [Agents](/oss/python/langchain/agents). ### Prompts #### Static prompt rename The `prompt` parameter has been renamed to [`system\\_prompt`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent\\(system\\_prompt\\)):  ```python v1 (new) theme={null} from langchain.agents import create\\_agent agent = create\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=[check\\_weather], system\\_prompt=\"You are a helpful assistant\" # [!code highlight] ) ``` ```python v0 (old) theme={null} from langgraph.prebuilt import create\\_react\\_agent agent = create\\_react\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=[check\\_weather], prompt=\"You are a helpful assistant\" # [!code highlight] ) ```  #### `SystemMessage` to string If using [`SystemMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.SystemMessage) objects in the system prompt, extract the string content:  ```python v1 (new) theme={null} from langchain.agents import create\\_agent agent = create\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=[check\\_weather], system\\_prompt=\"You are a helpful assistant\" # [!code highlight] ) ``` ```python v0 (old) theme={null} from langchain.messages import SystemMessage from langgraph.prebuilt import create\\_react\\_agent agent = create\\_react\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=[check\\_weather], prompt=SystemMessage(content=\"You are a helpful assistant\") # [!code highlight] ) ```  #### Dynamic prompts Dynamic prompts are a core context engineering pattern\u2014 they adapt what you tell the model based on the current conversation state. To do this, use the [`@dynamic\\_prompt`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.dynamic\\_prompt) decorator:  ```python v1 (new) theme={null} from dataclasses import dataclass from langchain.agents import create\\_agent from langchain.agents.middleware import dynamic\\_prompt, ModelRequest from langgraph.runtime import Runtime @dataclass class Context: # [!code highlight] user\\_role: str = \"user\" @dynamic\\_prompt # [!code highlight] def dynamic\\_prompt(request: ModelRequest) -> str: # [!code highlight] user\\_role = request.runtime.context.user\\_role base\\_prompt = \"You are a helpful assistant.\" if user\\_role == \"expert\": prompt = ( f\"{base\\_prompt} Provide detailed technical responses.\" ) elif user\\_role == \"beginner\": prompt = ( f\"{base\\_prompt} Explain concepts simply and avoid jargon.\" ) else: prompt = base\\_prompt return prompt # [!code highlight] agent = create\\_agent( model=\"gpt-4o\", tools=tools, middleware=[dynamic\\_prompt], # [!code highlight] context\\_schema=Context ) # Use with context agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"Explain async programming\"}]}, context=Context(user\\_role=\"expert\") ) ``` ```python v0 (old) theme={null} from dataclasses import dataclass from langgraph.prebuilt import create\\_react\\_agent, AgentState from langgraph.runtime import get\\_runtime @dataclass class Context: user\\_role: str def dynamic\\_prompt(state: AgentState) -> str: runtime = get\\_runtime(Context) # [!code highlight] user\\_role = runtime.context.user\\_role base\\_prompt = \"You are a helpful assistant.\" if user\\_role == \"expert\": return f\"{base\\_prompt} Provide detailed technical responses.\" elif user\\_role == \"beginner\": return f\"{base\\_prompt} Explain concepts simply and avoid jargon.\" return base\\_prompt agent = create\\_react\\_agent( model=\"gpt-4o\", tools=tools, prompt=dynamic\\_prompt, context\\_schema=Context ) # Use with context agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"Explain async programming\"}]}, context=Context(user\\_role=\"expert\") ) ```  ### Pre-model hook Pre-model hooks are now implemented as middleware with the `before\\_model` method. This new pattern is more extensible--you can define multiple middlewares to run before the model is called, reusing common patterns across different agents. Common use cases include: \\* Summarizing conversation history \\* Trimming messages \\* Input guardrails, like PII redaction v1 now has summarization middleware as a built in option:  ```python v1 (new) theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import SummarizationMiddleware agent = create\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=tools, middleware=[ SummarizationMiddleware( # [!code highlight] model=\"claude-sonnet-4-5-20250929\", # [!code highlight] trigger={\"tokens\": 1000} # [!code highlight] ) # [!code highlight] ] # [!code highlight] ) ``` ```python v0 (old) theme={null} from langgraph.prebuilt import create\\_react\\_agent, AgentState def custom\\_summarization\\_function(state: AgentState): \"\"\"Custom logic for message summarization.\"\"\" ... agent = create\\_react\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=tools, pre\\_model\\_hook=custom\\_summarization\\_function ) ```  ### Post-model hook Post-model hooks are now implemented as middleware with the `after\\_model` method. This new pattern is more extensible--you can define multiple middlewares to run after the model is called, reusing common patterns across different agents. Common use cases include: \\* [Human in the loop](/oss/python/langchain/human-in-the-loop) \\* Output guardrails v1 has a built in middleware for human in the loop approval for tool calls:  ```python v1 (new) theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import HumanInTheLoopMiddleware agent = create\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=[read\\_email, send\\_email], middleware=[ HumanInTheLoopMiddleware( interrupt\\_on={ \"send\\_email\": { \"description\": \"Please review this email before sending\", \"allowed\\_decisions\": [\"approve\", \"reject\"] } } ) ] ) ``` ```python v0 (old) theme={null} from langgraph.prebuilt import create\\_react\\_agent from langgraph.prebuilt import AgentState def custom\\_human\\_in\\_the\\_loop\\_hook(state: AgentState): \"\"\"Custom logic for human in the loop approval.\"\"\" ... agent = create\\_react\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=[read\\_email, send\\_email], post\\_model\\_hook=custom\\_human\\_in\\_the\\_loop\\_hook ) ```  ### Custom state Custom state extends the default agent state with additional fields. You can define custom state in two ways: 1. \\*\\*Via [`state\\_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state\\_schema) on [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent)\\*\\* - Best for state used in tools 2. \\*\\*Via middleware\\*\\* - Best for state managed by specific middleware hooks and tools attached to said middleware  Defining custom state via middleware is preferred over defining it via [`state\\_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state\\_schema) on [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent) because it allows you to keep state extensions conceptually scoped to the relevant middleware and tools. `state\\_schema` is still supported for backwards compatibility on `create\\_agent`.  #### Defining state via `state\\_schema` Use the [`state\\_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state\\_schema) parameter when your custom state needs to be accessed by tools:  ```python v1 (new) theme={null} from langchain.tools import tool, ToolRuntime from langchain.agents import create\\_agent, AgentState # [!code highlight] # Define custom state extending AgentState class CustomState(AgentState): user\\_name: str @tool # [!code highlight] def greet( runtime: ToolRuntime[None, CustomState] ) -> str: \"\"\"Use this to greet the user by name.\"\"\" user\\_name = runtime.state.get(\"user\\_name\", \"Unknown\") # [!code highlight] return f\"Hello {user\\_name}!\" agent = create\\_agent( # [!code highlight] model=\"claude-sonnet-4-5-20250929\", tools=[greet], state\\_schema=CustomState # [!code highlight] ) ``` ```python v0 (old) theme={null} from typing import Annotated from langgraph.prebuilt import InjectedState, create\\_react\\_agent from langgraph.prebuilt.chat\\_agent\\_executor import AgentState class CustomState(AgentState): user\\_name: str def greet( state: Annotated[CustomState, InjectedState] ) -> str: \"\"\"Use this to greet the user by name.\"\"\" user\\_name = state[\"user\\_name\"] return f\"Hello {user\\_name}!\" agent = create\\_react\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=[greet], state\\_schema=CustomState ) ```  #### Defining state via middleware Middleware can also define custom state by setting the [`state\\_schema`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware.state\\_schema) attribute. This helps to keep state extensions conceptually scoped to the relevant middleware and tools. ```python theme={null} from langchain.agents.middleware import AgentState, AgentMiddleware from typing\\_extensions import NotRequired from typing import Any class CustomState(AgentState): model\\_call\\_count: NotRequired[int] class CallCounterMiddleware(AgentMiddleware[CustomState]): state\\_schema = CustomState # [!code highlight] def before\\_model(self, state: CustomState, runtime) -> dict[str, Any] | None: count = state.get(\"model\\_call\\_count\", 0) if count > 10: return {\"jump\\_to\": \"end\"} return None def after\\_model(self, state: CustomState, runtime) -> dict[str, Any] | None: return {\"model\\_call\\_count\": state.get(\"model\\_call\\_count\", 0) + 1} agent = create\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=[...], middleware=[CallCounterMiddleware()] # [!code highlight] ) ``` See the [middleware documentation](/oss/python/langchain/middleware#custom-state-schema) for more details on defining custom state via middleware. #### State type restrictions [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent) only supports `TypedDict` for state schemas. Pydantic models and dataclasses are no longer supported.  ```python v1 (new) theme={null} from langchain.agents import AgentState, create\\_agent # AgentState is a TypedDict class CustomAgentState(AgentState): # [!code highlight] user\\_id: str agent = create\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=tools, state\\_schema=CustomAgentState # [!code highlight] ) ``` ```python v0 (old) theme={null} from typing\\_extensions import Annotated from pydantic import BaseModel from langgraph.graph import StateGraph from langgraph.graph.messages import add\\_messages from langchain.messages import AnyMessage class AgentState(BaseModel): # [!code highlight] messages: Annotated[list[AnyMessage], add\\_messages] user\\_id: str agent = create\\_react\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=tools, state\\_schema=AgentState ) ```  Simply inherit from `langchain.agents.AgentState` instead of `BaseModel` or decorating with `dataclass`. If you need to perform validation, handle it in middleware hooks instead. ### Model Dynamic model selection allows you to choose different models based on runtime context (e.g., task complexity, cost constraints, or user preferences). [`create\\_react\\_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat\\_agent\\_executor.create\\_react\\_agent) released in v0.6 of [`langgraph-prebuilt`](https://pypi.org/project/langgraph-prebuilt) supported dynamic model and tool selection via a callable passed to the `model` parameter. This functionality has been ported to the middleware interface in v1. #### Dynamic model selection  ```python v1 (new) theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import ( AgentMiddleware, ModelRequest ) from langchain.agents.middleware.types import ModelResponse from langchain\\_openai import ChatOpenAI from typing import Callable basic\\_model = ChatOpenAI(model=\"gpt-5-nano\") advanced\\_model = ChatOpenAI(model=\"gpt-5\") class DynamicModelMiddleware(AgentMiddleware): def wrap\\_model\\_call(self, request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse]) -> ModelResponse: if len(request.state.messages) > self.messages\\_threshold: model = advanced\\_model else: model = basic\\_model return handler(request.override(model=model)) def \\_\\_init\\_\\_(self, messages\\_threshold: int) -> None: self.messages\\_threshold = messages\\_threshold agent = create\\_agent( model=basic\\_model, tools=tools, middleware=[DynamicModelMiddleware(messages\\_threshold=10)] ) ``` ```python v0 (old) theme={null} from langgraph.prebuilt import create\\_react\\_agent, AgentState from langchain\\_openai import ChatOpenAI basic\\_model = ChatOpenAI(model=\"gpt-5-nano\") advanced\\_model = ChatOpenAI(model=\"gpt-5\") def select\\_model(state: AgentState) -> BaseChatModel: # use a more advanced model for longer conversations if len(state.messages) > 10: return advanced\\_model return basic\\_model agent = create\\_react\\_agent( model=select\\_model, tools=tools, ) ```  #### Pre-bound models To better support structured output, [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent) no longer accepts pre-bound models with tools or configuration: ```python theme={null} # No longer supported model\\_with\\_tools = ChatOpenAI().bind\\_tools([some\\_tool]) agent = create\\_agent(model\\_with\\_tools, tools=[]) # Use instead agent = create\\_agent(\"gpt-4o-mini\", tools=[some\\_tool]) ```  Dynamic model functions can return pre-bound models if structured output is \\*not\\* used.  ### Tools The [`tools`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent\\(tools\\)) argument to [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent) accepts a list of: \\* LangChain [`BaseTool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.BaseTool) instances (functions decorated with [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool)) \\* Callable objects (functions) with proper type hints and a docstring \\* `dict` that represents a built-in provider tools The argument will no longer accept [`ToolNode`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.tool\\_node.ToolNode) instances.  ```python v1 (new) theme={null} from langchain.agents import create\\_agent agent = create\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=[check\\_weather, search\\_web] ) ``` ```python v0 (old) theme={null} from langgraph.prebuilt import create\\_react\\_agent, ToolNode agent = create\\_react\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=ToolNode([check\\_weather, search\\_web]) # [!code highlight] ) ```  #### Handling tool errors You can now configure the handling of tool errors with middleware implementing the `wrap\\_tool\\_call` method.  ```python v1 (new) theme={null} @wrap\\_tool\\_call def retry\\_on\\_error(request, handler): max\\_retries = 3 for attempt in range(max\\_retries): try: return handler(request) except Exception: if attempt == max\\_retries - 1: raise ``` ```python v0 (old) theme={null} # Example coming soon ```  ### Structured output #### Node changes Structured output used to be generated in a separate node from the main agent. This is no longer the case. We generate structured output in the main loop, reducing cost and latency. #### Tool and provider strategies In v1, there are two new structured output strategies: \\* `ToolStrategy` uses artificial tool calling to generate structured output \\* `ProviderStrategy` uses provider-native structured output generation  ```python v1 (new) theme={null} from langchain.agents import create\\_agent from langchain.agents.structured\\_output import ToolStrategy, ProviderStrategy from pydantic import BaseModel class OutputSchema(BaseModel): summary: str sentiment: str # Using ToolStrategy agent = create\\_agent( model=\"gpt-4o-mini\", tools=tools, # explicitly using tool strategy response\\_format=ToolStrategy(OutputSchema) # [!code highlight] ) ``` ```python v0 (old) theme={null} from langgraph.prebuilt import create\\_react\\_agent from pydantic import BaseModel class OutputSchema(BaseModel): summary: str sentiment: str agent = create\\_react\\_agent( model=\"gpt-4o-mini\", tools=tools, # using tool strategy by default with no option for provider strategy response\\_format=OutputSchema # [!code highlight] ) # OR agent = create\\_react\\_agent( model=\"gpt-4o-mini\", tools=tools, # using a custom prompt to instruct the model to generate the output schema response\\_format=(\"please generate ...\", OutputSchema) # [!code highlight] ) ```  #### Prompted output removed \\*\\*Prompted output\\*\\* is no longer supported via the `response\\_format` argument. Compared to strategies like artificial tool calling and provider native structured output, prompted output has not proven to be particularly reliable. ### Streaming node name rename When streaming events from agents, the node name has changed from `\"agent\"` to `\"model\"` to better reflect the node's purpose. ### Runtime context When you invoke an agent, it's often the case that you want to pass two types of data: \\* Dynamic state that changes throughout the conversation (e.g., message history) \\* Static context that doesn't change during the conversation (e.g., user metadata) In v1, static context is supported by setting the `context` parameter to `invoke` and `stream`.  ```python v1 (new) theme={null} from dataclasses import dataclass from langchain.agents import create\\_agent @dataclass class Context: user\\_id: str session\\_id: str agent = create\\_agent( model=model, tools=tools, context\\_schema=Context # [!code highlight] ) result = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]}, context=Context(user\\_id=\"123\", session\\_id=\"abc\") # [!code highlight] ) ``` ```python v0 (old) theme={null} from langgraph.prebuilt import create\\_react\\_agent agent = create\\_react\\_agent(model, tools) # Pass context via configurable result = agent.invoke( {\"messages\": [{\"role\": \"user\", \"content\": \"Hello\"}]}, config={ # [!code highlight] \"configurable\": { # [!code highlight] \"user\\_id\": \"123\", # [!code highlight] \"session\\_id\": \"abc\" # [!code highlight] } # [!code highlight] } # [!code highlight] ) ```   The old `config[\"configurable\"]` pattern still works for backward compatibility, but using the new `context` parameter is recommended for new applications or applications migrating to v1.  \\*\\*\\* ## Standard content In v1, messages gain provider-agnostic standard content blocks. Access them via [`message.content\\_blocks`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.messages.BaseMessage.content\\_blocks) for a consistent, typed view across providers. The existing [`message.content`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.messages.BaseMessage.content\\_blocks) field remains unchanged for strings or provider-native structures. ### What changed \\* New [`content\\_blocks`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.messages.BaseMessage.content\\_blocks) property on messages for normalized content \\* Standardized block shapes, documented in [Messages](/oss/python/langchain/messages#standard-content-blocks) \\* Optional serialization of standard blocks into `content` via `LC\\_OUTPUT\\_VERSION=v1` or `output\\_version=\"v1\"` ### Read standardized content  ```python v1 (new) theme={null} from langchain.chat\\_models import init\\_chat\\_model model = init\\_chat\\_model(\"gpt-5-nano\") response = model.invoke(\"Explain AI\") for block in response.content\\_blocks: if block[\"type\"] == \"reasoning\": print(block.get(\"reasoning\")) elif block[\"type\"] == \"text\": print(block.get(\"text\")) ``` ```python v0 (old) theme={null} # Provider-native formats vary; you needed per-provider handling response = model.invoke(\"Explain AI\") for item in response.content: if item.get(\"type\") == \"reasoning\": ... # OpenAI-style reasoning elif item.get(\"type\") == \"thinking\": ... # Anthropic-style thinking elif item.get(\"type\") == \"text\": ... # Text ```  ### Create multimodal messages  ```python v1 (new) theme={null} from langchain.messages import HumanMessage message = HumanMessage(content\\_blocks=[ {\"type\": \"text\", \"text\": \"Describe this image.\"}, {\"type\": \"image\", \"url\": \"https://example.com/image.jpg\"}, ]) res = model.invoke([message]) ``` ```python v0 (old) theme={null} from langchain.messages import HumanMessage message = HumanMessage(content=[ # Provider-native structure {\"type\": \"text\", \"text\": \"Describe this image.\"}, {\"type\": \"image\\_url\", \"image\\_url\": {\"url\": \"https://example.com/image.jpg\"}}, ]) res = model.invoke([message]) ```  ### Example block shapes ```python theme={null} # Text block text\\_block = { \"type\": \"text\", \"text\": \"Hello world\", } # Image block image\\_block = { \"type\": \"image\", \"url\": \"https://example.com/image.png\", \"mime\\_type\": \"image/png\", } ``` See the content blocks [reference](/oss/python/langchain/messages#content-block-reference) for more details. ### Serialize standard content Standard content blocks are \\*\\*not serialized\\*\\* into the `content` attribute by default. If you need to access standard content blocks in the `content` attribute (e.g., when sending messages to a client), you can opt-in to serializing them into `content`.  ```bash Environment variable theme={null} export LC\\_OUTPUT\\_VERSION=v1 ``` ```python Initialization parameter theme={null} from langchain.chat\\_models import init\\_chat\\_model model = init\\_chat\\_model( \"gpt-5-nano\", output\\_version=\"v1\", ) ```   Learn more: [Messages](/oss/python/langchain/messages#message-content), [Standard content blocks](/oss/python/langchain/messages#standard-content-blocks), and [Multimodal](/oss/python/langchain/messages#multimodal).  \\*\\*\\* ## Simplified package The `langchain` package namespace has been significantly reduced in v1 to focus on essential building blocks for agents. The streamlined package makes it easier to discover and use the core functionality. ### Namespace | Module | What's available | Notes | | ------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------- | | [`langchain.agents`](https://reference.langchain.com/python/langchain/agents) | [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent), [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState) | Core agent creation functionality | | [`langchain.messages`](https://reference.langchain.com/python/langchain/messages) | Message types, [content blocks](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ContentBlock), [`trim\\_messages`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.trim\\_messages) | Re-exported from `langchain-core` | | [`langchain.tools`](https://reference.langchain.com/python/langchain/tools) | [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool), [`BaseTool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.BaseTool), injection helpers | Re-exported from `langchain-core` | | [`langchain.chat\\_models`](https://reference.langchain.com/python/langchain/models) | [`init\\_chat\\_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat\\_models.init\\_chat\\_model), [`BaseChatModel`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.language\\_models.chat\\_models.BaseChatModel) | Unified model initialization | | [`langchain.embeddings`](https://reference.langchain.com/python/langchain/embeddings) | [`init\\_embeddings`](https://reference.langchain.com/python/langchain\\_core/embeddings/#langchain\\_core.embeddings.embeddings.Embeddings), [`Embeddings`](https://reference.langchain.com/python/langchain\\_core/embeddings/#langchain\\_core.embeddings.embeddings.Embeddings) | Embedding models | ### `langchain-classic` If you were using any of the following from the `langchain` package, you'll need to install [`langchain-classic`](https://pypi.org/project/langchain-classic/) and update your imports: \\* Legacy chains (`LLMChain`, `ConversationChain`, etc.) \\* Retrievers (e.g. `MultiQueryRetriever` or anything from the previous `langchain.retrievers` module) \\* The indexing API \\* The hub module (for managing prompts programmatically) \\* Embeddings modules (e.g. `CacheBackedEmbeddings` and community embeddings) \\* [`langchain-community`](https://pypi.org/project/langchain-community) re-exports \\* Other deprecated functionality  ```python v1 (new) theme={null} # Chains from langchain\\_classic.chains import LLMChain # Retrievers from langchain\\_classic.retrievers import ... # Indexing from langchain\\_classic.indexes import ... # Hub from langchain\\_classic import hub ``` ```python v0 (old) theme={null} # Chains from langchain.chains import LLMChain # Retrievers from langchain.retrievers import ... # Indexing from langchain.indexes import ... # Hub from langchain import hub ```  \\*\\*Installation\\*\\*: ```bash theme={null} uv pip install langchain-classic ``` \\*\\*\\* ## Breaking changes ### Dropped Python 3.9 support All LangChain packages now require \\*\\*Python 3.10 or higher\\*\\*. Python 3.9 reaches [end of life](https://devguide.python.org/versions/) in October 2025. ### Updated return type for chat models The return type signature for chat model invocation has been fixed from [`BaseMessage`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.messages.BaseMessage) to [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage). Custom chat models implementing [`bind\\_tools`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.language\\_models.chat\\_models.BaseChatModel.bind\\_tools) should update their return signature:  ```python v1 (new) theme={null} def bind\\_tools( ... ) -> Runnable[LanguageModelInput, AIMessage]: ``` ```python v0 (old) theme={null} def bind\\_tools( ... ) -> Runnable[LanguageModelInput, BaseMessage]: ```  ### Default message format for OpenAI Responses API When interacting with the Responses API, `langchain-openai` now defaults to storing response items in message `content`. To restore previous behavior, set the `LC\\_OUTPUT\\_VERSION` environment variable to `v0`, or specify `output\\_version=\"v0\"` when instantiating [`ChatOpenAI`](https://reference.langchain.com/python/integrations/langchain\\_openai/ChatOpenAI). ```python theme={null} # Enforce previous behavior with output\\_version flag model = ChatOpenAI(model=\"gpt-4o-mini\", output\\_version=\"v0\") ``` ### Default `max\\_tokens` in `langchain-anthropic` The `max\\_tokens` parameter in `langchain-anthropic` now defaults to higher values based on the model chosen, rather than the previous default of `1024`. If you relied on the old default, explicitly set `max\\_tokens=1024`. ### Legacy code moved to `langchain-classic` Existing functionality outside the focus of standard interfaces and agents has been moved to the [`langchain-classic`](https://pypi.org/project/langchain-classic) package. See the [Simplified namespace](#simplified-package) section for details on what's available in the core `langchain` package and what moved to `langchain-classic`. ### Removal of deprecated APIs Methods, functions, and other objects that were already deprecated and slated for removal in 1.0 have been deleted. Check the [deprecation notices](https://python.langchain.com/docs/versions/migrating\\_chains) from previous versions for replacement APIs. ### Text property Use of the `.text()` method on message objects should drop the parentheses, as it is now a property: ```python theme={null} # Property access text = response.text # Deprecated method call text = response.text() ``` Existing usage patterns (i.e., `.text()`) will continue to function but now emit a warning. The method form will be removed in v2. ### `example` parameter removed from `AIMessage` The `example` parameter has been removed from [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) objects. We recommend migrating to use `additional\\_kwargs` for passing extra metadata as needed. ## Minor changes \\* `AIMessageChunk` objects now include a `chunk\\_position` attribute with position `'last'` to indicate the final chunk in a stream. This allows for clearer handling of streamed messages. If the chunk is not the final one, `chunk\\_position` will be `None`. \\* `LanguageModelOutputVar` is now typed to [`AIMessage`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.AIMessage) instead of [`BaseMessage`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.messages.BaseMessage). \\* The logic for merging message chunks (`AIMessageChunk.add`) has been updated with more sophisticated selection handling for the final id for the merged chunk. It prioritizes provider-assigned IDs over LangChain-generated IDs. \\* We now open files with `utf-8` encoding by default. \\* Standard tests now use multimodal content blocks. \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/migrate/langchain-v1.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/migrate/langgraph-v1.md",
      "title": null,
      "raw_content": "# LangGraph v1 migration guide This guide outlines changes in LangGraph v1 and how to migrate from previous versions. For a high-level overview of changes, see the [what's new](/oss/python/releases/langgraph-v1) page. To upgrade:  ```bash pip theme={null} pip install -U langgraph langchain-core ``` ```bash uv theme={null} uv add langgraph langchain-core ```  ## Summary of changes LangGraph v1 is largely backwards compatible with previous versions. The main change is the deprecation of [`create\\_react\\_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat\\_agent\\_executor.create\\_react\\_agent) in favor of LangChain's new [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent) function. ## Deprecations The following table lists all items deprecated in LangGraph v1: | Deprecated item | Alternative | | ------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | | `create\\_react\\_agent` | [`langchain.agents.create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent) | | `AgentState` | [`langchain.agents.AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState) | | `AgentStatePydantic` | `langchain.agents.AgentState` (no more pydantic state) | | `AgentStateWithStructuredResponse` | `langchain.agents.AgentState` | | `AgentStateWithStructuredResponsePydantic` | `langchain.agents.AgentState` (no more pydantic state) | | `HumanInterruptConfig` | `langchain.agents.middleware.human\\_in\\_the\\_loop.InterruptOnConfig` | | `ActionRequest` | `langchain.agents.middleware.human\\_in\\_the\\_loop.InterruptOnConfig` | | `HumanInterrupt` | `langchain.agents.middleware.human\\_in\\_the\\_loop.HITLRequest` | | `ValidationNode` | Tools automatically validate input with [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent) | | `MessageGraph` | [`StateGraph`](https://reference.langchain.com/python/langgraph/graphs/#langgraph.graph.state.StateGraph) with a `messages` key, like [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent) provides | ## `create\\_react\\_agent` \u2192 `create\\_agent` LangGraph v1 deprecates the [`create\\_react\\_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat\\_agent\\_executor.create\\_react\\_agent) prebuilt. Use LangChain's [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent), which runs on LangGraph and adds a flexible middleware system. See the LangChain v1 docs for details: \\* [Release notes](/oss/python/releases/langchain-v1#createagent) \\* [Migration guide](/oss/python/migrate/langchain-v1#migrate-to-create\\_agent)  ```python v1 (new) theme={null} from langchain.agents import create\\_agent agent = create\\_agent( # [!code highlight] model, tools, system\\_prompt=\"You are a helpful assistant.\", ) ``` ```python v0 (old) theme={null} from langgraph.prebuilt import create\\_react\\_agent agent = create\\_react\\_agent( # [!code highlight] model, tools, prompt=\"You are a helpful assistant.\", # [!code highlight] ) ```  ## Breaking changes ### Dropped Python 3.9 support All LangChain packages now require \\*\\*Python 3.10 or higher\\*\\*. Python 3.9 reached [end of life](https://devguide.python.org/versions/) in October 2025. \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/migrate/langgraph-v1.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/reference/overview.md",
      "title": null,
      "raw_content": "# Reference Comprehensive API reference documentation for the LangChain and LangGraph Python and TypeScript libraries. ## Reference sites   Complete API reference for LangChain Python, including chat models, tools, agents, and more.   Complete API reference for LangGraph Python, including graph APIs, state management, checkpointing, and more.   LangChain packages to connect with popular LLM providers, vector stores, tools, and other services.   Use Model Context Protocol (MCP) tools within LangChain and LangGraph applications.   Build agents that can plan, use subagents, and leverage file systems for complex tasks   \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/reference/overview.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/release-policy.md",
      "title": null,
      "raw_content": "# Release policy This page explains the LangChain and LangGraph release policies. Click on the tabs below to view the release policies for each:   The LangChain ecosystem is composed of different component packages (e.g., `langchain-core`, `langchain`, `langchain-community`, partner packages, etc.) ## Release cadence With the release of LangChain 1.0, \\*\\*minor\\*\\* releases (e.g., from `1.0.x` to `1.1.0`) of `langchain` and `langchain-core` follow semantic versioning and may be released frequently. Minor releases contain new features and improvements but do not include breaking changes. Patch versions are released frequently, up to a few times per week, as they contain bug fixes and minor improvements. ## API stability The development of LLM applications is a rapidly evolving field, and we are constantly learning from our users and the community. As such, we expect that the APIs in `langchain` and `langchain-core` will continue to evolve to better serve the needs of our users. With LangChain 1.0's adoption of semantic versioning: \\* Breaking changes to the public API will only occur in major version releases (e.g., `2.0.0`) \\* Minor version bumps (e.g., `1.0.0` to `1.1.0`) add new features without breaking changes \\* Patch version bumps (e.g., `1.0.0` to `1.0.1`) contain bug fixes and minor improvements We will generally try to avoid making unnecessary changes, and will provide a deprecation policy for features that are being removed. ### Stability of other packages The stability of other packages in the LangChain ecosystem may vary: \\* \\*\\*Partner packages maintained by LangChain\\*\\* (such as `langchain-openai` and `langchain-anthropic`) follow semantic versioning and are expected to be stable post 1.0. Other partner packages may follow different stability and versioning policies, and users should refer to the documentation of those packages for more information. \\* \\*\\*`langchain-community`\\*\\* is a community maintained package that contains 3rd party integrations. Due to the number of integrations there, `langchain-community` does not follow the same strict semantic versioning policy as `langchain` and `langchain-core`. See the \"Special considerations\" section under Long-term support for more details. ## Deprecation policy We will generally avoid deprecating features until a better alternative is available. With LangChain 1.0's semantic versioning approach, deprecated features will continue to work throughout the entire 1.x release series. Breaking changes, including the removal of deprecated features, will only occur in major version releases (e.g., 2.0). When a feature is deprecated in `langchain` or `langchain-core`, we will: \\* Clearly mark it as deprecated in the code and documentation \\* Provide migration guidance to the recommended alternative \\* Provide security updates for the deprecated feature through all 1.x minor releases In some situations, we may allow deprecated features to remain in the code base even longer if they are not causing maintenance issues, to further reduce the burden on users. ## Long-term support (LTS) LangChain follows a long-term support (LTS) policy to provide stability for production applications: ### Release status definitions Packages are marked with one of the following statuses: \\* \\*\\*ACTIVE\\*\\*: Current active development, includes bug fixes, security patches, and new features \\* \\*\\*MAINTENANCE\\*\\*: Receives all security patches and critical bug fixes, but no new features ### Current LTS releases \\*\\*LangChain 1.0\\*\\* is designated as an LTS release: \\* \\*\\*Status\\*\\*: ACTIVE until the release of 2.0 \\* \\*\\*Support period\\*\\*: After 2.0 is released, 1.0 will enter MAINTENANCE mode for at least 1 year \\* \\*\\*Semver compliance\\*\\*: Users can upgrade between minor versions (e.g., 1.0 to 1.1) without breaking changes ### Legacy version support \\*\\*LangChain 0.3\\*\\*: \\* \\*\\*Status\\*\\*: MAINTENANCE mode \\* \\*\\*Support period\\*\\*: Until December 2026 \\* \\*\\*Support includes\\*\\*: Security patches and critical bug fixes ### Special considerations \\*\\*langchain-community 0.4\\*\\*: Due to the nature of community contributions and third-party integrations, `langchain-community` may have breaking changes on minor releases. It has been released as version 0.4 to reflect this different stability policy.   LangGraph follows a structured release policy to ensure stability and predictability for users building production applications. ## Release cadence We expect to space out \\*\\*major\\*\\* releases by at least 6-12 months to provide stability for production applications. \\*\\*Minor\\*\\* releases are typically released every 1-2 months with new features and improvements. \\*\\*Patch\\*\\* releases are released as needed, often weekly, to address bugs and security issues. ## API stability ### Stable APIs All APIs without special prefixes are considered stable and ready for production use. We maintain backward compatibility for stable features within a major version. ### Beta features Features marked as `beta` in the documentation are: \\* Feature-complete and tested \\* Safe for production use with the understanding they may change \\* Subject to minor API adjustments based on user feedback ### Experimental features Features marked as `experimental` or `alpha`: \\* Are under active development \\* May change significantly or be removed \\* Should be used with caution in production ### Internal APIs APIs prefixed with underscore (`\\_`) or explicitly marked as internal: \\* Are not part of the public API \\* May change without notice \\* Should not be used directly ## Deprecation policy When deprecating features: 1. \\*\\*Deprecation Notice\\*\\*: Features are marked as deprecated with clear migration guidance 2. \\*\\*Grace Period\\*\\*: Deprecated features remain functional for at least one minor version 3. \\*\\*Removal\\*\\*: Features are removed only in major version releases 4. \\*\\*Migration Support\\*\\*: We provide migration guides and, when possible, automated tools ## Platform compatibility ### Python support \\* We support Python versions that are actively maintained by the Python Software Foundation \\* Python version requirements may change only in major releases \\* Currently requires Python 3.10 or later ## Breaking changes Breaking changes are only introduced in major versions and include: \\* Removal of deprecated APIs \\* Changes to required parameters \\* Changes to default behavior that affect existing applications \\* Minimum Python/Node.js version updates ## Migration support For major version upgrades, we provide: \\* Comprehensive migration guides \\* Automated migration scripts when feasible \\* Extended support period for the previous major version \\* Clear documentation of all breaking changes ## Long-term support (LTS) LangGraph follows a long-term support (LTS) policy to provide stability for production applications: ### Release status definitions Packages are marked with one of the following statuses: \\* \\*\\*ACTIVE\\*\\*: Current active development, includes bug fixes, security patches, and new features \\* \\*\\*MAINTENANCE\\*\\*: Receives all security patches and critical bug fixes, but no new features ### Current LTS releases \\*\\*LangGraph 1.0\\*\\* is designated as an LTS release: \\* \\*\\*Status\\*\\*: ACTIVE until the release of 2.0 \\* \\*\\*Support period\\*\\*: After 2.0 is released, 1.0 will enter MAINTENANCE mode for at least 1 year \\* \\*\\*Semver compliance\\*\\*: Users can upgrade between minor versions (e.g., 1.0 to 1.1) without breaking changes ### Legacy version support \\*\\*LangGraph 0.4\\*\\*: \\* \\*\\*Status\\*\\*: MAINTENANCE mode \\* \\*\\*Support period\\*\\*: Until December 2026 \\* \\*\\*Support includes\\*\\*: All security patches and critical bug fixes ## See also \\* [Versioning](/oss/python/versioning) - Version numbering and support details \\* [Releases](/oss/python/releases) - Version-specific release notes and migration guides   \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/release-policy.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/releases/changelog.md",
      "title": null,
      "raw_content": "# Changelog > Log of updates and improvements to our Python packages  ## `langchain` v1.1.0 \\* [Model profiles](/oss/python/langchain/models#model-profiles): Chat models now expose supported features and capabilities through a `.profile` attribute. These data are derived from [models.dev](https://models.dev), an open source project providing model capability data. \\* [Summarization middleware](/oss/python/langchain/middleware/built-in#summarization): Updated to support flexible trigger points using model profiles for context-aware summarization. \\* [Structured output](/oss/python/langchain/structured-output): `ProviderStrategy` support (native structured output) can now be inferred from model profiles. \\* [`SystemMessage` for `create\\_agent`](/oss/python/langchain/middleware/custom#working-with-system-messages): Support for passing `SystemMessage` instances directly to `create\\_agent`'s `system\\_prompt` parameter, enabling advanced features like cache control and structured content blocks. \\* [Model retry middleware](/oss/python/langchain/middleware/built-in#model-retry): New middleware for automatically retrying failed model calls with configurable exponential backoff. \\* [Content moderation middleware](/oss/python/langchain/middleware/built-in#content-moderation): OpenAI content moderation middleware for detecting and handling unsafe content in agent interactions. Supports checking user input, model output, and tool results.   ## v1.0.0 ### `langchain` \\* [Release notes](/oss/python/releases/langchain-v1) \\* [Migration guide](/oss/python/migrate/langchain-v1) ### `langgraph` \\* [Release notes](/oss/python/releases/langgraph-v1) \\* [Migration guide](/oss/python/migrate/langgraph-v1)  \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/releases/changelog.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/releases/langchain-v1.md",
      "title": null,
      "raw_content": "# What's new in LangChain v1 \\*\\*LangChain v1 is a focused, production-ready foundation for building agents.\\*\\* We've streamlined the framework around three core improvements:   The new standard for building agents in LangChain, replacing `langgraph.prebuilt.create\\_react\\_agent`.   A new `content\\_blocks` property that provides unified access to modern LLM features across providers.   The `langchain` namespace has been streamlined to focus on essential building blocks for agents, with legacy functionality moved to `langchain-classic`.   To upgrade,  ```bash pip theme={null} pip install -U langchain ``` ```bash uv theme={null} uv add langchain ```  For a complete list of changes, see the [migration guide](/oss/python/migrate/langchain-v1). ## `create\\_agent` [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent) is the standard way to build agents in LangChain 1.0. It provides a simpler interface than [`langgraph.prebuilt.create\\_react\\_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat\\_agent\\_executor.create\\_react\\_agent) while offering greater customization potential by using [middleware](#middleware). ```python theme={null} from langchain.agents import create\\_agent agent = create\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=[search\\_web, analyze\\_data, send\\_email], system\\_prompt=\"You are a helpful research assistant.\" ) result = agent.invoke({ \"messages\": [ {\"role\": \"user\", \"content\": \"Research AI safety trends\"} ] }) ``` Under the hood, [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent) is built on the basic agent loop -- calling a model, letting it choose tools to execute, and then finishing when it calls no more tools:For more information, see [Agents](/oss/python/langchain/agents). ### Middleware Middleware is the defining feature of [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent). It offers a highly customizable entry-point, raising the ceiling for what you can build. Great agents require [context engineering](/oss/python/langchain/context-engineering): getting the right information to the model at the right time. Middleware helps you control dynamic prompts, conversation summarization, selective tool access, state management, and guardrails through a composable abstraction. #### Prebuilt middleware LangChain provides a few [prebuilt middlewares](/oss/python/langchain/middleware#built-in-middleware) for common patterns, including: \\* [`PIIMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.PIIMiddleware): Redact sensitive information before sending to the model \\* [`SummarizationMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.SummarizationMiddleware): Condense conversation history when it gets too long \\* [`HumanInTheLoopMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.HumanInTheLoopMiddleware): Require approval for sensitive tool calls ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.middleware import ( PIIMiddleware, SummarizationMiddleware, HumanInTheLoopMiddleware ) agent = create\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=[read\\_email, send\\_email], middleware=[ PIIMiddleware(\"email\", strategy=\"redact\", apply\\_to\\_input=True), PIIMiddleware( \"phone\\_number\", detector=( r\"(?:\\+?\\d{1,3}[\\s.-]?)?\" r\"(?:\\(?\\d{2,4}\\)?[\\s.-]?)?\" r\"\\d{3,4}[\\s.-]?\\d{4}\" ), strategy=\"block\" ), SummarizationMiddleware( model=\"claude-sonnet-4-5-20250929\", trigger={\"tokens\": 500} ), HumanInTheLoopMiddleware( interrupt\\_on={ \"send\\_email\": { \"allowed\\_decisions\": [\"approve\", \"edit\", \"reject\"] } } ), ] ) ``` #### Custom middleware You can also build custom middleware to fit your needs. Middleware exposes hooks at each step in an agent's execution:Build custom middleware by implementing any of these hooks on a subclass of the [`AgentMiddleware`](https://reference.langchain.com/python/langchain/middleware/#langchain.agents.middleware.AgentMiddleware) class: | Hook | When it runs | Use cases | | ----------------- | ------------------------ | --------------------------------------- | | `before\\_agent` | Before calling the agent | Load memory, validate input | | `before\\_model` | Before each LLM call | Update prompts, trim messages | | `wrap\\_model\\_call` | Around each LLM call | Intercept and modify requests/responses | | `wrap\\_tool\\_call` | Around each tool call | Intercept and modify tool execution | | `after\\_model` | After each LLM response | Validate output, apply guardrails | | `after\\_agent` | After agent completes | Save results, cleanup | Example custom middleware: ```python expandable theme={null} from dataclasses import dataclass from typing import Callable from langchain\\_openai import ChatOpenAI from langchain.agents.middleware import ( AgentMiddleware, ModelRequest ) from langchain.agents.middleware.types import ModelResponse @dataclass class Context: user\\_expertise: str = \"beginner\" class ExpertiseBasedToolMiddleware(AgentMiddleware): def wrap\\_model\\_call( self, request: ModelRequest, handler: Callable[[ModelRequest], ModelResponse] ) -> ModelResponse: user\\_level = request.runtime.context.user\\_expertise if user\\_level == \"expert\": # More powerful model model = ChatOpenAI(model=\"gpt-5\") tools = [advanced\\_search, data\\_analysis] else: # Less powerful model model = ChatOpenAI(model=\"gpt-5-nano\") tools = [simple\\_search, basic\\_calculator] return handler(request.override(model=model, tools=tools)) agent = create\\_agent( model=\"claude-sonnet-4-5-20250929\", tools=[ simple\\_search, advanced\\_search, basic\\_calculator, data\\_analysis ], middleware=[ExpertiseBasedToolMiddleware()], context\\_schema=Context ) ``` For more information, see [the complete middleware guide](/oss/python/langchain/middleware). ### Built on LangGraph Because [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent) is built on [LangGraph](/oss/python/langgraph), you automatically get built in support for long running and reliable agents via:   Conversations automatically persist across sessions with built-in checkpointing   Stream tokens, tool calls, and reasoning traces in real-time   Pause agent execution for human approval before sensitive actions   Rewind conversations to any point and explore alternate paths and prompts   You don't need to learn LangGraph to use these features\u2014they work out of the box. ### Structured output [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent) has improved structured output generation: \\* \\*\\*Main loop integration\\*\\*: Structured output is now generated in the main loop instead of requiring an additional LLM call \\* \\*\\*Structured output strategy\\*\\*: Models can choose between calling tools or using provider-side structured output generation \\* \\*\\*Cost reduction\\*\\*: Eliminates extra expense from additional LLM calls ```python theme={null} from langchain.agents import create\\_agent from langchain.agents.structured\\_output import ToolStrategy from pydantic import BaseModel class Weather(BaseModel): temperature: float condition: str def weather\\_tool(city: str) -> str: \"\"\"Get the weather for a city.\"\"\" return f\"it's sunny and 70 degrees in {city}\" agent = create\\_agent( \"gpt-4o-mini\", tools=[weather\\_tool], response\\_format=ToolStrategy(Weather) ) result = agent.invoke({ \"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in SF?\"}] }) print(repr(result[\"structured\\_response\"])) # results in `Weather(temperature=70.0, condition='sunny')` ``` \\*\\*Error handling\\*\\*: Control error handling via the `handle\\_errors` parameter to `ToolStrategy`: \\* \\*\\*Parsing errors\\*\\*: Model generates data that doesn't match desired structure \\* \\*\\*Multiple tool calls\\*\\*: Model generates 2+ tool calls for structured output schemas \\*\\*\\* ## Standard content blocks  Content block support is currently only available for the following integrations: \\* [`langchain-anthropic`](https://pypi.org/project/langchain-anthropic/) \\* [`langchain-aws`](https://pypi.org/project/langchain-aws/) \\* [`langchain-openai`](https://pypi.org/project/langchain-openai/) \\* [`langchain-google-genai`](https://pypi.org/project/langchain-google-genai/) \\* [`langchain-ollama`](https://pypi.org/project/langchain-ollama/) Broader support for content blocks will be rolled out gradually across more providers.  The new [`content\\_blocks`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.messages.BaseMessage.content\\_blocks) property introduces a standard representation for message content that works across providers: ```python theme={null} from langchain\\_anthropic import ChatAnthropic model = ChatAnthropic(model=\"claude-sonnet-4-5-20250929\") response = model.invoke(\"What's the capital of France?\") # Unified access to content blocks for block in response.content\\_blocks: if block[\"type\"] == \"reasoning\": print(f\"Model reasoning: {block['reasoning']}\") elif block[\"type\"] == \"text\": print(f\"Response: {block['text']}\") elif block[\"type\"] == \"tool\\_call\": print(f\"Tool call: {block['name']}({block['args']})\") ``` ### Benefits \\* \\*\\*Provider agnostic\\*\\*: Access reasoning traces, citations, built-in tools (web search, code interpreters, etc.), and other features using the same API regardless of provider \\* \\*\\*Type safe\\*\\*: Full type hints for all content block types \\* \\*\\*Backward compatible\\*\\*: Standard content can be [loaded lazily](/oss/python/langchain/messages#standard-content-blocks), so there are no associated breaking changes For more information, see our guide on [content blocks](/oss/python/langchain/messages#standard-content-blocks). \\*\\*\\* ## Simplified package LangChain v1 streamlines the [`langchain`](https://pypi.org/project/langchain/) package namespace to focus on essential building blocks for agents. The refined namespace exposes the most useful and relevant functionality: ### Namespace | Module | What's available | Notes | | ------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------- | | [`langchain.agents`](https://reference.langchain.com/python/langchain/agents) | [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent), [`AgentState`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.AgentState) | Core agent creation functionality | | [`langchain.messages`](https://reference.langchain.com/python/langchain/messages) | Message types, [content blocks](https://reference.langchain.com/python/langchain/messages/#langchain.messages.ContentBlock), [`trim\\_messages`](https://reference.langchain.com/python/langchain/messages/#langchain.messages.trim\\_messages) | Re-exported from [`langchain-core`](https://reference.langchain.com/python/langchain\\_core/) | | [`langchain.tools`](https://reference.langchain.com/python/langchain/tools) | [`@tool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.tool), [`BaseTool`](https://reference.langchain.com/python/langchain/tools/#langchain.tools.BaseTool), injection helpers | Re-exported from [`langchain-core`](https://reference.langchain.com/python/langchain\\_core/) | | [`langchain.chat\\_models`](https://reference.langchain.com/python/langchain/models) | [`init\\_chat\\_model`](https://reference.langchain.com/python/langchain/models/#langchain.chat\\_models.init\\_chat\\_model), [`BaseChatModel`](https://reference.langchain.com/python/langchain\\_core/language\\_models/#langchain\\_core.language\\_models.chat\\_models.BaseChatModel) | Unified model initialization | | [`langchain.embeddings`](https://reference.langchain.com/python/langchain/embeddings) | [`Embeddings`](https://reference.langchain.com/python/langchain\\_core/embeddings/#langchain\\_core.embeddings.embeddings.Embeddings), [`init\\_embeddings`](https://reference.langchain.com/python/langchain\\_core/embeddings/#langchain\\_core.embeddings.embeddings.Embeddings) | Embedding models | Most of these are re-exported from `langchain-core` for convenience, which gives you a focused API surface for building agents. ```python theme={null} # Agent building from langchain.agents import create\\_agent # Messages and content from langchain.messages import AIMessage, HumanMessage # Tools from langchain.tools import tool # Model initialization from langchain.chat\\_models import init\\_chat\\_model from langchain.embeddings import init\\_embeddings ``` ### `langchain-classic` Legacy functionality has moved to [`langchain-classic`](https://pypi.org/project/langchain-classic) to keep the core packages lean and focused. \\*\\*What's in `langchain-classic`:\\*\\* \\* Legacy chains and chain implementations \\* Retrievers (e.g. `MultiQueryRetriever` or anything from the previous `langchain.retrievers` module) \\* The indexing API \\* The hub module (for managing prompts programmatically) \\* [`langchain-community`](https://pypi.org/project/langchain-community) exports \\* Other deprecated functionality If you use any of this functionality, install [`langchain-classic`](https://pypi.org/project/langchain-classic):  ```bash pip theme={null} pip install langchain-classic ``` ```bash uv theme={null} uv add langchain-classic ```  Then update your imports: ```python theme={null} from langchain import ... # [!code --] from langchain\\_classic import ... # [!code ++] from langchain.chains import ... # [!code --] from langchain\\_classic.chains import ... # [!code ++] from langchain.retrievers import ... # [!code --] from langchain\\_classic.retrievers import ... # [!code ++] from langchain import hub # [!code --] from langchain\\_classic import hub # [!code ++] ``` ## Migration guide See our [migration guide](/oss/python/migrate/langchain-v1) for help updating your code to LangChain v1. ## Reporting issues Please report any issues discovered with 1.0 on [GitHub](https://github.com/langchain-ai/langchain/issues) using the `'v1'` [label](https://github.com/langchain-ai/langchain/issues?q=state%3Aopen%20label%3Av1). ## Additional resources   Read the announcement   Deep dive into middleware   Full agent documentation   New content blocks API   How to migrate to LangChain v1   Report issues or contribute   ## See also \\* [Versioning](/oss/python/versioning) \u2013 Understanding version numbers \\* [Release policy](/oss/python/release-policy) \u2013 Detailed release policies \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/releases/langchain-v1.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/releases/langgraph-v1.md",
      "title": null,
      "raw_content": "# What's new in LangGraph v1 \\*\\*LangGraph v1 is a stability-focused release for the agent runtime.\\*\\* It keeps the core graph APIs and execution model unchanged, while refining type safety, docs, and developer ergonomics. It's designed to work hand-in-hand with [LangChain v1](/oss/python/releases/langchain-v1) (whose `create\\_agent` is built on LangGraph) so you can start high-level and drop down to granular control when needed.   Graph primitives (state, nodes, edges) and the execution/runtime model are unchanged, making upgrades straightforward.   Durable execution with checkpointing, persistence, streaming, and human-in-the-loop continues to be first-class.   LangChain's `create\\_agent` runs on LangGraph. Use LangChain for a fast start; drop to LangGraph for custom orchestration.   To upgrade,  ```bash pip theme={null} pip install -U langgraph ``` ```bash uv theme={null} uv add langgraph ```  ## Deprecation of `create\\_react\\_agent` The LangGraph [`create\\_react\\_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat\\_agent\\_executor.create\\_react\\_agent) prebuilt has been deprecated in favor of LangChain's [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent). It provides a simpler interface, and offers greater customization potential through the introduction of middleware. \\* For information on the new [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent) API, see the [LangChain v1 release notes](/oss/python/releases/langchain-v1#create-agent). \\* For information on migrating from [`create\\_react\\_agent`](https://reference.langchain.com/python/langgraph/agents/#langgraph.prebuilt.chat\\_agent\\_executor.create\\_react\\_agent) to [`create\\_agent`](https://reference.langchain.com/python/langchain/agents/#langchain.agents.create\\_agent), see the [LangChain v1 migration guide](/oss/python/migrate/langchain-v1#create-agent). ## Reporting issues Please report any issues discovered with 1.0 on [GitHub](https://github.com/langchain-ai/langgraph/issues) using the [`'v1'` label](https://github.com/langchain-ai/langgraph/issues?q=state%3Aopen%20label%3Av1). ## Additional resources   Read the announcement   What LangGraph is and when to use it   Build graphs with state, nodes, and edges   High-level agents built on LangGraph   How to migrate to LangGraph v1   Report issues or contribute   ## See also \\* [Versioning](/oss/python/versioning) \u2013 Understanding version numbers \\* [Release policy](/oss/python/release-policy) \u2013 Detailed release policies \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/python/releases/langgraph-v1.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/security-policy.md",
      "title": null,
      "raw_content": "# Security policy LangChain has a large ecosystem of integrations with various external resources like local and remote file systems, APIs and databases. These integrations allow developers to create versatile applications that combine the power of LLMs with the ability to access, interact with and manipulate external resources. ## Best practices When building such applications developers should remember to follow good security practices: \\* [\\*\\*Limit permissions\\*\\*](https://en.wikipedia.org/wiki/Principle\\_of\\_least\\_privilege): Scope permissions specifically to the application's need. Granting broad or excessive permissions can introduce significant security vulnerabilities. To avoid such vulnerabilities, consider using read-only credentials, disallowing access to sensitive resources, using sandboxing techniques (such as running inside a container), specifying proxy configurations to control external requests, etc. as appropriate for your application. \\* \\*\\*Anticipate potential misuse\\*\\*: Just as humans can err, so can Large Language Models (LLMs). Always assume that any system access or credentials may be used in any way allowed by the permissions they are assigned. For example, if a pair of database credentials allows deleting data, it's safest to assume that any LLM able to use those credentials may in fact delete data. \\* [\\*\\*Defense in depth\\*\\*](https://en.wikipedia.org/wiki/Defense\\_in\\_depth\\_\\(computing\\)): No security technique is perfect. Fine-tuning and good chain design can reduce, but not eliminate, the odds that a Large Language Model (LLM) may make a mistake. It's best to combine multiple layered security approaches rather than relying on any single layer of defense to ensure security. For example: use both read-only permissions and sandboxing to ensure that LLMs are only able to access data that is explicitly meant for them to use. Risks of not doing so include, but are not limited to: \\* Data corruption or loss. \\* Unauthorized access to confidential information. \\* Compromised performance or availability of critical resources. Example scenarios with mitigation strategies: \\* A user may ask an agent with access to the file system to delete files that should not be deleted or read the content of files that contain sensitive information. To mitigate, limit the agent to only use a specific directory and only allow it to read or write files that are safe to read or write. Consider further sandboxing the agent by running it in a container. \\* A user may ask an agent with write access to an external API to write malicious data to the API, or delete data from that API. To mitigate, give the agent read-only API keys, or limit it to only use endpoints that are already resistant to such misuse. \\* A user may ask an agent with access to a database to drop a table or mutate the schema. To mitigate, scope the credentials to only the tables that the agent needs to access and consider issuing READ-ONLY credentials. If you're building applications that access external resources like file systems, APIs or databases, consider speaking with your company's security team to determine how to best design and secure your applications. ## Reporting OSS vulnerabilities LangChain is partnered with [huntr by Protect AI](https://huntr.com/) to provide a bounty program for our open source projects. Please report security vulnerabilities associated with the LangChain open source projects at [huntr](https://huntr.com/bounties/disclose/?target=https%3A%2F%2Fgithub.com%2Flangchain-ai%2Flangchain\\&validSearch=true). Before reporting a vulnerability, please review: 1. [In-scope targets](#in-scope-targets) and [out-of-scope targets](#out-of-scope-targets). 2. The [langchain-ai/langchain](https://python.langchain.com/docs/contributing/repo\\_structure) monorepo structure. 3. The [best practices](#best-practices) above to understand what we consider to be a security vulnerability vs. developer responsibility. ### In-scope targets The following packages and repositories are eligible for bug bounties: \\* langchain-core \\* langchain (see exceptions) \\* langchain-community (see exceptions) \\* langgraph \\* langserve ### Out-of-scope targets All out of scope targets defined by huntr as well as: \\* \\*\\*langchain-experimental\\*\\*: This repository is for experimental code and is not eligible for bug bounties (see [package warning](https://pypi.org/project/langchain-experimental/)), bug reports to it will be marked as interesting or waste of time and published with no bounty attached. \\* \\*\\*tools\\*\\*: Tools in either langchain or langchain-community are not eligible for bug bounties. This includes the following directories \\* libs/langchain/langchain/tools \\* libs/community/langchain\\\\_community/tools \\* Please review the [best practices](#best-practices) for more details, but generally tools interact with the real world. Developers are expected to understand the security implications of their code and are responsible for the security of their tools. \\* Code documented with security notices. This will be decided on a case by case basis, but likely will not be eligible for a bounty as the code is already documented with guidelines for developers that should be followed for making their application secure. \\* Any LangSmith related repositories or APIs (see [Reporting LangSmith vulnerabilities](#reporting-langsmith-vulnerabilities)). ## Reporting LangSmith vulnerabilities Please report security vulnerabilities associated with LangSmith by email to `security@langchain.dev`. \\* LangSmith site: [https://smith.langchain.com](https://smith.langchain.com) \\* SDK client: [https://github.com/langchain-ai/langsmith-sdk](https://github.com/langchain-ai/langsmith-sdk) ### Other security concerns For any other security concerns, please contact us at `security@langchain.dev`. \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/security-policy.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    },
    {
      "url": "https://docs.langchain.com/oss/python/versioning.md",
      "title": null,
      "raw_content": "# Versioning Each LangChain and LangGraph version number follows the format: `MAJOR.MINOR.PATCH` \\* \\*\\*Major\\*\\*: Breaking API updates that require code changes. \\* \\*\\*Minor\\*\\*: New features and improvements that maintain backward compatibility. \\* \\*\\*Patch\\*\\*: Bug fixes and minor improvements. ## Version numbering LangChain and LangGraph follow [Semantic Versioning](https://semver.org/) principles: \\* `1.0.0`: First stable release with production-ready APIs \\* `1.1.0`: New features added in a backward-compatible manner \\* `1.0.1`: Backward-compatible bug fixes ## API stability We communicate the stability of our APIs as follows: ### Stable APIs All APIs without special prefixes are considered stable and ready for production use. We maintain backward compatibility for stable features and only introduce breaking changes in major releases. ### Beta APIs APIs marked as `beta` are feature-complete but may undergo minor changes based on user feedback. They are safe for production use but may require small adjustments in future releases. ### Alpha APIs APIs marked as `alpha` are experimental and subject to significant changes. Use these with caution in production environments. ### Deprecated APIs APIs marked as `deprecated` will be removed in future major releases. When possible, we specify the intended version of removal. To handle deprecations: 1. Switch to the recommended alternative API 2. Follow the migration guide (released alongside major releases) 3. Use automated migration tools when available ### Internal APIs Certain APIs are explicitly marked as \"internal\" in a couple of ways: \\* Some documentation refers to internals and mentions them as such. If the documentation says that something is internal, it may change. \\* Functions, methods, and other objects prefixed by a leading underscore (\\*\\*`\\_`\\*\\*). This is the standard Python convention of indicating that something is private; if any method starts with a single \\*\\*`\\_`\\*\\*, it's an internal API. \\* \\*\\*Exception:\\*\\* Certain methods are prefixed with `\\_` , but do not contain an implementation. These methods are \\*meant\\* to be overridden by sub-classes that provide the implementation. Such methods are generally part of the \\*\\*Public API\\*\\* of LangChain. ## Release cycles   Major releases (e.g., `1.0.0` \u2192 `2.0.0`) may include: \\* Breaking API changes \\* Removal of deprecated features \\* Significant architectural improvements We provide: \\* Detailed migration guides \\* Automated migration tools when possible \\* Extended support period for the previous major version   Minor releases (e.g., `1.0.0` \u2192 `1.1.0`) include: \\* New features and capabilities \\* Performance improvements \\* New optional parameters \\* Backward-compatible enhancements   Patch releases (e.g., `1.0.0` \u2192 `1.0.1`) include: \\* Bug fixes \\* Security updates \\* Documentation improvements \\* Performance optimizations without API changes   ## Version support policy \\* \\*\\*Latest major version\\*\\*: Full support with active development (ACTIVE status) \\* \\*\\*Previous major version\\*\\*: Security updates and critical bug fixes for 12 months after the next major release (MAINTENANCE status) \\* \\*\\*Older versions\\*\\*: Community support only ### Long-term support (LTS) releases Both LangChain and LangGraph 1.0 are designated as LTS releases: \\* Version 1.0 will remain in ACTIVE status until version 2.0 is released \\* After version 2.0 is released, version 1.0 will enter MAINTENANCE mode for at least 1 year \\* LTS releases follow semantic versioning (semver), allowing safe upgrades between minor versions \\* Legacy versions (LangChain 0.3 and LangGraph 0.4) are in MAINTENANCE mode until December 2026 For detailed information about release status and support timelines, see the [Release policy](/oss/python/release-policy). ## Check your version To check your installed version:  ```python LangChain theme={null} import langchain\\_core print(langchain\\_core.\\_\\_version\\_\\_) ``` ```python LangGraph theme={null} import langgraph print(langgraph.\\_\\_version\\_\\_) ```  ## Upgrade  ```bash LangChain theme={null} # Upgrade to the latest version pip install -U langchain-core langchain # Upgrade to a specific version pip install langchain-core==1.0.0 ``` ```bash LangGraph theme={null} # Upgrade to the latest version pip install -U langgraph # Upgrade to a specific version pip install langgraph==1.0.0 ```  ## Pre-release versions We occasionally release alpha and beta versions for early testing: \\* \\*\\*Alpha\\*\\* (e.g., `1.0.0a1`): Early preview, significant changes expected \\* \\*\\*Beta\\*\\* (e.g., `1.0.0b1`): Feature-complete, minor changes possible \\* \\*\\*Release Candidate\\*\\* (e.g., `1.0.0rc1`): Final testing before stable release ## See also \\* [Release policy](/oss/python/release-policy) - Detailed release and deprecation policies \\* [Releases](/oss/python/releases) - Version-specific release notes and migration guides \\*\\*\\*  [Edit the source of this page on GitHub.](https://github.com/langchain-ai/docs/edit/main/src/oss/versioning.mdx)   [Connect these docs programmatically](/use-these-docs) to Claude, VSCode, and more via MCP for real-time answers.  --- > To find navigation and other pages in this documentation, fetch the llms.txt file at: https://docs.langchain.com/llms.txt",
      "images": []
    }
  ]
}